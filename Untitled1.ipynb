{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'th' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d6e7f1f52d3c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mConditionalDiscriminator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;34m\"\"\" Discriminator of the GAN \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'th' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "class ConditionalDiscriminator(th.nn.Module):\n",
    "    \"\"\" Discriminator of the GAN \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes, height=7, feature_size=512):\n",
    "        \"\"\"\n",
    "        constructor for the class\n",
    "        :param num_classes: number of classes for conditional discrimination\n",
    "        :param height: total height of the discriminator (Must be equal to the Generator depth)\n",
    "        :param feature_size: size of the deepest features extracted\n",
    "                             (Must be equal to Generator latent_size)\n",
    "        \"\"\"\n",
    "        from torch.nn import ModuleList, AvgPool2d\n",
    "\n",
    "        super(ConditionalDiscriminator, self).__init__()\n",
    "\n",
    "        assert feature_size != 0 and ((feature_size & (feature_size - 1)) == 0), \\\n",
    "            \"latent size not a power of 2\"\n",
    "        if height >= 4:\n",
    "            assert feature_size >= np.power(2, height - 4), \"feature size cannot be produced\"\n",
    "\n",
    "        # create state of the object\n",
    "        self.height = height\n",
    "        self.feature_size = feature_size\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.final_block = ConDisFinalBlock(self.feature_size, self.num_classes)\n",
    "\n",
    "        # create a module list of the other required general convolution blocks\n",
    "        self.layers = ModuleList([])  # initialize to empty list\n",
    "\n",
    "        # create the fromRGB layers for various inputs:\n",
    "        self.fromRGB = lambda out_channels: \\\n",
    "            _equalized_conv2d(3, out_channels, (1, 1), bias=True)\n",
    "\n",
    "        self.rgb_to_features = ModuleList([self.fromRGB(self.feature_size)])\n",
    "\n",
    "        # create the remaining layers\n",
    "        for i in range(self.height - 1):\n",
    "            if i > 2:\n",
    "                layer = DisGeneralConvBlock(\n",
    "                    int(self.feature_size // np.power(2, i - 2)),\n",
    "                    int(self.feature_size // np.power(2, i - 3))\n",
    "                )\n",
    "                rgb = self.fromRGB(int(self.feature_size // np.power(2, i - 2)))\n",
    "            else:\n",
    "                layer = DisGeneralConvBlock(self.feature_size,\n",
    "                                            self.feature_size)\n",
    "                rgb = self.fromRGB(self.feature_size)\n",
    "\n",
    "            self.layers.append(layer)\n",
    "            self.rgb_to_features.append(rgb)\n",
    "\n",
    "        # register the temporary downSampler\n",
    "        self.temporaryDownsampler = AvgPool2d(2)\n",
    "\n",
    "    def forward(self, x, labels, height, alpha):\n",
    "        \"\"\"\n",
    "        forward pass of the discriminator\n",
    "        :param x: input to the network\n",
    "        :param labels: labels required for conditional discrimination\n",
    "                       note that these are pure integer labels of shape [B x 1]\n",
    "        :param height: current height of operation (Progressive GAN)\n",
    "        :param alpha: current value of alpha for fade-in\n",
    "        :return: out => raw prediction values\n",
    "        \"\"\"\n",
    "\n",
    "        assert height < self.height, \"Requested output depth cannot be produced\"\n",
    "\n",
    "        if height > 0:\n",
    "            residual = self.rgb_to_features[height - 1](self.temporaryDownsampler(x))\n",
    "\n",
    "            straight = self.layers[height - 1](\n",
    "                self.rgb_to_features[height](x)\n",
    "            )\n",
    "\n",
    "            y = (alpha * straight) + ((1 - alpha) * residual)\n",
    "\n",
    "            for block in reversed(self.layers[:height - 1]):\n",
    "                y = block(y)\n",
    "        else:\n",
    "            y = self.rgb_to_features[0](x)\n",
    "\n",
    "        out = self.final_block(y, labels)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    \n",
    "class CondWGAN_GP:\n",
    "\n",
    "    def __init__(self, dis, drift=0.001, use_gp=False):\n",
    "        self.dis = dis\n",
    "        self.drift = drift\n",
    "        self.use_gp = use_gp\n",
    "\n",
    "    def __gradient_penalty(self, real_samps, fake_samps, labels,\n",
    "                           height, alpha, reg_lambda=10):\n",
    "        \"\"\"\n",
    "        private helper for calculating the gradient penalty\n",
    "        :param real_samps: real samples\n",
    "        :param fake_samps: fake samples\n",
    "        :param labels: used for conditional loss calculation\n",
    "                       Note that this is just [Batch x 1] plain integer labels\n",
    "        :param height: current depth in the optimization\n",
    "        :param alpha: current alpha for fade-in\n",
    "        :param reg_lambda: regularisation lambda\n",
    "        :return: tensor (gradient penalty)\n",
    "        \"\"\"\n",
    "        from torch.autograd import grad\n",
    "\n",
    "        batch_size = real_samps.shape[0]\n",
    "\n",
    "        # generate random epsilon\n",
    "        epsilon = th.rand((batch_size, 1, 1, 1)).to(fake_samps.device)\n",
    "\n",
    "        # create the merge of both real and fake samples\n",
    "        merged = (epsilon * real_samps) + ((1 - epsilon) * fake_samps)\n",
    "        merged.requires_grad_(True)\n",
    "\n",
    "        # forward pass\n",
    "        op = self.dis(merged, labels, height, alpha)\n",
    "\n",
    "        # obtain gradient of op wrt. merged\n",
    "        gradient = grad(outputs=op, inputs=merged,\n",
    "                        grad_outputs=th.ones_like(op), create_graph=True,\n",
    "                        retain_graph=True, only_inputs=True)[0]\n",
    "\n",
    "        # calculate the penalty using these gradients\n",
    "        gradient = gradient.view(batch_size, -1)\n",
    "        penalty = reg_lambda * ((gradient.norm(p=2, dim=1) - 1) ** 2).mean()\n",
    "\n",
    "        # return the calculated penalty:\n",
    "        return penalty\n",
    "\n",
    "    def dis_loss(self, real_samps, fake_samps, labels, height, alpha):\n",
    "        # define the (Wasserstein) loss\n",
    "        fake_out = self.dis(fake_samps, labels, height, alpha)\n",
    "        real_out = self.dis(real_samps, labels, height, alpha)\n",
    "\n",
    "        loss = (th.mean(fake_out) - th.mean(real_out)\n",
    "                + (self.drift * th.mean(real_out ** 2)))\n",
    "\n",
    "        if self.use_gp:\n",
    "            # calculate the WGAN-GP (gradient penalty)\n",
    "            gp = self.__gradient_penalty(real_samps, fake_samps,\n",
    "                                         labels, height, alpha)\n",
    "            loss += gp\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def gen_loss(self, _, fake_samps, labels, height, alpha):\n",
    "        # calculate the WGAN loss for generator\n",
    "        loss = -th.mean(self.dis(fake_samps, labels, height, alpha))\n",
    "\n",
    "        return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class ConDisFinalBlock(th.nn.Module):\n",
    "    \"\"\" Final block for the Conditional Discriminator\n",
    "        Uses the Projection mechanism from the paper -> https://arxiv.org/pdf/1802.05637.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        \"\"\"\n",
    "        constructor of the class\n",
    "        :param in_channels: number of input channels\n",
    "        :param num_classes: number of classes for conditional discrimination\n",
    "        \"\"\"\n",
    "        from torch.nn import LeakyReLU, Embedding\n",
    "\n",
    "        super(ConDisFinalBlock, self).__init__()\n",
    "\n",
    "        # declare the required modules for forward pass\n",
    "        self.batch_discriminator = MinibatchStdDev()\n",
    "        self.conv_1 = _equalized_conv2d(in_channels + 1, in_channels, (3, 3), pad=1, bias=True)\n",
    "        self.conv_2 = _equalized_conv2d(in_channels, in_channels, (4, 4), bias=True)\n",
    "\n",
    "        # final conv layer emulates a fully connected layer\n",
    "        self.conv_3 = _equalized_conv2d(in_channels, 1, (1, 1), bias=True)\n",
    "\n",
    "        # we also need an embedding matrix for the label vectors\n",
    "        self.label_embedder = Embedding(num_classes, in_channels, max_norm=1)\n",
    "\n",
    "        # leaky_relu:\n",
    "        self.lrelu = LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        \"\"\"\n",
    "        forward pass of the FinalBlock\n",
    "        :param x: input\n",
    "        :param labels: samples' labels for conditional discrimination\n",
    "                       Note that these are pure integer labels [Batch_size x 1]\n",
    "        :return: y => output\n",
    "        \"\"\"\n",
    "        # minibatch_std_dev layer\n",
    "        y = self.batch_discriminator(x)  # [B x C x 4 x 4]\n",
    "\n",
    "        # perform the forward pass\n",
    "        y = self.lrelu(self.conv_1(y))  # [B x C x 4 x 4]\n",
    "\n",
    "        # obtain the computed features\n",
    "        y = self.lrelu(self.conv_2(y))  # [B x C x 1 x 1]\n",
    "\n",
    "        # embed the labels\n",
    "        labels = self.label_embedder(labels)  # [B x C]\n",
    "\n",
    "        # compute the inner product with the label embeddings\n",
    "        y_ = th.squeeze(th.squeeze(y, dim=-1), dim=-1)  # [B x C]\n",
    "        projection_scores = (y_ * labels).sum(dim=-1)  # [B]\n",
    "\n",
    "        # normal discrimination score\n",
    "        y = self.lrelu(self.conv_3(y))  # This layer has linear activation\n",
    "\n",
    "        # calculate the total score\n",
    "        final_score = y.view(-1) + projection_scores\n",
    "\n",
    "        # return the output raw discriminator scores\n",
    "        return final_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
