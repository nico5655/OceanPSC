{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 896,
     "status": "ok",
     "timestamp": 1593958143447,
     "user": {
      "displayName": "Manos Panagiotou",
      "photoUrl": "",
      "userId": "06697745822174955863"
     },
     "user_tz": -180
    },
    "id": "6C1l-sW7IQIH"
   },
   "outputs": [],
   "source": [
    "#DataTools.py\n",
    "\"\"\" Module for the data loading pipeline for the model to train \"\"\"\n",
    "\n",
    "\n",
    "def get_transform(new_size=None):\n",
    "    \"\"\"\n",
    "    obtain the image transforms required for the input data\n",
    "    :param new_size: size of the resized images\n",
    "    :return: image_transform => transform object from TorchVision\n",
    "    \"\"\"\n",
    "    from torchvision.transforms import ToTensor, Normalize, Compose, Resize\n",
    "\n",
    "    if new_size is not None:\n",
    "        image_transform = Compose([\n",
    "            Resize(new_size),\n",
    "            ToTensor(),\n",
    "            Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "        ])\n",
    "\n",
    "    else:\n",
    "        image_transform = Compose([\n",
    "            ToTensor(),\n",
    "            Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "        ])\n",
    "    return image_transform\n",
    "\n",
    "\n",
    "def get_data_loader(dataset, batch_size, num_workers):\n",
    "    \"\"\"\n",
    "    generate the data_loader from the given dataset\n",
    "    :param dataset: dataset for training (Should be a PyTorch dataset)\n",
    "                    Make sure every item is an Image\n",
    "    :param batch_size: batch size of the data\n",
    "    :param num_workers: num of parallel readers\n",
    "    :return: dl => dataloader for the dataset\n",
    "    \"\"\"\n",
    "    from torch.utils.data import DataLoader\n",
    "\n",
    "    dl = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5580,
     "status": "ok",
     "timestamp": 1593958148150,
     "user": {
      "displayName": "Manos Panagiotou",
      "photoUrl": "",
      "userId": "06697745822174955863"
     },
     "user_tz": -180
    },
    "id": "QYp4Pxx0IV5k"
   },
   "outputs": [],
   "source": [
    "#Losses.py\n",
    "\"\"\" Module implementing various loss functions \"\"\"\n",
    "\n",
    "import torch as th\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================\n",
    "# Normal versions of the Losses:\n",
    "# =============================================================\n",
    "\n",
    "\n",
    "\n",
    "class WGAN_GP:\n",
    "\n",
    "    def __init__(self, dis, drift=0.001, use_gp=False):\n",
    "        self.dis=dis\n",
    "        self.drift = drift\n",
    "        self.use_gp = use_gp\n",
    "\n",
    "    def __gradient_penalty(self, real_samps, fake_samps,\n",
    "                           height, alpha, reg_lambda=10):\n",
    "        \"\"\"\n",
    "        private helper for calculating the gradient penalty\n",
    "        :param real_samps: real samples\n",
    "        :param fake_samps: fake samples\n",
    "        :param height: current depth in the optimization\n",
    "        :param alpha: current alpha for fade-in\n",
    "        :param reg_lambda: regularisation lambda\n",
    "        :return: tensor (gradient penalty)\n",
    "        \"\"\"\n",
    "        batch_size = real_samps.shape[0]\n",
    "\n",
    "        # generate random epsilon\n",
    "        epsilon = th.rand((batch_size, 1, 1, 1)).to(fake_samps.device)\n",
    "\n",
    "        # create the merge of both real and fake samples\n",
    "        merged = epsilon * real_samps + ((1 - epsilon) * fake_samps)\n",
    "        merged.requires_grad_(True)\n",
    "\n",
    "        # forward pass\n",
    "        op = self.dis(merged, height, alpha)\n",
    "\n",
    "        # perform backward pass from op to merged for obtaining the gradients\n",
    "        gradient = th.autograd.grad(outputs=op, inputs=merged,\n",
    "                                    grad_outputs=th.ones_like(op), create_graph=True,\n",
    "                                    retain_graph=True, only_inputs=True)[0]\n",
    "\n",
    "        # calculate the penalty using these gradients\n",
    "        gradient = gradient.view(gradient.shape[0], -1)\n",
    "        penalty = reg_lambda * ((gradient.norm(p=2, dim=1) - 1) ** 2).mean()\n",
    "\n",
    "        # return the calculated penalty:\n",
    "        return penalty\n",
    "\n",
    "    def dis_loss(self, real_samps, fake_samps, height, alpha):\n",
    "        # define the (Wasserstein) loss\n",
    "        fake_out = self.dis(fake_samps, height, alpha)\n",
    "        real_out = self.dis(real_samps, height, alpha)\n",
    "\n",
    "        loss = (th.mean(fake_out) - th.mean(real_out)\n",
    "                + (self.drift * th.mean(real_out ** 2)))\n",
    "\n",
    "        # calculate the WGAN-GP (gradient penalty)\n",
    "        gp = self.__gradient_penalty(real_samps, fake_samps, height, alpha)\n",
    "        loss += gp\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def gen_loss(self, _, fake_samps, height, alpha):\n",
    "        # calculate the WGAN loss for generator\n",
    "        loss = -th.mean(self.dis(fake_samps, height, alpha))\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6856,
     "status": "ok",
     "timestamp": 1593958149436,
     "user": {
      "displayName": "Manos Panagiotou",
      "photoUrl": "",
      "userId": "06697745822174955863"
     },
     "user_tz": -180
    },
    "id": "Wfv0NxHKHp9P"
   },
   "outputs": [],
   "source": [
    "#CustomLayers.py\n",
    "\"\"\" Module containing custom layers \"\"\"\n",
    "\n",
    "import torch as th\n",
    "\n",
    "\n",
    "# extending Conv2D and Deconv2D layers for equalized learning rate logic\n",
    "class _equalized_conv2d(th.nn.Module):\n",
    "    \"\"\" conv2d with the concept of equalized learning rate\n",
    "        Args:\n",
    "            :param c_in: input channels\n",
    "            :param c_out:  output channels\n",
    "            :param k_size: kernel size (h, w) should be a tuple or a single integer\n",
    "            :param stride: stride for conv\n",
    "            :param pad: padding\n",
    "            :param bias: whether to use bias or not\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, c_in, c_out, k_size, stride=1, pad=0, bias=True):\n",
    "        \"\"\" constructor for the class \"\"\"\n",
    "        from torch.nn.modules.utils import _pair\n",
    "        from numpy import sqrt, prod\n",
    "\n",
    "        super(_equalized_conv2d, self).__init__()\n",
    "\n",
    "        # define the weight and bias if to be used\n",
    "        self.weight = th.nn.Parameter(th.nn.init.normal_(\n",
    "            th.empty(c_out, c_in, *_pair(k_size))\n",
    "        ))\n",
    "\n",
    "        self.use_bias = bias\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = th.nn.Parameter(th.FloatTensor(c_out).fill_(0))\n",
    "\n",
    "        fan_in = prod(_pair(k_size)) * c_in  # value of fan_in\n",
    "        self.scale = sqrt(2) / sqrt(fan_in)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        forward pass of the network\n",
    "        :param x: input\n",
    "        :return: y => output\n",
    "        \"\"\"\n",
    "        from torch.nn.functional import conv2d\n",
    "\n",
    "        return conv2d(input=x,\n",
    "                      weight=self.weight * self.scale,  # scale the weight on runtime\n",
    "                      bias=self.bias if self.use_bias else None,\n",
    "                      stride=self.stride,\n",
    "                      padding=self.pad)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return \", \".join(map(str, self.weight.shape))\n",
    "\n",
    "\n",
    "class _equalized_deconv2d(th.nn.Module):\n",
    "    \"\"\" Transpose convolution using the equalized learning rate\n",
    "        Args:\n",
    "            :param c_in: input channels\n",
    "            :param c_out: output channels\n",
    "            :param k_size: kernel size\n",
    "            :param stride: stride for convolution transpose\n",
    "            :param pad: padding\n",
    "            :param bias: whether to use bias or not\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, c_in, c_out, k_size, stride=1, pad=0, bias=True):\n",
    "        \"\"\" constructor for the class \"\"\"\n",
    "        from torch.nn.modules.utils import _pair\n",
    "        from numpy import sqrt\n",
    "\n",
    "        super(_equalized_deconv2d, self).__init__()\n",
    "\n",
    "        # define the weight and bias if to be used\n",
    "        self.weight = th.nn.Parameter(th.nn.init.normal_(\n",
    "            th.empty(c_in, c_out, *_pair(k_size))\n",
    "        ))\n",
    "\n",
    "        self.use_bias = bias\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = th.nn.Parameter(th.FloatTensor(c_out).fill_(0))\n",
    "\n",
    "        fan_in = c_in  # value of fan_in for deconv\n",
    "        self.scale = sqrt(2) / sqrt(fan_in)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        forward pass of the layer\n",
    "        :param x: input\n",
    "        :return: y => output\n",
    "        \"\"\"\n",
    "        from torch.nn.functional import conv_transpose2d\n",
    "\n",
    "        return conv_transpose2d(input=x,\n",
    "                                weight=self.weight * self.scale,  # scale the weight on runtime\n",
    "                                bias=self.bias if self.use_bias else None,\n",
    "                                stride=self.stride,\n",
    "                                padding=self.pad)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return \", \".join(map(str, self.weight.shape))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Pixelwise feature vector normalization.\n",
    "# reference: https://github.com/tkarras/progressive_growing_of_gans/blob/master/networks.py#L120\n",
    "# ----------------------------------------------------------------------------\n",
    "class PixelwiseNorm(th.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PixelwiseNorm, self).__init__()\n",
    "\n",
    "    def forward(self, x, alpha=1e-8):\n",
    "        \"\"\"\n",
    "        forward pass of the module\n",
    "        :param x: input activations volume\n",
    "        :param alpha: small number for numerical stability\n",
    "        :return: y => pixel normalized activations\n",
    "        \"\"\"\n",
    "        y = x.pow(2.).mean(dim=1, keepdim=True).add(alpha).sqrt()  # [N1HW]\n",
    "        y = x / y  # normalize the input x volume\n",
    "        return y\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# Layers required for Building The generator and\n",
    "# discriminator\n",
    "# ==========================================================\n",
    "class GenInitialBlock(th.nn.Module):\n",
    "    \"\"\" Module implementing the initial block of the input \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels):\n",
    "        \"\"\"\n",
    "        constructor for the inner class\n",
    "        :param in_channels: number of input channels to the block\n",
    "        \"\"\"\n",
    "        from torch.nn import LeakyReLU\n",
    "\n",
    "        super(GenInitialBlock, self).__init__()\n",
    "\n",
    "        self.conv_1 = _equalized_deconv2d(in_channels, in_channels, (4, 4), bias=True,pad=1)\n",
    "        \n",
    "        self.conv_2 = _equalized_conv2d(in_channels, in_channels, (3, 3), pad=1, bias=True)\n",
    "\n",
    "        # Pixelwise feature vector normalization operation\n",
    "        self.pixNorm = PixelwiseNorm()\n",
    "\n",
    "        # leaky_relu:\n",
    "        self.lrelu = LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        forward pass of the block\n",
    "        :param x: input to the module\n",
    "        :return: y => output\n",
    "        \"\"\"\n",
    "        # convert the tensor shape:\n",
    "        y = th.unsqueeze(th.unsqueeze(x, -1), -1)\n",
    "\n",
    "        # perform the forward computations:\n",
    "        y = self.lrelu(self.conv_1(y))\n",
    "        y = self.lrelu(self.conv_2(y))\n",
    "\n",
    "        # apply pixel norm\n",
    "        y = self.pixNorm(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "class GenGeneralConvBlock(th.nn.Module):\n",
    "    \"\"\" Module implementing a general convolutional block \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        \"\"\"\n",
    "        constructor for the class\n",
    "        :param in_channels: number of input channels to the block\n",
    "        :param out_channels: number of output channels required\n",
    "        \"\"\"\n",
    "        from torch.nn import LeakyReLU\n",
    "        from torch.nn.functional import interpolate\n",
    "\n",
    "        super(GenGeneralConvBlock, self).__init__()\n",
    "\n",
    "        self.upsample = lambda x: interpolate(x, scale_factor=2)\n",
    "\n",
    "        self.conv_1 = _equalized_conv2d(in_channels, out_channels, (3, 3),\n",
    "                                        pad=1, bias=True)\n",
    "        self.conv_2 = _equalized_conv2d(out_channels, out_channels, (3, 3),\n",
    "                                        pad=1, bias=True)\n",
    "\n",
    "        # Pixelwise feature vector normalization operation\n",
    "        self.pixNorm = PixelwiseNorm()\n",
    "\n",
    "        # leaky_relu:\n",
    "        self.lrelu = LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        forward pass of the block\n",
    "        :param x: input\n",
    "        :return: y => output\n",
    "        \"\"\"\n",
    "        y = self.upsample(x)\n",
    "        y = self.pixNorm(self.lrelu(self.conv_1(y)))\n",
    "        y = self.pixNorm(self.lrelu(self.conv_2(y)))\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "# function to calculate the Exponential moving averages for the Generator weights\n",
    "# This function updates the exponential average weights based on the current training\n",
    "def update_average(model_tgt, model_src, beta):\n",
    "    \"\"\"\n",
    "    update the model_target using exponential moving averages\n",
    "    :param model_tgt: target model\n",
    "    :param model_src: source model\n",
    "    :param beta: value of decay beta\n",
    "    :return: None (updates the target model)\n",
    "    \"\"\"\n",
    "\n",
    "    # utility function for toggling the gradient requirements of the models\n",
    "    def toggle_grad(model, requires_grad):\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad_(requires_grad)\n",
    "\n",
    "    # turn off gradient calculation\n",
    "    toggle_grad(model_tgt, False)\n",
    "    toggle_grad(model_src, False)\n",
    "\n",
    "    param_dict_src = dict(model_src.named_parameters())\n",
    "\n",
    "    for p_name, p_tgt in model_tgt.named_parameters():\n",
    "        p_src = param_dict_src[p_name]\n",
    "        assert (p_src is not p_tgt)\n",
    "        p_tgt.copy_(beta * p_tgt + (1. - beta) * p_src)\n",
    "\n",
    "    # turn back on the gradient calculation\n",
    "    toggle_grad(model_tgt, True)\n",
    "    toggle_grad(model_src, True)\n",
    "\n",
    "\n",
    "class MinibatchStdDev(th.nn.Module):\n",
    "    \"\"\"\n",
    "    Minibatch standard deviation layer for the discriminator\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        derived class constructor\n",
    "        \"\"\"\n",
    "        super(MinibatchStdDev, self).__init__()\n",
    "\n",
    "    def forward(self, x, alpha=1e-8):\n",
    "        \"\"\"\n",
    "        forward pass of the layer\n",
    "        :param x: input activation volume\n",
    "        :param alpha: small number for numerical stability\n",
    "        :return: y => x appended with standard deviation constant map\n",
    "        \"\"\"\n",
    "        batch_size, _, height, width = x.shape\n",
    "\n",
    "        # [B x C x H x W] Subtract mean over batch.\n",
    "        y = x - x.mean(dim=0, keepdim=True)\n",
    "\n",
    "        # [1 x C x H x W]  Calc standard deviation over batch\n",
    "        y = th.sqrt(y.pow(2.).mean(dim=0, keepdim=False) + alpha)\n",
    "\n",
    "        # [1]  Take average over feature_maps and pixels.\n",
    "        y = y.mean().view(1, 1, 1, 1)\n",
    "\n",
    "        # [B x 1 x H x W]  Replicate over group and pixels.\n",
    "        y = y.repeat(batch_size, 1, height, width)\n",
    "\n",
    "        # [B x C x H x W]  Append as new feature_map.\n",
    "        y = th.cat([x, y], 1)\n",
    "\n",
    "        # return the computed values:\n",
    "        return y\n",
    "\n",
    "\n",
    "class DisFinalBlock(th.nn.Module):\n",
    "    \"\"\" Final block for the Discriminator \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels):\n",
    "        \"\"\"\n",
    "        constructor of the class\n",
    "        :param in_channels: number of input channels\n",
    "        \"\"\"\n",
    "        from torch.nn import LeakyReLU\n",
    "\n",
    "        super(DisFinalBlock, self).__init__()\n",
    "\n",
    "        # declare the required modules for forward pass\n",
    "        self.batch_discriminator = MinibatchStdDev()\n",
    "\n",
    "        self.conv_1 = _equalized_conv2d(in_channels + 1, in_channels, (3, 3), pad=1, bias=True)\n",
    "        self.conv_2 = _equalized_conv2d(in_channels, in_channels, (4, 4), pad=1, bias=True)\n",
    "        # final conv layer emulates a fully connected layer\n",
    "        self.conv_3 = _equalized_conv2d(in_channels, 1, (1, 1), bias=True)\n",
    "\n",
    "        # leaky_relu:\n",
    "        self.lrelu = LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        forward pass of the FinalBlock\n",
    "        :param x: input\n",
    "        :return: y => output\n",
    "        \"\"\"\n",
    "        # minibatch_std_dev layer\n",
    "        y = self.batch_discriminator(x)\n",
    "\n",
    "        # define the computations\n",
    "        y = self.lrelu(self.conv_1(y))\n",
    "        y = self.lrelu(self.conv_2(y))\n",
    "\n",
    "        # fully connected layer\n",
    "        y = self.conv_3(y)  # This layer has linear activation\n",
    "\n",
    "        # flatten the output raw discriminator scores\n",
    "        return y.view(-1)\n",
    "\n",
    "\n",
    "\n",
    "class DisGeneralConvBlock(th.nn.Module):\n",
    "    \"\"\" General block in the discriminator  \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        \"\"\"\n",
    "        constructor of the class\n",
    "        :param in_channels: number of input channels\n",
    "        :param out_channels: number of output channels\n",
    "        \"\"\"\n",
    "        from torch.nn import AvgPool2d, LeakyReLU\n",
    "\n",
    "        super(DisGeneralConvBlock, self).__init__()\n",
    "\n",
    "        self.conv_1 = _equalized_conv2d(in_channels, in_channels, (3, 3), pad=1, bias=True)\n",
    "        self.conv_2 = _equalized_conv2d(in_channels, out_channels, (3, 3), pad=1, bias=True)\n",
    "\n",
    "        self.downSampler = AvgPool2d(2)\n",
    "\n",
    "        # leaky_relu:\n",
    "        self.lrelu = LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        forward pass of the module\n",
    "        :param x: input\n",
    "        :return: y => output\n",
    "        \"\"\"\n",
    "        # define the computations\n",
    "        y = self.lrelu(self.conv_1(x))\n",
    "        y = self.lrelu(self.conv_2(y))\n",
    "        y = self.downSampler(y)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9166,
     "status": "ok",
     "timestamp": 1593958151751,
     "user": {
      "displayName": "Manos Panagiotou",
      "photoUrl": "",
      "userId": "06697745822174955863"
     },
     "user_tz": -180
    },
    "id": "Bqld9yfzHSh-"
   },
   "outputs": [],
   "source": [
    "\"\"\" Module implementing GAN which will be trained using the Progressive growing\n",
    "    technique -> https://arxiv.org/abs/1710.10196\n",
    "\"\"\"\n",
    "import datetime\n",
    "import os\n",
    "import time\n",
    "import timeit\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch as th\n",
    "\n",
    "\n",
    "# ========================================================================================\n",
    "# Generator Module\n",
    "# can be used with ProGAN, ConditionalProGAN or standalone (for inference)\n",
    "# ========================================================================================\n",
    "\n",
    "class Generator(th.nn.Module):\n",
    "    \"\"\" Generator of the GAN network \"\"\"\n",
    "\n",
    "    def __init__(self, depth, latent_size):\n",
    "        \"\"\"\n",
    "        constructor for the Generator class\n",
    "        :param depth: required depth of the Network\n",
    "        :param latent_size: size of the latent manifold\n",
    "        \"\"\"\n",
    "        from torch.nn import ModuleList\n",
    "        from torch.nn.functional import interpolate\n",
    "\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        assert latent_size != 0 and ((latent_size & (latent_size - 1)) == 0), \\\n",
    "            \"latent size not a power of 2\"\n",
    "        if depth >= 4:\n",
    "            assert latent_size >= np.power(2, depth - 4), \"latent size will diminish to zero\"\n",
    "\n",
    "        # state of the generator:\n",
    "        self.depth = depth\n",
    "        self.latent_size = latent_size\n",
    "\n",
    "        # register the modules required for the GAN\n",
    "        self.initial_block = GenInitialBlock(self.latent_size)\n",
    "\n",
    "        # create a module list of the other required general convolution blocks\n",
    "        self.layers = ModuleList([])  # initialize to empty list\n",
    "\n",
    "        # create the ToRGB layers for various outputs:\n",
    "        self.toRGB = lambda in_channels: \\\n",
    "            _equalized_conv2d(in_channels, 3, (1, 1), bias=True)\n",
    "\n",
    "        self.rgb_converters = ModuleList([self.toRGB(self.latent_size)])\n",
    "\n",
    "        # create the remaining layers\n",
    "        for i in range(self.depth - 1):\n",
    "            if i <= 2:\n",
    "                layer = GenGeneralConvBlock(self.latent_size,\n",
    "                                            self.latent_size)\n",
    "                rgb = self.toRGB(self.latent_size)\n",
    "            else:\n",
    "                layer = GenGeneralConvBlock(\n",
    "                    int(self.latent_size // np.power(2, i - 3)),\n",
    "                    int(self.latent_size // np.power(2, i - 2))\n",
    "                )\n",
    "                rgb = self.toRGB(int(self.latent_size // np.power(2, i - 2)))\n",
    "            self.layers.append(layer)\n",
    "            self.rgb_converters.append(rgb)\n",
    "\n",
    "        # register the temporary upsampler\n",
    "        self.temporaryUpsampler = lambda x: interpolate(x, scale_factor=2)\n",
    "\n",
    "    def forward(self, x, depth, alpha):\n",
    "        \"\"\"\n",
    "        forward pass of the Generator\n",
    "        :param x: input noise\n",
    "        :param depth: current depth from where output is required\n",
    "        :param alpha: value of alpha for fade-in effect\n",
    "        :return: y => output\n",
    "        \"\"\"\n",
    "\n",
    "        assert depth < self.depth, \"Requested output depth cannot be produced\"\n",
    "\n",
    "        y = self.initial_block(x)\n",
    "\n",
    "        if depth > 0:\n",
    "            for block in self.layers[:depth - 1]:\n",
    "                y = block(y)\n",
    "\n",
    "            residual = self.rgb_converters[depth - 1](self.temporaryUpsampler(y))\n",
    "            straight = self.rgb_converters[depth](self.layers[depth - 1](y))\n",
    "\n",
    "            out = (alpha * straight) + ((1 - alpha) * residual)\n",
    "\n",
    "        else:\n",
    "            out = self.rgb_converters[0](y)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "# ========================================================================================\n",
    "# Discriminator Module\n",
    "# can be used with ProGAN or standalone (for inference).\n",
    "# Note this cannot be used with ConditionalProGAN\n",
    "# ========================================================================================\n",
    "\n",
    "class Discriminator(th.nn.Module):\n",
    "    \"\"\" Discriminator of the GAN \"\"\"\n",
    "\n",
    "    def __init__(self, height, feature_size):\n",
    "        \"\"\"\n",
    "        constructor for the class\n",
    "        :param height: total height of the discriminator (Must be equal to the Generator depth)\n",
    "        :param feature_size: size of the deepest features extracted\n",
    "                             (Must be equal to Generator latent_size)\n",
    "        \"\"\"\n",
    "        from torch.nn import ModuleList, AvgPool2d\n",
    "\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        assert feature_size != 0 and ((feature_size & (feature_size - 1)) == 0), \\\n",
    "            \"latent size not a power of 2\"\n",
    "        if height >= 4:\n",
    "            assert feature_size >= np.power(2, height - 4), \"feature size cannot be produced\"\n",
    "\n",
    "        # create state of the object\n",
    "        self.height = height\n",
    "        self.feature_size = feature_size\n",
    "\n",
    "        self.final_block = DisFinalBlock(self.feature_size)\n",
    "\n",
    "        # create a module list of the other required general convolution blocks\n",
    "        self.layers = ModuleList([])  # initialize to empty list\n",
    "\n",
    "        # create the fromRGB layers for various inputs:\n",
    "        self.fromRGB = lambda out_channels: \\\n",
    "            _equalized_conv2d(3, out_channels, (1, 1), bias=True)\n",
    "        self.rgb_to_features = ModuleList([self.fromRGB(self.feature_size)])\n",
    "\n",
    "        # create the remaining layers\n",
    "        for i in range(self.height - 1):\n",
    "            if i > 2:\n",
    "                layer = DisGeneralConvBlock(\n",
    "                    int(self.feature_size // np.power(2, i - 2)),\n",
    "                    int(self.feature_size // np.power(2, i - 3))\n",
    "                )\n",
    "                rgb = self.fromRGB(int(self.feature_size // np.power(2, i - 2)))\n",
    "            else:\n",
    "                layer = DisGeneralConvBlock(self.feature_size,\n",
    "                                            self.feature_size)\n",
    "                rgb = self.fromRGB(self.feature_size)\n",
    "\n",
    "            self.layers.append(layer)\n",
    "            self.rgb_to_features.append(rgb)\n",
    "\n",
    "        # register the temporary downSampler\n",
    "        self.temporaryDownsampler = AvgPool2d(2)\n",
    "\n",
    "    def forward(self, x, height, alpha):\n",
    "        \"\"\"\n",
    "        forward pass of the discriminator\n",
    "        :param x: input to the network\n",
    "        :param height: current height of operation (Progressive GAN)\n",
    "        :param alpha: current value of alpha for fade-in\n",
    "        :return: out => raw prediction values (WGAN-GP)\n",
    "        \"\"\"\n",
    "\n",
    "        assert height < self.height, \"Requested output depth cannot be produced\"\n",
    "\n",
    "        if height > 0:\n",
    "            residual = self.rgb_to_features[height - 1](self.temporaryDownsampler(x))\n",
    "\n",
    "            straight = self.layers[height - 1](\n",
    "                self.rgb_to_features[height](x)\n",
    "            )\n",
    "\n",
    "            y = (alpha * straight) + ((1 - alpha) * residual)\n",
    "\n",
    "            for block in reversed(self.layers[:height - 1]):\n",
    "                y = block(y)\n",
    "        else:\n",
    "            y = self.rgb_to_features[0](x)\n",
    "\n",
    "        out = self.final_block(y)\n",
    "\n",
    "        return out\n",
    "\n",
    "# ========================================================================================\n",
    "# ProGAN Module (Unconditional)\n",
    "# ========================================================================================\n",
    "\n",
    "class ProGAN:\n",
    "    \"\"\" Wrapper around the Generator and the Discriminator \"\"\"\n",
    "\n",
    "    def __init__(self, depth=7, latent_size=512, learning_rate=0.001, beta_1=0,\n",
    "                 beta_2=0.99, eps=1e-8, drift=0.001, n_critic=1,\n",
    "                 use_ema=True, ema_decay=0.999,\n",
    "                 device=th.device(\"cpu\")):\n",
    "        \"\"\"\n",
    "        constructor for the class\n",
    "        :param depth: depth of the GAN (will be used for each generator and discriminator)\n",
    "        :param latent_size: latent size of the manifold used by the GAN\n",
    "        :param learning_rate: learning rate for Adam\n",
    "        :param beta_1: beta_1 for Adam\n",
    "        :param beta_2: beta_2 for Adam\n",
    "        :param eps: epsilon for Adam\n",
    "        :param n_critic: number of times to update discriminator per generator update\n",
    "        :param drift: drift penalty for the\n",
    "                      (Used only if loss is wgan or wgan-gp)\n",
    "        :param use_ema: boolean for whether to use exponential moving averages\n",
    "        :param ema_decay: value of mu for ema\n",
    "        :param device: device to run the GAN on (GPU / CPU)\n",
    "        \"\"\"\n",
    "\n",
    "        from torch.optim import Adam\n",
    "        from torch.nn import DataParallel\n",
    "\n",
    "        # Create the Generator and the Discriminator\n",
    "        self.gen = Generator(depth, latent_size).to(device)\n",
    "        self.dis = Discriminator(depth, latent_size).to(device)\n",
    "\n",
    "        # if code is to be run on GPU, we can use DataParallel:\n",
    "        if device == th.device(\"cuda\"):\n",
    "            self.gen = DataParallel(self.gen)\n",
    "            self.dis = DataParallel(self.dis)\n",
    "\n",
    "        # state of the object\n",
    "        self.latent_size = latent_size\n",
    "        self.depth = depth\n",
    "        self.use_ema = use_ema\n",
    "        self.ema_decay = ema_decay\n",
    "        self.n_critic = n_critic\n",
    "        self.device = device\n",
    "        self.drift = drift\n",
    "\n",
    "        # define the optimizers for the discriminator and generator\n",
    "        self.gen_optim = Adam(self.gen.parameters(), lr=learning_rate,\n",
    "                              betas=(beta_1, beta_2), eps=eps)\n",
    "\n",
    "        self.dis_optim = Adam(self.dis.parameters(), lr=learning_rate,\n",
    "                              betas=(beta_1, beta_2), eps=eps)\n",
    "\n",
    "        # define the loss function used for training the GAN\n",
    "        self.loss = WGAN_GP(self.dis, self.drift, use_gp=True)\n",
    "\n",
    "        if self.use_ema:\n",
    "\n",
    "            # create a shadow copy of the generator\n",
    "            self.gen_shadow = copy.deepcopy(self.gen)\n",
    "\n",
    "            # updater function:\n",
    "            self.ema_updater = update_average\n",
    "\n",
    "            # initialize the gen_shadow weights equal to the\n",
    "            # weights of gen\n",
    "            self.ema_updater(self.gen_shadow, self.gen, beta=0)\n",
    "\n",
    "\n",
    "    def __progressive_downsampling(self, real_batch, depth, alpha):\n",
    "        \"\"\"\n",
    "        private helper for downsampling the original images in order to facilitate the\n",
    "        progressive growing of the layers.\n",
    "        :param real_batch: batch of real samples\n",
    "        :param depth: depth at which training is going on\n",
    "        :param alpha: current value of the fader alpha\n",
    "        :return: real_samples => modified real batch of samples\n",
    "        \"\"\"\n",
    "\n",
    "        from torch.nn import AvgPool2d\n",
    "        from torch.nn.functional import interpolate\n",
    "\n",
    "        # downsample the real_batch for the given depth\n",
    "        down_sample_factor = int(np.power(2, self.depth - depth - 1))\n",
    "        prior_downsample_factor = max(int(np.power(2, self.depth - depth)), 0)\n",
    "        ds_real_samples = AvgPool2d(down_sample_factor)(real_batch)\n",
    "\n",
    "        if depth > 0:\n",
    "            prior_ds_real_samples = interpolate(AvgPool2d(prior_downsample_factor)(real_batch),\n",
    "                                                scale_factor=2)\n",
    "        else:\n",
    "            prior_ds_real_samples = ds_real_samples\n",
    "\n",
    "        # real samples are a combination of ds_real_samples and prior_ds_real_samples\n",
    "        real_samples = (alpha * ds_real_samples) + ((1 - alpha) * prior_ds_real_samples)\n",
    "\n",
    "        # return the so computed real_samples\n",
    "        return real_samples\n",
    "\n",
    "    def optimize_discriminator(self, noise, real_batch, depth, alpha):\n",
    "        \"\"\"\n",
    "        performs one step of weight update on discriminator using the batch of data\n",
    "        :param noise: input noise of sample generation\n",
    "        :param real_batch: real samples batch\n",
    "        :param depth: current depth of optimization\n",
    "        :param alpha: current alpha for fade-in\n",
    "        :return: current loss (Wasserstein loss)\n",
    "        \"\"\"\n",
    "\n",
    "        real_samples = self.__progressive_downsampling(real_batch, depth, alpha)\n",
    "\n",
    "        loss_val = 0\n",
    "        for _ in range(self.n_critic):\n",
    "            # generate a batch of samples\n",
    "            fake_samples = self.gen(noise, depth, alpha).detach()\n",
    "            \n",
    "            loss = self.loss.dis_loss(real_samples, fake_samples, depth, alpha)\n",
    "\n",
    "            # optimize discriminator\n",
    "            self.dis_optim.zero_grad()\n",
    "            loss.backward()\n",
    "            self.dis_optim.step()\n",
    "\n",
    "            loss_val += loss.item()\n",
    "\n",
    "        return loss_val / self.n_critic\n",
    "\n",
    "    def optimize_generator(self, noise, real_batch, depth, alpha):\n",
    "        \"\"\"\n",
    "        performs one step of weight update on generator for the given batch_size\n",
    "        :param noise: input random noise required for generating samples\n",
    "        :param real_batch: batch of real samples\n",
    "        :param depth: depth of the network at which optimization is done\n",
    "        :param alpha: value of alpha for fade-in effect\n",
    "        :return: current loss (Wasserstein estimate)\n",
    "        \"\"\"\n",
    "\n",
    "        real_samples = self.__progressive_downsampling(real_batch, depth, alpha)\n",
    "\n",
    "        # generate fake samples:\n",
    "        fake_samples = self.gen(noise, depth, alpha)\n",
    "\n",
    "        # TODO_complete:\n",
    "        # Change this implementation for making it compatible for relativisticGAN\n",
    "        loss = self.loss.gen_loss(real_samples, fake_samples, depth, alpha)\n",
    "\n",
    "        # optimize the generator\n",
    "        self.gen_optim.zero_grad()\n",
    "        loss.backward()\n",
    "        self.gen_optim.step()\n",
    "\n",
    "        # if use_ema is true, apply ema to the generator parameters\n",
    "        if self.use_ema:\n",
    "            self.ema_updater(self.gen_shadow, self.gen, self.ema_decay)\n",
    "\n",
    "        # return the loss value\n",
    "        return loss.item()\n",
    "\n",
    "    @staticmethod\n",
    "    def create_grid(samples, scale_factor, img_file):\n",
    "        \"\"\"\n",
    "        utility function to create a grid of GAN samples\n",
    "        :param samples: generated samples for storing\n",
    "        :param scale_factor: factor for upscaling the image\n",
    "        :param img_file: name of file to write\n",
    "        :return: None (saves a file)\n",
    "        \"\"\"\n",
    "        from torchvision.utils import save_image\n",
    "        from torch.nn.functional import interpolate\n",
    "\n",
    "        # upsample the image\n",
    "        if scale_factor > 1:\n",
    "            samples = interpolate(samples, scale_factor=scale_factor)\n",
    "\n",
    "        # save the images:\n",
    "        save_image(samples, img_file, nrow=int(np.sqrt(len(samples))),\n",
    "                   normalize=True, scale_each=True)\n",
    "\n",
    "    def train(self, dataset, epochs, batch_sizes,\n",
    "              fade_in_percentage, num_samples=16,\n",
    "              start_depth=0, start_epoch=1, num_workers=3, feedback_factor=100,\n",
    "              log_dir=\"./models/\", sample_dir=\"./samples/\", save_dir=\"./models/\",\n",
    "              checkpoint_factor=1):\n",
    "        \"\"\"\n",
    "        Utility method for training the ProGAN. Note that you don't have to necessarily use this\n",
    "        you can use the optimize_generator and optimize_discriminator for your own training routine.\n",
    "        :param dataset: object of the dataset used for training.\n",
    "                        Note that this is not the dataloader (we create dataloader in this method\n",
    "                        since the batch_sizes for resolutions can be different)\n",
    "        :param epochs: list of number of epochs to train the network for every resolution\n",
    "        :param batch_sizes: list of batch_sizes for every resolution\n",
    "        :param fade_in_percentage: list of percentages of epochs per resolution\n",
    "                                   used for fading in the new layer\n",
    "                                   not used for first resolution, but dummy value still needed.\n",
    "        :param num_samples: number of samples generated in sample_sheet. def=36\n",
    "        :param start_depth: start training from this depth. def=0\n",
    "        :param num_workers: number of workers for reading the data. def=3\n",
    "        :param feedback_factor: number of logs per epoch. def=100\n",
    "        :param log_dir: directory for saving the loss logs. def=\"./models/\"\n",
    "        :param sample_dir: directory for saving the generated samples. def=\"./samples/\"\n",
    "        :param checkpoint_factor: save model after these many epochs.\n",
    "                                  Note that only one model is stored per resolution.\n",
    "                                  during one resolution, the checkpoint will be updated (Rewritten)\n",
    "                                  according to this factor.\n",
    "        :param save_dir: directory for saving the models (.pth files)\n",
    "        :return: None (Writes multiple files to disk)\n",
    "        \"\"\"\n",
    "\n",
    "        assert self.depth == len(batch_sizes), \"batch_sizes not compatible with depth\"\n",
    "\n",
    "        #RELOAD\n",
    "        gen_save_file = os.path.join(save_dir, \"GAN_GEN_\" + str(start_depth) + \".pth\")\n",
    "        dis_save_file = os.path.join(save_dir, \"GAN_DIS_\" + str(start_depth) + \".pth\")\n",
    "        gen_optim_save_file = os.path.join(save_dir,\n",
    "                                            \"GAN_GEN_OPTIM_\" + str(start_depth)\n",
    "                                            + \".pth\")\n",
    "        dis_optim_save_file = os.path.join(save_dir,\n",
    "                                            \"GAN_DIS_OPTIM_\" + str(start_depth)\n",
    "                                            + \".pth\")\n",
    "        \n",
    "        #self.gen.load_state_dict(th.load(gen_save_file))\n",
    "        #self.dis.load_state_dict(th.load(dis_save_file))\n",
    "        #self.gen_optim.load_state_dict(th.load(gen_optim_save_file))\n",
    "        #self.dis_optim.load_state_dict(th.load(dis_optim_save_file))\n",
    "\n",
    "        # also save the shadow generator if use_ema is True\n",
    "        if self.use_ema:\n",
    "            gen_shadow_save_file = os.path.join(save_dir, \"GAN_GEN_SHADOW_\" +\n",
    "                                                str(start_depth) + \".pth\")\n",
    "            #self.gen_shadow.load_state_dict(th.load(gen_shadow_save_file))\n",
    "        #RELOAD\n",
    "\n",
    "        # turn the generator and discriminator into train mode\n",
    "        self.gen.train()\n",
    "        self.dis.train()\n",
    "        if self.use_ema:\n",
    "            self.gen_shadow.train()\n",
    "\n",
    "        # create a global time counter\n",
    "        global_time = time.time()\n",
    "\n",
    "        # create fixed_input for debugging\n",
    "        fixed_input = th.randn(num_samples, self.latent_size).to(self.device)\n",
    "\n",
    "        print(\"Starting the training process ... \")\n",
    "        for current_depth in range(start_depth, self.depth):\n",
    "\n",
    "\n",
    "            print(\"\\n\\nCurrently working on Depth: \", current_depth)\n",
    "            current_res = np.power(2, current_depth + 1)\n",
    "            print(\"Current resolution: %d x %d\" % (current_res, current_res))\n",
    "            \n",
    "            if current_depth != START_DEPTH:\n",
    "                start_epoch=1\n",
    "            data = get_data_loader(dataset, batch_sizes[current_depth], num_workers)\n",
    "            if (start_epoch > 1): ticker = len(range(1,start_epoch))* len(iter(data))\n",
    "            else: ticker = 1\n",
    "            print(\"Ticker\", ticker)\n",
    "            for epoch in range(start_epoch, epochs[current_depth] + 1):\n",
    "                start = timeit.default_timer()  # record time at the start of epoch\n",
    "\n",
    "                print(\"\\nEpoch: %d\" % epoch)\n",
    "                total_batches = len(iter(data))\n",
    "\n",
    "                fader_point = int((fade_in_percentage[current_depth] / 100)\n",
    "                                  * epochs[current_depth] * total_batches)\n",
    "\n",
    "                step = 0  # counter for number of iterations\n",
    "\n",
    "                for (i, batch) in enumerate(data, 1):\n",
    "                    # calculate the alpha for fading in the layers\n",
    "                    alpha = ticker / fader_point if ticker <= fader_point else 1\n",
    "\n",
    "                    # extract current batch of data for training\n",
    "                    images = batch[0].to(self.device)\n",
    "\n",
    "                    gan_input = th.randn(images.shape[0], self.latent_size).to(self.device)\n",
    "\n",
    "                    # optimize the discriminator:\n",
    "                    dis_loss = self.optimize_discriminator(gan_input, images,\n",
    "                                                           current_depth, alpha)\n",
    "\n",
    "                    # optimize the generator:\n",
    "                    gen_loss = self.optimize_generator(gan_input, images, current_depth, alpha)\n",
    "\n",
    "                    # provide a loss feedback\n",
    "                    if i % int(total_batches / feedback_factor) == 0 or i == 1:\n",
    "                        elapsed = time.time() - global_time\n",
    "                        elapsed = str(datetime.timedelta(seconds=elapsed))\n",
    "                        print(\"Elapsed: [%s]  batch: %d  d_loss: %f  g_loss: %f\"\n",
    "                              % (elapsed, i, dis_loss, gen_loss))\n",
    "\n",
    "                        # also write the losses to the log file:\n",
    "                        os.makedirs(log_dir, exist_ok=True)\n",
    "                        log_file = os.path.join(log_dir, \"loss_\" + str(current_depth) + \".log\")\n",
    "                        with open(log_file, \"a\") as log:\n",
    "                            log.write(str(step) + \"\\t\" + str(dis_loss) +\n",
    "                                      \"\\t\" + str(gen_loss) + \"\\n\")\n",
    "                    # increment the alpha ticker and the step\n",
    "                    ticker += 1\n",
    "                    step += 1\n",
    "                # create a grid of samples and save it\n",
    "                os.makedirs(sample_dir, exist_ok=True)\n",
    "                gen_img_file = os.path.join(sample_dir, \"gen_\" + str(current_depth) +\n",
    "                                            \"_\" + str(epoch) + \"_\" +\n",
    "                                            str(i) + \".png\")\n",
    "\n",
    "                # this is done to allow for more GPU space\n",
    "                with th.no_grad():\n",
    "                    self.create_grid(\n",
    "                        samples=self.gen(\n",
    "                            fixed_input,\n",
    "                            current_depth,\n",
    "                            alpha\n",
    "                        ).detach() if not self.use_ema\n",
    "                        else self.gen_shadow(\n",
    "                            fixed_input,\n",
    "                            current_depth,\n",
    "                            alpha\n",
    "                        ).detach(),\n",
    "                        scale_factor=int(np.power(2, self.depth - current_depth - 1)),\n",
    "                        img_file=gen_img_file,\n",
    "                    )\n",
    "\n",
    "                stop = timeit.default_timer()\n",
    "                print(\"Time taken for epoch: %.3f secs\" % (stop - start))\n",
    "                print(\"ticker = \", ticker)\n",
    "                if epoch % checkpoint_factor == 0 or epoch == 1 or epoch == epochs[current_depth]:\n",
    "                    os.makedirs(save_dir, exist_ok=True)\n",
    "                    gen_save_file = os.path.join(save_dir, \"GAN_GEN_\" + str(current_depth) + \".pth\")\n",
    "                    dis_save_file = os.path.join(save_dir, \"GAN_DIS_\" + str(current_depth) + \".pth\")\n",
    "                    gen_optim_save_file = os.path.join(save_dir,\n",
    "                                                       \"GAN_GEN_OPTIM_\" + str(current_depth)\n",
    "                                                       + \".pth\")\n",
    "                    dis_optim_save_file = os.path.join(save_dir,\n",
    "                                                       \"GAN_DIS_OPTIM_\" + str(current_depth)\n",
    "                                                       + \".pth\")\n",
    "\n",
    "                    th.save(self.gen.state_dict(), gen_save_file)\n",
    "                    th.save(self.dis.state_dict(), dis_save_file)\n",
    "                    th.save(self.gen_optim.state_dict(), gen_optim_save_file)\n",
    "                    th.save(self.dis_optim.state_dict(), dis_optim_save_file)\n",
    "\n",
    "                    # also save the shadow generator if use_ema is True\n",
    "                    if self.use_ema:\n",
    "                        gen_shadow_save_file = os.path.join(save_dir, \"GAN_GEN_SHADOW_\" +\n",
    "                                                            str(current_depth) + \".pth\")\n",
    "                        th.save(self.gen_shadow.state_dict(), gen_shadow_save_file)\n",
    "\n",
    "        # put the gen, shadow_gen and dis in eval mode\n",
    "        self.gen.eval()\n",
    "        self.dis.eval()\n",
    "        if self.use_ema:\n",
    "            self.gen_shadow.eval()\n",
    "\n",
    "        print(\"Training completed ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torchvision as tv\n",
    "from torch.utils.data import TensorDataset\n",
    "# select the device to be used for training\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8760582,
     "status": "error",
     "timestamp": 1593966903198,
     "user": {
      "displayName": "Manos Panagiotou",
      "photoUrl": "",
      "userId": "06697745822174955863"
     },
     "user_tz": -180
    },
    "id": "z3L3ciAgDP1a",
    "outputId": "0cc0cdc2-0fbe-472e-85c5-86968ae7a4e4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ./training_images\n",
      "    StandardTransform\n",
      "Transform: ToTensor()\n",
      "[torch.Size([3, 128, 128])]\n",
      "Starting the training process ... \n",
      "\n",
      "\n",
      "Currently working on Depth:  0\n",
      "Current resolution: 2 x 2\n",
      "Ticker 1\n",
      "\n",
      "Epoch: 1\n",
      "Elapsed: [0:00:07.953917]  batch: 1  d_loss: 1.358527  g_loss: -1.025170\n",
      "Elapsed: [0:00:09.863514]  batch: 50  d_loss: -1.375130  g_loss: 2.600798\n",
      "Elapsed: [0:00:11.821596]  batch: 100  d_loss: -1.971123  g_loss: 1.174121\n",
      "Elapsed: [0:00:13.784848]  batch: 150  d_loss: -2.257158  g_loss: 1.994805\n",
      "Elapsed: [0:00:15.757812]  batch: 200  d_loss: -1.753736  g_loss: 1.485612\n",
      "Elapsed: [0:00:17.797737]  batch: 250  d_loss: -1.023956  g_loss: 0.887000\n",
      "Elapsed: [0:00:19.919987]  batch: 300  d_loss: -0.871387  g_loss: 0.625090\n",
      "Elapsed: [0:00:22.258585]  batch: 350  d_loss: -0.630591  g_loss: 0.497270\n",
      "Elapsed: [0:00:24.261093]  batch: 400  d_loss: -0.550776  g_loss: 0.436087\n",
      "Elapsed: [0:00:26.388425]  batch: 450  d_loss: -0.312255  g_loss: 0.195266\n",
      "Elapsed: [0:00:28.353554]  batch: 500  d_loss: -0.214257  g_loss: 0.759075\n",
      "Time taken for epoch: 28.624 secs\n",
      "ticker =  501\n",
      "\n",
      "Epoch: 2\n",
      "Elapsed: [0:00:38.968005]  batch: 1  d_loss: -0.310337  g_loss: 0.638691\n",
      "Elapsed: [0:00:40.949204]  batch: 50  d_loss: -0.209989  g_loss: 0.274982\n",
      "Elapsed: [0:00:42.885105]  batch: 100  d_loss: -0.223711  g_loss: 0.315233\n",
      "Elapsed: [0:00:44.926115]  batch: 150  d_loss: -0.153221  g_loss: 0.180263\n",
      "Elapsed: [0:00:47.065528]  batch: 200  d_loss: -0.224153  g_loss: 0.738040\n",
      "Elapsed: [0:00:49.087806]  batch: 250  d_loss: -0.211022  g_loss: -0.134499\n",
      "Elapsed: [0:00:51.086008]  batch: 300  d_loss: -0.252067  g_loss: 0.206354\n",
      "Elapsed: [0:00:53.074295]  batch: 350  d_loss: -0.178619  g_loss: 0.178539\n",
      "Elapsed: [0:00:55.012563]  batch: 400  d_loss: -0.026302  g_loss: 0.355770\n",
      "Elapsed: [0:00:57.459868]  batch: 450  d_loss: -0.250545  g_loss: 0.303252\n",
      "Elapsed: [0:00:59.366382]  batch: 500  d_loss: -0.237009  g_loss: 0.449084\n",
      "Time taken for epoch: 30.935 secs\n",
      "ticker =  1001\n",
      "\n",
      "Epoch: 3\n",
      "Elapsed: [0:01:08.822043]  batch: 1  d_loss: -0.203290  g_loss: 0.404521\n",
      "Elapsed: [0:01:10.751980]  batch: 50  d_loss: -0.263471  g_loss: 0.472717\n",
      "Elapsed: [0:01:12.831159]  batch: 100  d_loss: -0.212065  g_loss: 0.291052\n",
      "Elapsed: [0:01:14.768182]  batch: 150  d_loss: -0.089122  g_loss: 0.090618\n",
      "Elapsed: [0:01:16.765617]  batch: 200  d_loss: -0.092060  g_loss: 0.309834\n",
      "Elapsed: [0:01:18.676700]  batch: 250  d_loss: -0.098487  g_loss: 0.369931\n",
      "Elapsed: [0:01:20.654555]  batch: 300  d_loss: -0.007813  g_loss: 0.126544\n",
      "Elapsed: [0:01:22.628683]  batch: 350  d_loss: 0.170052  g_loss: -0.145721\n",
      "Elapsed: [0:01:24.545054]  batch: 400  d_loss: -0.058965  g_loss: 0.243605\n",
      "Elapsed: [0:01:26.513555]  batch: 450  d_loss: 0.091707  g_loss: -0.026399\n",
      "Elapsed: [0:01:28.360504]  batch: 500  d_loss: 0.123842  g_loss: 0.039884\n",
      "Time taken for epoch: 28.910 secs\n",
      "ticker =  1501\n",
      "\n",
      "Epoch: 4\n",
      "Elapsed: [0:01:37.740151]  batch: 1  d_loss: 0.138706  g_loss: 0.154907\n",
      "Elapsed: [0:01:39.789592]  batch: 50  d_loss: 0.254206  g_loss: -0.111342\n",
      "Elapsed: [0:01:41.961150]  batch: 100  d_loss: -0.000241  g_loss: 0.037252\n",
      "Elapsed: [0:01:43.961161]  batch: 150  d_loss: 0.054058  g_loss: 0.096517\n",
      "Elapsed: [0:01:45.909497]  batch: 200  d_loss: 0.195191  g_loss: -0.044458\n",
      "Elapsed: [0:01:47.843657]  batch: 250  d_loss: 0.122796  g_loss: 0.331856\n",
      "Elapsed: [0:01:49.689039]  batch: 300  d_loss: 0.083010  g_loss: 0.040868\n",
      "Elapsed: [0:01:51.713196]  batch: 350  d_loss: 0.103612  g_loss: 0.220826\n",
      "Elapsed: [0:01:53.676996]  batch: 400  d_loss: 0.139307  g_loss: -0.081214\n",
      "Elapsed: [0:01:55.710156]  batch: 450  d_loss: -0.014591  g_loss: 0.151122\n",
      "Elapsed: [0:01:57.666397]  batch: 500  d_loss: -0.179935  g_loss: 0.080510\n",
      "Time taken for epoch: 29.258 secs\n",
      "ticker =  2001\n",
      "\n",
      "Epoch: 5\n",
      "Elapsed: [0:02:07.201183]  batch: 1  d_loss: -0.046247  g_loss: 0.076636\n",
      "Elapsed: [0:02:09.218119]  batch: 50  d_loss: 0.201159  g_loss: -0.367252\n",
      "Elapsed: [0:02:11.301571]  batch: 100  d_loss: -0.060725  g_loss: -0.058484\n",
      "Elapsed: [0:02:13.225620]  batch: 150  d_loss: 0.128291  g_loss: -0.094892\n",
      "Elapsed: [0:02:15.195349]  batch: 200  d_loss: 0.051803  g_loss: 0.016033\n",
      "Elapsed: [0:02:17.059967]  batch: 250  d_loss: -0.096508  g_loss: 0.174192\n",
      "Elapsed: [0:02:19.166876]  batch: 300  d_loss: -0.040486  g_loss: 0.051149\n",
      "Elapsed: [0:02:21.713136]  batch: 350  d_loss: -0.030156  g_loss: 0.247459\n",
      "Elapsed: [0:02:24.004635]  batch: 400  d_loss: 0.170873  g_loss: 0.278568\n",
      "Elapsed: [0:02:26.281087]  batch: 450  d_loss: 0.029345  g_loss: -0.100522\n",
      "Elapsed: [0:02:28.544350]  batch: 500  d_loss: 0.037439  g_loss: 0.006276\n",
      "Time taken for epoch: 30.848 secs\n",
      "ticker =  2501\n",
      "\n",
      "Epoch: 6\n",
      "Elapsed: [0:02:38.726441]  batch: 1  d_loss: 0.004279  g_loss: 0.024222\n",
      "Elapsed: [0:02:40.925603]  batch: 50  d_loss: -0.014300  g_loss: 0.163393\n",
      "Elapsed: [0:02:43.184122]  batch: 100  d_loss: -0.027638  g_loss: 0.075727\n",
      "Elapsed: [0:02:45.606269]  batch: 150  d_loss: -0.003570  g_loss: 0.082545\n",
      "Elapsed: [0:02:47.533253]  batch: 200  d_loss: -0.039638  g_loss: -0.027490\n",
      "Elapsed: [0:02:49.496500]  batch: 250  d_loss: 0.036097  g_loss: 0.237801\n",
      "Elapsed: [0:02:51.648019]  batch: 300  d_loss: -0.031642  g_loss: -0.070249\n",
      "Elapsed: [0:02:54.061116]  batch: 350  d_loss: 0.012190  g_loss: 0.157231\n",
      "Elapsed: [0:02:56.435919]  batch: 400  d_loss: 0.052182  g_loss: 0.072252\n",
      "Elapsed: [0:02:58.865847]  batch: 450  d_loss: -0.025405  g_loss: 0.035246\n",
      "Elapsed: [0:03:01.249245]  batch: 500  d_loss: -0.006339  g_loss: 0.135524\n",
      "Time taken for epoch: 32.657 secs\n",
      "ticker =  3001\n",
      "\n",
      "Epoch: 7\n",
      "Elapsed: [0:03:13.268239]  batch: 1  d_loss: -0.105124  g_loss: 0.163423\n",
      "Elapsed: [0:03:15.273170]  batch: 50  d_loss: -0.012407  g_loss: 0.036189\n",
      "Elapsed: [0:03:17.126724]  batch: 100  d_loss: -0.043818  g_loss: 0.040345\n",
      "Elapsed: [0:03:19.712297]  batch: 150  d_loss: -0.084262  g_loss: 0.269600\n",
      "Elapsed: [0:03:22.032058]  batch: 200  d_loss: -0.065124  g_loss: -0.007879\n",
      "Elapsed: [0:03:23.927192]  batch: 250  d_loss: 0.051631  g_loss: 0.148216\n",
      "Elapsed: [0:03:25.975268]  batch: 300  d_loss: -0.005658  g_loss: 0.292049\n",
      "Elapsed: [0:03:27.901627]  batch: 350  d_loss: 0.082010  g_loss: 0.059171\n",
      "Elapsed: [0:03:29.928836]  batch: 400  d_loss: 0.053520  g_loss: -0.112561\n",
      "Elapsed: [0:03:32.000930]  batch: 450  d_loss: 0.042280  g_loss: 0.056898\n",
      "Elapsed: [0:03:33.869600]  batch: 500  d_loss: -0.049700  g_loss: 0.084687\n",
      "Time taken for epoch: 32.485 secs\n",
      "ticker =  3501\n",
      "\n",
      "Epoch: 8\n",
      "Elapsed: [0:03:42.911597]  batch: 1  d_loss: -0.028755  g_loss: 0.159210\n",
      "Elapsed: [0:03:44.776528]  batch: 50  d_loss: -0.017797  g_loss: 0.099615\n",
      "Elapsed: [0:03:46.616457]  batch: 100  d_loss: -0.031886  g_loss: 0.185073\n",
      "Elapsed: [0:03:48.536535]  batch: 150  d_loss: 0.013396  g_loss: 0.127496\n",
      "Elapsed: [0:03:50.352548]  batch: 200  d_loss: -0.082803  g_loss: 0.248503\n",
      "Elapsed: [0:03:52.166033]  batch: 250  d_loss: -0.044500  g_loss: -0.001535\n",
      "Elapsed: [0:03:54.020271]  batch: 300  d_loss: -0.053907  g_loss: 0.227603\n",
      "Elapsed: [0:03:55.844258]  batch: 350  d_loss: 0.014329  g_loss: 0.153451\n",
      "Elapsed: [0:03:57.759059]  batch: 400  d_loss: 0.058996  g_loss: 0.028477\n",
      "Elapsed: [0:03:59.755858]  batch: 450  d_loss: -0.072366  g_loss: 0.245498\n",
      "Elapsed: [0:04:01.681472]  batch: 500  d_loss: 0.022846  g_loss: 0.063878\n",
      "Time taken for epoch: 27.738 secs\n",
      "ticker =  4001\n",
      "\n",
      "Epoch: 9\n",
      "Elapsed: [0:04:11.790132]  batch: 1  d_loss: 0.018665  g_loss: 0.016097\n",
      "Elapsed: [0:04:13.864944]  batch: 50  d_loss: 0.072036  g_loss: 0.116992\n",
      "Elapsed: [0:04:15.894374]  batch: 100  d_loss: 0.031920  g_loss: 0.064118\n",
      "Elapsed: [0:04:17.831231]  batch: 150  d_loss: -0.009907  g_loss: 0.045655\n",
      "Elapsed: [0:04:19.756444]  batch: 200  d_loss: -0.014418  g_loss: 0.066941\n",
      "Elapsed: [0:04:21.682203]  batch: 250  d_loss: -0.094003  g_loss: 0.160461\n",
      "Elapsed: [0:04:23.594866]  batch: 300  d_loss: -0.005857  g_loss: -0.044992\n",
      "Elapsed: [0:04:25.857929]  batch: 350  d_loss: 0.019822  g_loss: 0.001202\n",
      "Elapsed: [0:04:27.964313]  batch: 400  d_loss: -0.045397  g_loss: 0.231191\n",
      "Elapsed: [0:04:29.860238]  batch: 450  d_loss: -0.043665  g_loss: 0.051611\n",
      "Elapsed: [0:04:31.841370]  batch: 500  d_loss: -0.026032  g_loss: 0.147306\n",
      "Time taken for epoch: 30.126 secs\n",
      "ticker =  4501\n",
      "\n",
      "Epoch: 10\n",
      "Elapsed: [0:04:41.143291]  batch: 1  d_loss: -0.038579  g_loss: 0.127275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: [0:04:43.157167]  batch: 50  d_loss: 0.018389  g_loss: 0.077798\n",
      "Elapsed: [0:04:45.041252]  batch: 100  d_loss: -0.080008  g_loss: 0.263681\n",
      "Elapsed: [0:04:46.867120]  batch: 150  d_loss: -0.018589  g_loss: 0.059944\n",
      "Elapsed: [0:04:48.685553]  batch: 200  d_loss: -0.007458  g_loss: 0.063142\n",
      "Elapsed: [0:04:50.571781]  batch: 250  d_loss: -0.050699  g_loss: 0.227165\n",
      "Elapsed: [0:04:52.499213]  batch: 300  d_loss: -0.082847  g_loss: 0.207220\n",
      "Elapsed: [0:04:54.498968]  batch: 350  d_loss: -0.053322  g_loss: 0.070259\n",
      "Elapsed: [0:04:56.421018]  batch: 400  d_loss: -0.065243  g_loss: -0.010705\n",
      "Elapsed: [0:04:58.301960]  batch: 450  d_loss: 0.029731  g_loss: 0.356249\n",
      "Elapsed: [0:05:00.206697]  batch: 500  d_loss: -0.072914  g_loss: 0.048720\n",
      "Time taken for epoch: 28.186 secs\n",
      "ticker =  5001\n",
      "\n",
      "\n",
      "Currently working on Depth:  1\n",
      "Current resolution: 4 x 4\n",
      "Ticker 1\n",
      "\n",
      "Epoch: 1\n",
      "Elapsed: [0:05:09.577384]  batch: 1  d_loss: 2.270641  g_loss: 0.017553\n",
      "Elapsed: [0:05:12.590697]  batch: 50  d_loss: 1.055428  g_loss: -0.879582\n",
      "Elapsed: [0:05:15.379746]  batch: 100  d_loss: -0.905500  g_loss: 1.277902\n",
      "Elapsed: [0:05:18.185629]  batch: 150  d_loss: 1.102750  g_loss: -0.928345\n",
      "Elapsed: [0:05:21.060431]  batch: 200  d_loss: 0.939588  g_loss: -0.324904\n",
      "Elapsed: [0:05:23.855819]  batch: 250  d_loss: -0.169094  g_loss: 0.880979\n",
      "Elapsed: [0:05:26.633013]  batch: 300  d_loss: 1.180495  g_loss: -1.129264\n",
      "Elapsed: [0:05:29.432992]  batch: 350  d_loss: 1.650066  g_loss: -1.655959\n",
      "Elapsed: [0:05:32.341825]  batch: 400  d_loss: -0.854307  g_loss: 1.021847\n",
      "Elapsed: [0:05:35.362510]  batch: 450  d_loss: 0.658067  g_loss: 0.053173\n",
      "Elapsed: [0:05:38.325386]  batch: 500  d_loss: 0.005694  g_loss: 0.051672\n",
      "Time taken for epoch: 38.045 secs\n",
      "ticker =  501\n",
      "\n",
      "Epoch: 2\n",
      "Elapsed: [0:05:47.348831]  batch: 1  d_loss: 0.121803  g_loss: 0.110902\n",
      "Elapsed: [0:05:50.121278]  batch: 50  d_loss: -0.034531  g_loss: 0.156592\n",
      "Elapsed: [0:05:53.015982]  batch: 100  d_loss: -0.019136  g_loss: 0.129680\n",
      "Elapsed: [0:05:55.873729]  batch: 150  d_loss: -0.028485  g_loss: 0.062225\n",
      "Elapsed: [0:05:58.865565]  batch: 200  d_loss: -0.055213  g_loss: 0.140158\n",
      "Elapsed: [0:06:01.675727]  batch: 250  d_loss: -0.124120  g_loss: 0.344063\n",
      "Elapsed: [0:06:04.490309]  batch: 300  d_loss: -0.040624  g_loss: 0.433663\n",
      "Elapsed: [0:06:07.386146]  batch: 350  d_loss: -0.097728  g_loss: 0.263759\n",
      "Elapsed: [0:06:10.172107]  batch: 400  d_loss: -0.028632  g_loss: 0.077980\n",
      "Elapsed: [0:06:13.068277]  batch: 450  d_loss: 0.031334  g_loss: 0.161021\n",
      "Elapsed: [0:06:15.836864]  batch: 500  d_loss: -0.144029  g_loss: 0.181759\n",
      "Time taken for epoch: 37.470 secs\n",
      "ticker =  1001\n",
      "\n",
      "Epoch: 3\n",
      "Elapsed: [0:06:26.147188]  batch: 1  d_loss: -0.043535  g_loss: 0.066273\n",
      "Elapsed: [0:06:29.024430]  batch: 50  d_loss: -0.131866  g_loss: 0.109783\n",
      "Elapsed: [0:06:31.829058]  batch: 100  d_loss: -0.090594  g_loss: 0.160786\n",
      "Elapsed: [0:06:34.700260]  batch: 150  d_loss: -0.118690  g_loss: 0.428105\n",
      "Elapsed: [0:06:37.635484]  batch: 200  d_loss: -0.116754  g_loss: -0.094446\n",
      "Elapsed: [0:06:40.420248]  batch: 250  d_loss: 0.017601  g_loss: 0.223804\n",
      "Elapsed: [0:06:43.242958]  batch: 300  d_loss: -0.084510  g_loss: 0.138590\n",
      "Elapsed: [0:06:46.006828]  batch: 350  d_loss: -0.114227  g_loss: -0.081662\n",
      "Elapsed: [0:06:48.801327]  batch: 400  d_loss: -0.094820  g_loss: 0.201055\n",
      "Elapsed: [0:06:51.573796]  batch: 450  d_loss: -0.022091  g_loss: 0.177078\n",
      "Elapsed: [0:06:54.342348]  batch: 500  d_loss: -0.070984  g_loss: 0.148519\n",
      "Time taken for epoch: 38.443 secs\n",
      "ticker =  1501\n",
      "\n",
      "Epoch: 4\n",
      "Elapsed: [0:07:03.254320]  batch: 1  d_loss: -0.042995  g_loss: 0.125182\n",
      "Elapsed: [0:07:06.009627]  batch: 50  d_loss: -0.123536  g_loss: 0.143565\n",
      "Elapsed: [0:07:08.813287]  batch: 100  d_loss: -0.039387  g_loss: 0.051538\n",
      "Elapsed: [0:07:11.612293]  batch: 150  d_loss: -0.060516  g_loss: 0.219763\n",
      "Elapsed: [0:07:14.361111]  batch: 200  d_loss: -0.083030  g_loss: 0.014852\n",
      "Elapsed: [0:07:17.111509]  batch: 250  d_loss: -0.104558  g_loss: 0.286115\n",
      "Elapsed: [0:07:19.863700]  batch: 300  d_loss: -0.102366  g_loss: 0.056861\n",
      "Elapsed: [0:07:22.600898]  batch: 350  d_loss: -0.162929  g_loss: 0.337500\n",
      "Elapsed: [0:07:25.322848]  batch: 400  d_loss: -0.075059  g_loss: 0.119097\n",
      "Elapsed: [0:07:28.112588]  batch: 450  d_loss: -0.069681  g_loss: 0.087905\n",
      "Elapsed: [0:07:30.850086]  batch: 500  d_loss: -0.072674  g_loss: 0.174081\n",
      "Time taken for epoch: 36.429 secs\n",
      "ticker =  2001\n",
      "\n",
      "Epoch: 5\n",
      "Elapsed: [0:07:39.761056]  batch: 1  d_loss: -0.132898  g_loss: 0.207203\n",
      "Elapsed: [0:07:42.520539]  batch: 50  d_loss: -0.129440  g_loss: 0.252388\n",
      "Elapsed: [0:07:45.306909]  batch: 100  d_loss: -0.109226  g_loss: 0.238912\n",
      "Elapsed: [0:07:48.081614]  batch: 150  d_loss: -0.013859  g_loss: 0.339914\n",
      "Elapsed: [0:07:50.821733]  batch: 200  d_loss: -0.111562  g_loss: 0.176540\n",
      "Elapsed: [0:07:53.603632]  batch: 250  d_loss: 0.024149  g_loss: 0.119847\n",
      "Elapsed: [0:07:56.397984]  batch: 300  d_loss: -0.084845  g_loss: 0.249129\n",
      "Elapsed: [0:07:59.170665]  batch: 350  d_loss: -0.177732  g_loss: 0.212719\n",
      "Elapsed: [0:08:01.938820]  batch: 400  d_loss: -0.197236  g_loss: 0.264513\n",
      "Elapsed: [0:08:04.672070]  batch: 450  d_loss: -0.116128  g_loss: 0.322216\n",
      "Elapsed: [0:08:07.435958]  batch: 500  d_loss: -0.214537  g_loss: 0.135807\n",
      "Time taken for epoch: 36.515 secs\n",
      "ticker =  2501\n",
      "\n",
      "Epoch: 6\n",
      "Elapsed: [0:08:16.274809]  batch: 1  d_loss: -0.025875  g_loss: 0.139912\n",
      "Elapsed: [0:08:18.998343]  batch: 50  d_loss: -0.178404  g_loss: 0.142485\n",
      "Elapsed: [0:08:21.768350]  batch: 100  d_loss: -0.126488  g_loss: 0.358114\n",
      "Elapsed: [0:08:24.603865]  batch: 150  d_loss: -0.098232  g_loss: 0.129534\n",
      "Elapsed: [0:08:27.411811]  batch: 200  d_loss: -0.224164  g_loss: 0.205028\n",
      "Elapsed: [0:08:30.289105]  batch: 250  d_loss: -0.124433  g_loss: 0.235816\n",
      "Elapsed: [0:08:33.115968]  batch: 300  d_loss: -0.222954  g_loss: 0.256049\n",
      "Elapsed: [0:08:35.885823]  batch: 350  d_loss: -0.186052  g_loss: 0.118477\n",
      "Elapsed: [0:08:38.699551]  batch: 400  d_loss: -0.165700  g_loss: 0.158517\n",
      "Elapsed: [0:08:41.515847]  batch: 450  d_loss: -0.197874  g_loss: 0.323172\n",
      "Elapsed: [0:08:44.347417]  batch: 500  d_loss: -0.161627  g_loss: 0.231444\n",
      "Time taken for epoch: 36.836 secs\n",
      "ticker =  3001\n",
      "\n",
      "Epoch: 7\n",
      "Elapsed: [0:08:53.354855]  batch: 1  d_loss: -0.141637  g_loss: 0.135733\n",
      "Elapsed: [0:08:56.121932]  batch: 50  d_loss: -0.115151  g_loss: 0.196112\n",
      "Elapsed: [0:08:58.897051]  batch: 100  d_loss: -0.162748  g_loss: 0.418155\n",
      "Elapsed: [0:09:01.692667]  batch: 150  d_loss: -0.084129  g_loss: 0.172099\n",
      "Elapsed: [0:09:04.536869]  batch: 200  d_loss: -0.158742  g_loss: 0.141338\n",
      "Elapsed: [0:09:07.352288]  batch: 250  d_loss: -0.017485  g_loss: 0.283825\n",
      "Elapsed: [0:09:10.200849]  batch: 300  d_loss: -0.160999  g_loss: 0.182622\n",
      "Elapsed: [0:09:13.061062]  batch: 350  d_loss: -0.081590  g_loss: 0.266283\n",
      "Elapsed: [0:09:15.911973]  batch: 400  d_loss: -0.150148  g_loss: 0.346083\n",
      "Elapsed: [0:09:18.716480]  batch: 450  d_loss: -0.128846  g_loss: 0.095501\n",
      "Elapsed: [0:09:21.515367]  batch: 500  d_loss: -0.188174  g_loss: 0.249737\n",
      "Time taken for epoch: 37.091 secs\n",
      "ticker =  3501\n",
      "\n",
      "Epoch: 8\n",
      "Elapsed: [0:09:30.493724]  batch: 1  d_loss: -0.193026  g_loss: 0.256388\n",
      "Elapsed: [0:09:33.187281]  batch: 50  d_loss: -0.027440  g_loss: 0.186936\n",
      "Elapsed: [0:09:35.924454]  batch: 100  d_loss: -0.234133  g_loss: 0.459482\n",
      "Elapsed: [0:09:38.654946]  batch: 150  d_loss: -0.246278  g_loss: 0.306330\n",
      "Elapsed: [0:09:41.405190]  batch: 200  d_loss: -0.189101  g_loss: 0.097705\n",
      "Elapsed: [0:09:44.137392]  batch: 250  d_loss: -0.133275  g_loss: 0.333175\n",
      "Elapsed: [0:09:46.896464]  batch: 300  d_loss: -0.151096  g_loss: 0.225199\n",
      "Elapsed: [0:09:49.648412]  batch: 350  d_loss: -0.171634  g_loss: 0.332529\n",
      "Elapsed: [0:09:52.414278]  batch: 400  d_loss: -0.170439  g_loss: 0.339246\n",
      "Elapsed: [0:09:55.183390]  batch: 450  d_loss: -0.182048  g_loss: 0.204886\n",
      "Elapsed: [0:09:57.942514]  batch: 500  d_loss: -0.020335  g_loss: 0.231973\n",
      "Time taken for epoch: 36.369 secs\n",
      "ticker =  4001\n",
      "\n",
      "Epoch: 9\n",
      "Elapsed: [0:10:06.970879]  batch: 1  d_loss: 0.012668  g_loss: 0.185560\n",
      "Elapsed: [0:10:09.754517]  batch: 50  d_loss: 0.000696  g_loss: 0.224197\n",
      "Elapsed: [0:10:12.608744]  batch: 100  d_loss: -0.140817  g_loss: 0.307838\n",
      "Elapsed: [0:10:15.394548]  batch: 150  d_loss: 0.090936  g_loss: 0.133277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: [0:10:18.161973]  batch: 200  d_loss: -0.259844  g_loss: 0.348393\n",
      "Elapsed: [0:10:20.933716]  batch: 250  d_loss: -0.114600  g_loss: 0.217215\n",
      "Elapsed: [0:10:23.734231]  batch: 300  d_loss: -0.185148  g_loss: 0.162837\n",
      "Elapsed: [0:10:26.551600]  batch: 350  d_loss: -0.165648  g_loss: 0.383260\n",
      "Elapsed: [0:10:29.382267]  batch: 400  d_loss: -0.183015  g_loss: 0.230126\n",
      "Elapsed: [0:10:32.181732]  batch: 450  d_loss: -0.253866  g_loss: 0.360400\n",
      "Elapsed: [0:10:34.976525]  batch: 500  d_loss: -0.041960  g_loss: 0.150252\n",
      "Time taken for epoch: 36.944 secs\n",
      "ticker =  4501\n",
      "\n",
      "Epoch: 10\n",
      "Elapsed: [0:10:44.954864]  batch: 1  d_loss: -0.120720  g_loss: 0.177236\n",
      "Elapsed: [0:10:48.666890]  batch: 50  d_loss: -0.096432  g_loss: 0.177085\n",
      "Elapsed: [0:10:52.328467]  batch: 100  d_loss: -0.212464  g_loss: 0.196830\n",
      "Elapsed: [0:10:56.097406]  batch: 150  d_loss: -0.051673  g_loss: 0.328278\n",
      "Elapsed: [0:10:59.426878]  batch: 200  d_loss: -0.015798  g_loss: 0.114169\n",
      "Elapsed: [0:11:03.001943]  batch: 250  d_loss: -0.128807  g_loss: 0.412642\n",
      "Elapsed: [0:11:06.523019]  batch: 300  d_loss: -0.259093  g_loss: 0.467785\n",
      "Elapsed: [0:11:10.177462]  batch: 350  d_loss: 0.079213  g_loss: 0.132013\n",
      "Elapsed: [0:11:13.757394]  batch: 400  d_loss: -0.218922  g_loss: 0.267863\n",
      "Elapsed: [0:11:17.127337]  batch: 450  d_loss: -0.091419  g_loss: 0.384379\n",
      "Elapsed: [0:11:20.594916]  batch: 500  d_loss: -0.164152  g_loss: 0.341704\n",
      "Time taken for epoch: 45.607 secs\n",
      "ticker =  5001\n",
      "\n",
      "Epoch: 11\n",
      "Elapsed: [0:11:31.783335]  batch: 1  d_loss: -0.122257  g_loss: 0.171032\n",
      "Elapsed: [0:11:35.294799]  batch: 50  d_loss: -0.125428  g_loss: 0.267505\n",
      "Elapsed: [0:11:38.891455]  batch: 100  d_loss: -0.245019  g_loss: 0.187602\n",
      "Elapsed: [0:11:42.482736]  batch: 150  d_loss: -0.048794  g_loss: 0.218743\n",
      "Elapsed: [0:11:45.946717]  batch: 200  d_loss: -0.005191  g_loss: 0.202378\n",
      "Elapsed: [0:11:49.804191]  batch: 250  d_loss: -0.024729  g_loss: 0.073865\n",
      "Elapsed: [0:11:53.521742]  batch: 300  d_loss: 0.160790  g_loss: 0.279623\n",
      "Elapsed: [0:11:56.970695]  batch: 350  d_loss: -0.082118  g_loss: 0.199688\n",
      "Elapsed: [0:12:00.502506]  batch: 400  d_loss: -0.155758  g_loss: 0.280535\n",
      "Elapsed: [0:12:04.170300]  batch: 450  d_loss: -0.228793  g_loss: 0.377192\n",
      "Elapsed: [0:12:07.970916]  batch: 500  d_loss: 0.192993  g_loss: 0.087104\n",
      "Time taken for epoch: 47.277 secs\n",
      "ticker =  5501\n",
      "\n",
      "Epoch: 12\n",
      "Elapsed: [0:12:19.460415]  batch: 1  d_loss: 0.034043  g_loss: 0.342247\n",
      "Elapsed: [0:12:23.645831]  batch: 50  d_loss: 0.117141  g_loss: 0.090372\n",
      "Elapsed: [0:12:26.623438]  batch: 100  d_loss: -0.089133  g_loss: 0.101565\n",
      "Elapsed: [0:12:29.597152]  batch: 150  d_loss: 0.076465  g_loss: 0.117992\n",
      "Elapsed: [0:12:32.568302]  batch: 200  d_loss: -0.052232  g_loss: 0.253826\n",
      "Elapsed: [0:12:35.397360]  batch: 250  d_loss: -0.067746  g_loss: 0.299950\n",
      "Elapsed: [0:12:38.231178]  batch: 300  d_loss: -0.011018  g_loss: 0.201584\n",
      "Elapsed: [0:12:41.045598]  batch: 350  d_loss: -0.111060  g_loss: 0.067514\n",
      "Elapsed: [0:12:43.863957]  batch: 400  d_loss: -0.021489  g_loss: 0.197834\n",
      "Elapsed: [0:12:46.736599]  batch: 450  d_loss: 0.016406  g_loss: 0.116948\n",
      "Elapsed: [0:12:49.545499]  batch: 500  d_loss: -0.074805  g_loss: 0.217042\n",
      "Time taken for epoch: 41.397 secs\n",
      "ticker =  6001\n",
      "\n",
      "Epoch: 13\n",
      "Elapsed: [0:12:58.576896]  batch: 1  d_loss: -0.078319  g_loss: 0.231054\n",
      "Elapsed: [0:13:01.320440]  batch: 50  d_loss: -0.068737  g_loss: 0.347899\n",
      "Elapsed: [0:13:04.140627]  batch: 100  d_loss: -0.023131  g_loss: 0.258739\n",
      "Elapsed: [0:13:06.918441]  batch: 150  d_loss: -0.060382  g_loss: 0.095901\n",
      "Elapsed: [0:13:09.811578]  batch: 200  d_loss: 0.007680  g_loss: 0.397726\n",
      "Elapsed: [0:13:12.715451]  batch: 250  d_loss: -0.112823  g_loss: 0.187006\n",
      "Elapsed: [0:13:15.551034]  batch: 300  d_loss: -0.084927  g_loss: 0.294614\n",
      "Elapsed: [0:13:18.365988]  batch: 350  d_loss: -0.086759  g_loss: 0.444154\n",
      "Elapsed: [0:13:21.203931]  batch: 400  d_loss: -0.078667  g_loss: 0.203645\n",
      "Elapsed: [0:13:24.030931]  batch: 450  d_loss: -0.333451  g_loss: 0.195871\n",
      "Elapsed: [0:13:26.883525]  batch: 500  d_loss: -0.155848  g_loss: 0.156135\n",
      "Time taken for epoch: 37.285 secs\n",
      "ticker =  6501\n",
      "\n",
      "Epoch: 14\n",
      "Elapsed: [0:13:36.040891]  batch: 1  d_loss: -0.031869  g_loss: 0.141226\n",
      "Elapsed: [0:13:38.831622]  batch: 50  d_loss: -0.000494  g_loss: 0.166356\n",
      "Elapsed: [0:13:41.669078]  batch: 100  d_loss: 0.044683  g_loss: 0.194104\n",
      "Elapsed: [0:13:44.522641]  batch: 150  d_loss: -0.017470  g_loss: 0.093145\n",
      "Elapsed: [0:13:47.366730]  batch: 200  d_loss: -0.081495  g_loss: 0.258599\n",
      "Elapsed: [0:13:50.208522]  batch: 250  d_loss: -0.124554  g_loss: 0.129801\n",
      "Elapsed: [0:13:53.100286]  batch: 300  d_loss: -0.121896  g_loss: 0.214507\n",
      "Elapsed: [0:13:55.897468]  batch: 350  d_loss: -0.011194  g_loss: 0.063802\n",
      "Elapsed: [0:13:58.763966]  batch: 400  d_loss: -0.101994  g_loss: 0.057668\n",
      "Elapsed: [0:14:01.583741]  batch: 450  d_loss: 0.003348  g_loss: 0.234745\n",
      "Elapsed: [0:14:04.362886]  batch: 500  d_loss: -0.167993  g_loss: 0.556658\n",
      "Time taken for epoch: 37.379 secs\n",
      "ticker =  7001\n",
      "\n",
      "Epoch: 15\n",
      "Elapsed: [0:14:13.581453]  batch: 1  d_loss: -0.064352  g_loss: 0.305827\n",
      "Elapsed: [0:14:16.468498]  batch: 50  d_loss: -0.052953  g_loss: 0.217040\n",
      "Elapsed: [0:14:19.291222]  batch: 100  d_loss: -0.103584  g_loss: 0.339286\n",
      "Elapsed: [0:14:22.101064]  batch: 150  d_loss: 0.016194  g_loss: 0.386372\n",
      "Elapsed: [0:14:24.954106]  batch: 200  d_loss: -0.123977  g_loss: 0.245844\n",
      "Elapsed: [0:14:27.792276]  batch: 250  d_loss: -0.308355  g_loss: 0.325046\n",
      "Elapsed: [0:14:30.634649]  batch: 300  d_loss: 0.015047  g_loss: 0.210896\n",
      "Elapsed: [0:14:33.469345]  batch: 350  d_loss: -0.132026  g_loss: 0.343885\n",
      "Elapsed: [0:14:36.270540]  batch: 400  d_loss: 0.016623  g_loss: 0.101164\n",
      "Elapsed: [0:14:39.077562]  batch: 450  d_loss: -0.020465  g_loss: 0.327132\n",
      "Elapsed: [0:14:41.875079]  batch: 500  d_loss: -0.106363  g_loss: 0.331533\n",
      "Time taken for epoch: 37.425 secs\n",
      "ticker =  7501\n",
      "\n",
      "Epoch: 16\n",
      "Elapsed: [0:14:50.886689]  batch: 1  d_loss: -0.089082  g_loss: 0.255330\n",
      "Elapsed: [0:14:53.793902]  batch: 50  d_loss: -0.024411  g_loss: 0.219516\n",
      "Elapsed: [0:14:56.613745]  batch: 100  d_loss: -0.284623  g_loss: 0.073009\n",
      "Elapsed: [0:14:59.410194]  batch: 150  d_loss: -0.179652  g_loss: 0.148902\n",
      "Elapsed: [0:15:02.241301]  batch: 200  d_loss: -0.058759  g_loss: 0.266593\n",
      "Elapsed: [0:15:05.065759]  batch: 250  d_loss: -0.124817  g_loss: 0.255987\n",
      "Elapsed: [0:15:07.856819]  batch: 300  d_loss: -0.026492  g_loss: 0.228245\n",
      "Elapsed: [0:15:10.762753]  batch: 350  d_loss: -0.082489  g_loss: 0.130533\n",
      "Elapsed: [0:15:13.774060]  batch: 400  d_loss: -0.084126  g_loss: 0.433474\n",
      "Elapsed: [0:15:16.763117]  batch: 450  d_loss: -0.398498  g_loss: 0.086329\n",
      "Elapsed: [0:15:19.572263]  batch: 500  d_loss: -0.157853  g_loss: 0.261461\n",
      "Time taken for epoch: 37.609 secs\n",
      "ticker =  8001\n",
      "\n",
      "Epoch: 17\n",
      "Elapsed: [0:15:28.542462]  batch: 1  d_loss: -0.108220  g_loss: 0.289064\n",
      "Elapsed: [0:15:31.376638]  batch: 50  d_loss: -0.172079  g_loss: 0.336824\n",
      "Elapsed: [0:15:34.208813]  batch: 100  d_loss: -0.146559  g_loss: 0.303744\n",
      "Elapsed: [0:15:37.026779]  batch: 150  d_loss: -0.084713  g_loss: 0.368934\n",
      "Elapsed: [0:15:39.868376]  batch: 200  d_loss: -0.054357  g_loss: 0.260371\n",
      "Elapsed: [0:15:42.676768]  batch: 250  d_loss: 0.075814  g_loss: 0.244776\n",
      "Elapsed: [0:15:45.530706]  batch: 300  d_loss: -0.113942  g_loss: 0.227202\n",
      "Elapsed: [0:15:48.475606]  batch: 350  d_loss: -0.184707  g_loss: 0.349142\n",
      "Elapsed: [0:15:51.848594]  batch: 400  d_loss: -0.249165  g_loss: 0.063711\n",
      "Elapsed: [0:15:55.187604]  batch: 450  d_loss: -0.056476  g_loss: 0.337792\n",
      "Elapsed: [0:15:58.025578]  batch: 500  d_loss: -0.012650  g_loss: 0.176350\n",
      "Time taken for epoch: 38.401 secs\n",
      "ticker =  8501\n",
      "\n",
      "Epoch: 18\n",
      "Elapsed: [0:16:07.168628]  batch: 1  d_loss: -0.113837  g_loss: 0.190454\n",
      "Elapsed: [0:16:09.918719]  batch: 50  d_loss: -0.188470  g_loss: 0.284363\n",
      "Elapsed: [0:16:12.780065]  batch: 100  d_loss: -0.249222  g_loss: 0.377754\n",
      "Elapsed: [0:16:15.586303]  batch: 150  d_loss: -0.046852  g_loss: 0.449190\n",
      "Elapsed: [0:16:18.373763]  batch: 200  d_loss: -0.087377  g_loss: 0.326327\n",
      "Elapsed: [0:16:21.220002]  batch: 250  d_loss: -0.090299  g_loss: 0.198801\n",
      "Elapsed: [0:16:24.001650]  batch: 300  d_loss: 0.074921  g_loss: 0.206382\n",
      "Elapsed: [0:16:26.748328]  batch: 350  d_loss: -0.057095  g_loss: 0.260011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: [0:16:29.949060]  batch: 400  d_loss: -0.155367  g_loss: 0.314983\n",
      "Elapsed: [0:16:33.090598]  batch: 450  d_loss: 0.051540  g_loss: 0.221091\n",
      "Elapsed: [0:16:36.269662]  batch: 500  d_loss: -0.089193  g_loss: 0.190424\n",
      "Time taken for epoch: 38.175 secs\n",
      "ticker =  9001\n",
      "\n",
      "Epoch: 19\n",
      "Elapsed: [0:16:46.676976]  batch: 1  d_loss: 0.008240  g_loss: 0.051423\n",
      "Elapsed: [0:16:49.815106]  batch: 50  d_loss: 0.064033  g_loss: 0.065273\n",
      "Elapsed: [0:16:53.084525]  batch: 100  d_loss: 0.059366  g_loss: 0.168941\n",
      "Elapsed: [0:16:56.401689]  batch: 150  d_loss: -0.011071  g_loss: 0.184325\n",
      "Elapsed: [0:16:59.784539]  batch: 200  d_loss: -0.186043  g_loss: 0.342716\n",
      "Elapsed: [0:17:03.037068]  batch: 250  d_loss: 0.044187  g_loss: 0.080716\n",
      "Elapsed: [0:17:06.455585]  batch: 300  d_loss: -0.094477  g_loss: 0.332080\n",
      "Elapsed: [0:17:09.978284]  batch: 350  d_loss: -0.027604  g_loss: 0.232553\n",
      "Elapsed: [0:17:13.336878]  batch: 400  d_loss: -0.122804  g_loss: 0.250613\n",
      "Elapsed: [0:17:16.745128]  batch: 450  d_loss: 0.007331  g_loss: 0.337238\n",
      "Elapsed: [0:17:20.034629]  batch: 500  d_loss: -0.022559  g_loss: 0.280055\n",
      "Time taken for epoch: 43.693 secs\n",
      "ticker =  9501\n",
      "\n",
      "Epoch: 20\n",
      "Elapsed: [0:17:31.355726]  batch: 1  d_loss: -0.027765  g_loss: 0.120170\n",
      "Elapsed: [0:17:34.717685]  batch: 50  d_loss: 0.004529  g_loss: 0.085698\n",
      "Elapsed: [0:17:38.315436]  batch: 100  d_loss: -0.068000  g_loss: 0.215870\n",
      "Elapsed: [0:17:41.906264]  batch: 150  d_loss: 0.123032  g_loss: -0.028380\n",
      "Elapsed: [0:17:45.553778]  batch: 200  d_loss: -0.085252  g_loss: 0.236487\n",
      "Elapsed: [0:17:49.128356]  batch: 250  d_loss: 0.012684  g_loss: 0.250717\n",
      "Elapsed: [0:17:52.883244]  batch: 300  d_loss: -0.028717  g_loss: 0.340906\n",
      "Elapsed: [0:17:56.525319]  batch: 350  d_loss: -0.029788  g_loss: 0.286732\n",
      "Elapsed: [0:18:00.043863]  batch: 400  d_loss: 0.075955  g_loss: 0.075446\n",
      "Elapsed: [0:18:03.660722]  batch: 450  d_loss: 0.083847  g_loss: 0.406743\n",
      "Elapsed: [0:18:07.226686]  batch: 500  d_loss: -0.092034  g_loss: 0.045489\n",
      "Time taken for epoch: 47.096 secs\n",
      "ticker =  10001\n",
      "\n",
      "Epoch: 21\n",
      "Elapsed: [0:18:18.056906]  batch: 1  d_loss: 0.034894  g_loss: 0.120839\n",
      "Elapsed: [0:18:21.242604]  batch: 50  d_loss: 0.007754  g_loss: 0.415222\n",
      "Elapsed: [0:18:24.451757]  batch: 100  d_loss: -0.091437  g_loss: 0.146588\n",
      "Elapsed: [0:18:27.913359]  batch: 150  d_loss: -0.145166  g_loss: 0.175537\n",
      "Elapsed: [0:18:31.305886]  batch: 200  d_loss: 0.021919  g_loss: 0.239201\n",
      "Elapsed: [0:18:34.949363]  batch: 250  d_loss: -0.093094  g_loss: 0.268924\n",
      "Elapsed: [0:18:38.813147]  batch: 300  d_loss: 0.071830  g_loss: 0.139042\n",
      "Elapsed: [0:18:42.590264]  batch: 350  d_loss: -0.061125  g_loss: 0.230920\n",
      "Elapsed: [0:18:46.325603]  batch: 400  d_loss: -0.095890  g_loss: 0.191481\n",
      "Elapsed: [0:18:49.974746]  batch: 450  d_loss: 0.043024  g_loss: 0.282462\n",
      "Elapsed: [0:18:53.809624]  batch: 500  d_loss: -0.087973  g_loss: 0.115701\n",
      "Time taken for epoch: 46.527 secs\n",
      "ticker =  10501\n",
      "\n",
      "Epoch: 22\n",
      "Elapsed: [0:19:04.221648]  batch: 1  d_loss: -0.001253  g_loss: 0.086361\n",
      "Elapsed: [0:19:07.058669]  batch: 50  d_loss: -0.075670  g_loss: 0.221827\n",
      "Elapsed: [0:19:09.997165]  batch: 100  d_loss: -0.060774  g_loss: 0.277302\n",
      "Elapsed: [0:19:12.899037]  batch: 150  d_loss: -0.119216  g_loss: 0.158252\n",
      "Elapsed: [0:19:15.751352]  batch: 200  d_loss: -0.079778  g_loss: -0.121942\n",
      "Elapsed: [0:19:18.597169]  batch: 250  d_loss: -0.010875  g_loss: 0.232099\n",
      "Elapsed: [0:19:21.476852]  batch: 300  d_loss: -0.057257  g_loss: 0.189936\n",
      "Elapsed: [0:19:24.354723]  batch: 350  d_loss: -0.013766  g_loss: 0.110819\n",
      "Elapsed: [0:19:27.214478]  batch: 400  d_loss: -0.046024  g_loss: 0.199986\n",
      "Elapsed: [0:19:30.048693]  batch: 450  d_loss: -0.070348  g_loss: 0.204290\n",
      "Elapsed: [0:19:32.945632]  batch: 500  d_loss: -0.036727  g_loss: 0.063934\n",
      "Time taken for epoch: 38.978 secs\n",
      "ticker =  11001\n",
      "\n",
      "Epoch: 23\n",
      "Elapsed: [0:19:42.122941]  batch: 1  d_loss: 0.064562  g_loss: 0.192225\n",
      "Elapsed: [0:19:44.921317]  batch: 50  d_loss: -0.137504  g_loss: 0.302261\n",
      "Elapsed: [0:19:47.758450]  batch: 100  d_loss: 0.021249  g_loss: 0.099719\n",
      "Elapsed: [0:19:50.606533]  batch: 150  d_loss: -0.193099  g_loss: 0.425850\n",
      "Elapsed: [0:19:53.460771]  batch: 200  d_loss: -0.073761  g_loss: 0.070023\n",
      "Elapsed: [0:19:56.321819]  batch: 250  d_loss: -0.015446  g_loss: 0.196776\n",
      "Elapsed: [0:19:59.185534]  batch: 300  d_loss: -0.061865  g_loss: 0.092257\n",
      "Elapsed: [0:20:01.990393]  batch: 350  d_loss: 0.005104  g_loss: 0.274302\n",
      "Elapsed: [0:20:04.806253]  batch: 400  d_loss: -0.082555  g_loss: 0.236747\n",
      "Elapsed: [0:20:07.634354]  batch: 450  d_loss: -0.027829  g_loss: 0.304262\n",
      "Elapsed: [0:20:10.466490]  batch: 500  d_loss: -0.099012  g_loss: 0.376871\n",
      "Time taken for epoch: 37.436 secs\n",
      "ticker =  11501\n",
      "\n",
      "Epoch: 24\n",
      "Elapsed: [0:20:19.637644]  batch: 1  d_loss: -0.002977  g_loss: 0.253537\n",
      "Elapsed: [0:20:22.516682]  batch: 50  d_loss: -0.022793  g_loss: 0.245888\n",
      "Elapsed: [0:20:25.349995]  batch: 100  d_loss: -0.137123  g_loss: 0.392109\n",
      "Elapsed: [0:20:28.172489]  batch: 150  d_loss: -0.088634  g_loss: 0.254399\n",
      "Elapsed: [0:20:31.070341]  batch: 200  d_loss: -0.066192  g_loss: 0.247070\n",
      "Elapsed: [0:20:33.913953]  batch: 250  d_loss: -0.058392  g_loss: 0.269266\n",
      "Elapsed: [0:20:36.791676]  batch: 300  d_loss: -0.068946  g_loss: 0.136038\n",
      "Elapsed: [0:20:39.669661]  batch: 350  d_loss: -0.012437  g_loss: 0.254324\n",
      "Elapsed: [0:20:42.547186]  batch: 400  d_loss: -0.011386  g_loss: 0.155694\n",
      "Elapsed: [0:20:45.396536]  batch: 450  d_loss: 0.034796  g_loss: 0.089463\n",
      "Elapsed: [0:20:48.258586]  batch: 500  d_loss: -0.119069  g_loss: 0.253095\n",
      "Time taken for epoch: 37.698 secs\n",
      "ticker =  12001\n",
      "\n",
      "Epoch: 25\n",
      "Elapsed: [0:20:57.649658]  batch: 1  d_loss: -0.015192  g_loss: 0.150570\n",
      "Elapsed: [0:21:00.502756]  batch: 50  d_loss: -0.108404  g_loss: 0.357748\n",
      "Elapsed: [0:21:03.325150]  batch: 100  d_loss: 0.027703  g_loss: 0.154447\n",
      "Elapsed: [0:21:06.159293]  batch: 150  d_loss: 0.055522  g_loss: 0.043047\n",
      "Elapsed: [0:21:09.030912]  batch: 200  d_loss: -0.054038  g_loss: 0.195626\n",
      "Elapsed: [0:21:11.933053]  batch: 250  d_loss: 0.062150  g_loss: 0.224490\n",
      "Elapsed: [0:21:14.769885]  batch: 300  d_loss: -0.056204  g_loss: -0.012934\n",
      "Elapsed: [0:21:17.635360]  batch: 350  d_loss: -0.043677  g_loss: 0.164267\n",
      "Elapsed: [0:21:20.469884]  batch: 400  d_loss: -0.084682  g_loss: 0.176724\n",
      "Elapsed: [0:21:23.311800]  batch: 450  d_loss: -0.028947  g_loss: 0.114251\n",
      "Elapsed: [0:21:26.143838]  batch: 500  d_loss: -0.038582  g_loss: 0.309614\n",
      "Time taken for epoch: 37.833 secs\n",
      "ticker =  12501\n",
      "\n",
      "\n",
      "Currently working on Depth:  2\n",
      "Current resolution: 8 x 8\n",
      "Ticker 1\n",
      "\n",
      "Epoch: 1\n",
      "Elapsed: [0:21:35.298623]  batch: 1  d_loss: 2.357207  g_loss: 0.251056\n",
      "Elapsed: [0:21:38.786967]  batch: 50  d_loss: -0.206992  g_loss: 0.635240\n",
      "Elapsed: [0:21:42.311020]  batch: 100  d_loss: -0.113531  g_loss: 0.024900\n",
      "Elapsed: [0:21:45.899775]  batch: 150  d_loss: -0.066203  g_loss: 0.091413\n",
      "Elapsed: [0:21:49.491091]  batch: 200  d_loss: 0.255363  g_loss: 0.024881\n",
      "Elapsed: [0:21:53.019892]  batch: 250  d_loss: -0.089097  g_loss: 0.077322\n",
      "Elapsed: [0:21:56.573215]  batch: 300  d_loss: -0.051705  g_loss: 0.192681\n",
      "Elapsed: [0:22:00.085326]  batch: 350  d_loss: -0.032515  g_loss: 0.262882\n",
      "Elapsed: [0:22:03.597849]  batch: 400  d_loss: -0.115904  g_loss: 0.293429\n",
      "Elapsed: [0:22:07.114376]  batch: 450  d_loss: 0.017467  g_loss: 0.261089\n",
      "Elapsed: [0:22:10.690492]  batch: 500  d_loss: -0.161040  g_loss: 0.329778\n",
      "Time taken for epoch: 44.486 secs\n",
      "ticker =  501\n",
      "\n",
      "Epoch: 2\n",
      "Elapsed: [0:22:19.903823]  batch: 1  d_loss: -0.045271  g_loss: 0.301372\n",
      "Elapsed: [0:22:23.380525]  batch: 50  d_loss: -0.242839  g_loss: 0.178416\n",
      "Elapsed: [0:22:26.941196]  batch: 100  d_loss: -0.127655  g_loss: 0.383070\n",
      "Elapsed: [0:22:30.493831]  batch: 150  d_loss: -0.029382  g_loss: 0.246281\n",
      "Elapsed: [0:22:34.026209]  batch: 200  d_loss: 0.037313  g_loss: 0.198895\n",
      "Elapsed: [0:22:37.587353]  batch: 250  d_loss: -0.133173  g_loss: 0.278769\n",
      "Elapsed: [0:22:41.908914]  batch: 300  d_loss: -0.128779  g_loss: 0.134395\n",
      "Elapsed: [0:22:45.576917]  batch: 350  d_loss: -0.253655  g_loss: 0.399267\n",
      "Elapsed: [0:22:49.309140]  batch: 400  d_loss: -0.023258  g_loss: 0.260584\n",
      "Elapsed: [0:22:53.022583]  batch: 450  d_loss: -0.195351  g_loss: 0.201167\n",
      "Elapsed: [0:22:56.707245]  batch: 500  d_loss: -0.082485  g_loss: 0.215085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for epoch: 45.935 secs\n",
      "ticker =  1001\n",
      "\n",
      "Epoch: 3\n",
      "Elapsed: [0:23:05.936927]  batch: 1  d_loss: -0.181198  g_loss: 0.364263\n",
      "Elapsed: [0:23:09.422366]  batch: 50  d_loss: -0.215988  g_loss: 0.230436\n",
      "Elapsed: [0:23:12.987432]  batch: 100  d_loss: -0.157882  g_loss: 0.240106\n",
      "Elapsed: [0:23:16.603893]  batch: 150  d_loss: -0.036479  g_loss: 0.217309\n",
      "Elapsed: [0:23:20.201865]  batch: 200  d_loss: -0.071626  g_loss: 0.331861\n",
      "Elapsed: [0:23:23.743869]  batch: 250  d_loss: 0.049273  g_loss: 0.176732\n",
      "Elapsed: [0:23:27.296339]  batch: 300  d_loss: -0.053481  g_loss: 0.379103\n",
      "Elapsed: [0:23:30.834893]  batch: 350  d_loss: -0.189251  g_loss: 0.393229\n",
      "Elapsed: [0:23:34.382361]  batch: 400  d_loss: -0.046732  g_loss: 0.261065\n",
      "Elapsed: [0:23:37.950307]  batch: 450  d_loss: -0.177254  g_loss: 0.194913\n",
      "Elapsed: [0:23:41.547307]  batch: 500  d_loss: -0.072497  g_loss: 0.328359\n",
      "Time taken for epoch: 44.687 secs\n",
      "ticker =  1501\n",
      "\n",
      "Epoch: 4\n",
      "Elapsed: [0:23:50.889246]  batch: 1  d_loss: -0.278530  g_loss: 0.380759\n",
      "Elapsed: [0:23:54.373604]  batch: 50  d_loss: -0.071221  g_loss: 0.359299\n",
      "Elapsed: [0:23:57.908500]  batch: 100  d_loss: -0.022442  g_loss: 0.187863\n",
      "Elapsed: [0:24:01.455953]  batch: 150  d_loss: -0.193156  g_loss: 0.336911\n",
      "Elapsed: [0:24:04.989584]  batch: 200  d_loss: 0.064981  g_loss: 0.331985\n",
      "Elapsed: [0:24:08.549573]  batch: 250  d_loss: -0.168168  g_loss: 0.359584\n",
      "Elapsed: [0:24:12.086878]  batch: 300  d_loss: -0.108133  g_loss: 0.407308\n",
      "Elapsed: [0:24:15.654796]  batch: 350  d_loss: -0.259087  g_loss: 0.497332\n",
      "Elapsed: [0:24:19.218925]  batch: 400  d_loss: -0.188489  g_loss: 0.395867\n",
      "Elapsed: [0:24:22.769358]  batch: 450  d_loss: -0.160470  g_loss: 0.183830\n",
      "Elapsed: [0:24:26.338901]  batch: 500  d_loss: -0.150704  g_loss: 0.414554\n",
      "Time taken for epoch: 44.713 secs\n",
      "ticker =  2001\n",
      "\n",
      "Epoch: 5\n",
      "Elapsed: [0:24:35.620885]  batch: 1  d_loss: -0.095078  g_loss: 0.329094\n",
      "Elapsed: [0:24:39.166656]  batch: 50  d_loss: -0.081311  g_loss: 0.393710\n",
      "Elapsed: [0:24:42.692884]  batch: 100  d_loss: -0.038559  g_loss: 0.413151\n",
      "Elapsed: [0:24:46.250591]  batch: 150  d_loss: -0.001850  g_loss: 0.227055\n",
      "Elapsed: [0:24:49.777276]  batch: 200  d_loss: -0.034238  g_loss: 0.203368\n",
      "Elapsed: [0:24:53.333275]  batch: 250  d_loss: -0.158195  g_loss: 0.356223\n",
      "Elapsed: [0:24:56.877347]  batch: 300  d_loss: -0.114092  g_loss: 0.365013\n",
      "Elapsed: [0:25:00.431755]  batch: 350  d_loss: -0.058136  g_loss: 0.327131\n",
      "Elapsed: [0:25:03.947414]  batch: 400  d_loss: -0.084872  g_loss: 0.137525\n",
      "Elapsed: [0:25:07.506939]  batch: 450  d_loss: -0.208583  g_loss: 0.362548\n",
      "Elapsed: [0:25:11.200057]  batch: 500  d_loss: -0.380167  g_loss: 0.333419\n",
      "Time taken for epoch: 44.786 secs\n",
      "ticker =  2501\n",
      "\n",
      "Epoch: 6\n",
      "Elapsed: [0:25:20.527172]  batch: 1  d_loss: -0.005509  g_loss: 0.156196\n",
      "Elapsed: [0:25:23.987728]  batch: 50  d_loss: 0.001857  g_loss: 0.367196\n",
      "Elapsed: [0:25:27.575005]  batch: 100  d_loss: -0.196223  g_loss: 0.270557\n",
      "Elapsed: [0:25:31.096955]  batch: 150  d_loss: -0.078986  g_loss: 0.324923\n",
      "Elapsed: [0:25:34.620334]  batch: 200  d_loss: -0.266617  g_loss: 0.403554\n",
      "Elapsed: [0:25:38.150834]  batch: 250  d_loss: -0.163577  g_loss: 0.459398\n",
      "Elapsed: [0:25:41.685819]  batch: 300  d_loss: -0.068126  g_loss: 0.202014\n",
      "Elapsed: [0:25:45.221206]  batch: 350  d_loss: -0.127594  g_loss: 0.412855\n",
      "Elapsed: [0:25:48.777654]  batch: 400  d_loss: -0.155174  g_loss: 0.259842\n",
      "Elapsed: [0:25:52.312754]  batch: 450  d_loss: -0.172624  g_loss: 0.314317\n",
      "Elapsed: [0:25:55.913199]  batch: 500  d_loss: -0.251568  g_loss: 0.204031\n",
      "Time taken for epoch: 44.629 secs\n",
      "ticker =  3001\n",
      "\n",
      "Epoch: 7\n",
      "Elapsed: [0:26:05.149999]  batch: 1  d_loss: -0.190301  g_loss: 0.264700\n",
      "Elapsed: [0:26:08.643376]  batch: 50  d_loss: -0.248182  g_loss: 0.550600\n",
      "Elapsed: [0:26:12.306051]  batch: 100  d_loss: -0.167874  g_loss: 0.422904\n",
      "Elapsed: [0:26:15.836883]  batch: 150  d_loss: -0.106224  g_loss: 0.275453\n",
      "Elapsed: [0:26:19.322892]  batch: 200  d_loss: -0.125695  g_loss: 0.395309\n",
      "Elapsed: [0:26:22.862376]  batch: 250  d_loss: -0.361558  g_loss: 0.157463\n",
      "Elapsed: [0:26:26.461725]  batch: 300  d_loss: -0.158025  g_loss: 0.320829\n",
      "Elapsed: [0:26:30.056938]  batch: 350  d_loss: -0.264441  g_loss: 0.342927\n",
      "Elapsed: [0:26:33.608747]  batch: 400  d_loss: -0.199899  g_loss: 0.421445\n",
      "Elapsed: [0:26:37.142160]  batch: 450  d_loss: -0.112521  g_loss: 0.213335\n",
      "Elapsed: [0:26:40.869976]  batch: 500  d_loss: -0.218082  g_loss: 0.486831\n",
      "Time taken for epoch: 44.859 secs\n",
      "ticker =  3501\n",
      "\n",
      "Epoch: 8\n",
      "Elapsed: [0:26:50.132408]  batch: 1  d_loss: -0.179516  g_loss: 0.571024\n",
      "Elapsed: [0:26:53.562621]  batch: 50  d_loss: -0.278430  g_loss: 0.362510\n",
      "Elapsed: [0:26:57.234050]  batch: 100  d_loss: -0.158475  g_loss: 0.304080\n",
      "Elapsed: [0:27:00.972671]  batch: 150  d_loss: -0.043726  g_loss: 0.187660\n",
      "Elapsed: [0:27:04.548880]  batch: 200  d_loss: -0.067447  g_loss: 0.253705\n",
      "Elapsed: [0:27:08.083704]  batch: 250  d_loss: -0.039882  g_loss: 0.457837\n",
      "Elapsed: [0:27:11.725113]  batch: 300  d_loss: -0.090362  g_loss: 0.264077\n",
      "Elapsed: [0:27:15.248039]  batch: 350  d_loss: -0.189820  g_loss: 0.417997\n",
      "Elapsed: [0:27:18.783899]  batch: 400  d_loss: -0.048755  g_loss: 0.148842\n",
      "Elapsed: [0:27:22.349145]  batch: 450  d_loss: -0.163138  g_loss: 0.277265\n",
      "Elapsed: [0:27:25.909443]  batch: 500  d_loss: -0.110449  g_loss: 0.192432\n",
      "Time taken for epoch: 44.953 secs\n",
      "ticker =  4001\n",
      "\n",
      "Epoch: 9\n",
      "Elapsed: [0:27:35.144126]  batch: 1  d_loss: -0.181880  g_loss: 0.415268\n",
      "Elapsed: [0:27:38.697361]  batch: 50  d_loss: -0.152509  g_loss: 0.274933\n",
      "Elapsed: [0:27:42.287374]  batch: 100  d_loss: 0.108472  g_loss: 0.202667\n",
      "Elapsed: [0:27:45.828944]  batch: 150  d_loss: -0.093429  g_loss: 0.202010\n",
      "Elapsed: [0:27:49.360599]  batch: 200  d_loss: 0.060385  g_loss: 0.256927\n",
      "Elapsed: [0:27:52.917630]  batch: 250  d_loss: -0.156460  g_loss: 0.276727\n",
      "Elapsed: [0:27:56.474066]  batch: 300  d_loss: -0.160640  g_loss: 0.324956\n",
      "Elapsed: [0:27:59.994512]  batch: 350  d_loss: -0.089356  g_loss: 0.346857\n",
      "Elapsed: [0:28:03.516844]  batch: 400  d_loss: -0.189573  g_loss: 0.275761\n",
      "Elapsed: [0:28:07.018064]  batch: 450  d_loss: -0.056138  g_loss: 0.198839\n",
      "Elapsed: [0:28:10.527454]  batch: 500  d_loss: -0.119628  g_loss: 0.362073\n",
      "Time taken for epoch: 44.526 secs\n",
      "ticker =  4501\n",
      "\n",
      "Epoch: 10\n",
      "Elapsed: [0:28:19.742407]  batch: 1  d_loss: -0.071319  g_loss: 0.267646\n",
      "Elapsed: [0:28:23.232551]  batch: 50  d_loss: -0.145367  g_loss: 0.432990\n",
      "Elapsed: [0:28:26.850702]  batch: 100  d_loss: -0.131181  g_loss: 0.366175\n",
      "Elapsed: [0:28:30.486235]  batch: 150  d_loss: -0.109419  g_loss: 0.279670\n",
      "Elapsed: [0:28:34.081469]  batch: 200  d_loss: -0.135782  g_loss: 0.319538\n",
      "Elapsed: [0:28:38.248688]  batch: 250  d_loss: -0.155721  g_loss: 0.296113\n",
      "Elapsed: [0:28:42.330049]  batch: 300  d_loss: -0.106454  g_loss: 0.401948\n",
      "Elapsed: [0:28:46.344190]  batch: 350  d_loss: -0.104765  g_loss: 0.335179\n",
      "Elapsed: [0:28:50.417177]  batch: 400  d_loss: -0.062929  g_loss: 0.143349\n",
      "Elapsed: [0:28:54.415805]  batch: 450  d_loss: -0.208646  g_loss: 0.389985\n",
      "Elapsed: [0:28:58.439538]  batch: 500  d_loss: -0.032454  g_loss: 0.195819\n",
      "Time taken for epoch: 47.865 secs\n",
      "ticker =  5001\n",
      "\n",
      "Epoch: 11\n",
      "Elapsed: [0:29:08.084194]  batch: 1  d_loss: -0.150113  g_loss: 0.305201\n",
      "Elapsed: [0:29:11.644627]  batch: 50  d_loss: -0.108646  g_loss: 0.249246\n",
      "Elapsed: [0:29:15.207165]  batch: 100  d_loss: -0.098718  g_loss: 0.290519\n",
      "Elapsed: [0:29:18.773551]  batch: 150  d_loss: 0.013699  g_loss: 0.322992\n",
      "Elapsed: [0:29:22.330784]  batch: 200  d_loss: -0.042132  g_loss: 0.140632\n",
      "Elapsed: [0:29:25.922964]  batch: 250  d_loss: -0.068248  g_loss: 0.280470\n",
      "Elapsed: [0:29:29.458104]  batch: 300  d_loss: -0.147112  g_loss: 0.242257\n",
      "Elapsed: [0:29:32.981168]  batch: 350  d_loss: -0.051583  g_loss: 0.359220\n",
      "Elapsed: [0:29:36.520107]  batch: 400  d_loss: -0.203687  g_loss: 0.336097\n",
      "Elapsed: [0:29:40.103158]  batch: 450  d_loss: -0.147364  g_loss: 0.337458\n",
      "Elapsed: [0:29:43.690098]  batch: 500  d_loss: -0.137036  g_loss: 0.344677\n",
      "Time taken for epoch: 45.116 secs\n",
      "ticker =  5501\n",
      "\n",
      "Epoch: 12\n",
      "Elapsed: [0:29:52.989947]  batch: 1  d_loss: -0.150810  g_loss: 0.211954\n",
      "Elapsed: [0:29:56.470522]  batch: 50  d_loss: -0.133899  g_loss: 0.392014\n",
      "Elapsed: [0:30:00.106638]  batch: 100  d_loss: -0.251529  g_loss: 0.335964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: [0:30:03.712946]  batch: 150  d_loss: -0.148144  g_loss: 0.286676\n",
      "Elapsed: [0:30:07.276024]  batch: 200  d_loss: -0.236031  g_loss: 0.428189\n",
      "Elapsed: [0:30:10.922878]  batch: 250  d_loss: -0.157158  g_loss: 0.334931\n",
      "Elapsed: [0:30:14.536572]  batch: 300  d_loss: -0.166306  g_loss: 0.218399\n",
      "Elapsed: [0:30:18.176631]  batch: 350  d_loss: -0.161453  g_loss: 0.504371\n",
      "Elapsed: [0:30:21.719727]  batch: 400  d_loss: -0.103515  g_loss: 0.265895\n",
      "Elapsed: [0:30:25.297792]  batch: 450  d_loss: -0.006697  g_loss: 0.303266\n",
      "Elapsed: [0:30:28.873962]  batch: 500  d_loss: -0.072737  g_loss: 0.277854\n",
      "Time taken for epoch: 45.115 secs\n",
      "ticker =  6001\n",
      "\n",
      "Epoch: 13\n",
      "Elapsed: [0:30:38.116689]  batch: 1  d_loss: -0.190841  g_loss: 0.329919\n",
      "Elapsed: [0:30:41.645602]  batch: 50  d_loss: -0.222853  g_loss: 0.385628\n",
      "Elapsed: [0:30:45.262383]  batch: 100  d_loss: -0.205048  g_loss: 0.384488\n",
      "Elapsed: [0:30:48.836886]  batch: 150  d_loss: -0.119791  g_loss: 0.233025\n",
      "Elapsed: [0:30:52.709090]  batch: 200  d_loss: -0.099526  g_loss: 0.168420\n",
      "Elapsed: [0:30:56.333676]  batch: 250  d_loss: -0.215862  g_loss: 0.410113\n",
      "Elapsed: [0:30:59.911772]  batch: 300  d_loss: -0.084302  g_loss: 0.327139\n",
      "Elapsed: [0:31:03.535904]  batch: 350  d_loss: -0.144632  g_loss: 0.229236\n",
      "Elapsed: [0:31:07.102671]  batch: 400  d_loss: -0.136780  g_loss: 0.236412\n",
      "Elapsed: [0:31:10.763497]  batch: 450  d_loss: -0.134713  g_loss: 0.231502\n",
      "Elapsed: [0:31:14.420079]  batch: 500  d_loss: -0.219506  g_loss: 0.318933\n",
      "Time taken for epoch: 45.427 secs\n",
      "ticker =  6501\n",
      "\n",
      "Epoch: 14\n",
      "Elapsed: [0:31:23.819050]  batch: 1  d_loss: 0.020571  g_loss: 0.076221\n",
      "Elapsed: [0:31:27.438744]  batch: 50  d_loss: -0.187622  g_loss: 0.493041\n",
      "Elapsed: [0:31:31.075645]  batch: 100  d_loss: -0.051228  g_loss: 0.167807\n",
      "Elapsed: [0:31:34.664117]  batch: 150  d_loss: -0.022946  g_loss: 0.248770\n",
      "Elapsed: [0:31:38.236409]  batch: 200  d_loss: -0.248259  g_loss: 0.356638\n",
      "Elapsed: [0:31:41.846995]  batch: 250  d_loss: -0.091621  g_loss: 0.284585\n",
      "Elapsed: [0:31:45.494695]  batch: 300  d_loss: -0.266941  g_loss: 0.292421\n",
      "Elapsed: [0:31:49.067378]  batch: 350  d_loss: -0.025506  g_loss: 0.176372\n",
      "Elapsed: [0:31:52.665610]  batch: 400  d_loss: -0.033466  g_loss: 0.266706\n",
      "Elapsed: [0:31:56.267846]  batch: 450  d_loss: -0.083133  g_loss: 0.221652\n",
      "Elapsed: [0:31:59.834254]  batch: 500  d_loss: -0.115640  g_loss: 0.237119\n",
      "Time taken for epoch: 45.384 secs\n",
      "ticker =  7001\n",
      "\n",
      "Epoch: 15\n",
      "Elapsed: [0:32:09.153406]  batch: 1  d_loss: -0.115589  g_loss: 0.280986\n",
      "Elapsed: [0:32:12.697171]  batch: 50  d_loss: -0.187583  g_loss: 0.375656\n",
      "Elapsed: [0:32:16.287731]  batch: 100  d_loss: 0.007520  g_loss: 0.006105\n",
      "Elapsed: [0:32:19.842155]  batch: 150  d_loss: 0.017138  g_loss: 0.165070\n",
      "Elapsed: [0:32:23.406893]  batch: 200  d_loss: -0.034489  g_loss: 0.145049\n",
      "Elapsed: [0:32:26.967367]  batch: 250  d_loss: -0.088633  g_loss: 0.311861\n",
      "Elapsed: [0:32:30.526169]  batch: 300  d_loss: -0.112413  g_loss: 0.400560\n",
      "Elapsed: [0:32:34.113143]  batch: 350  d_loss: -0.072499  g_loss: 0.246026\n",
      "Elapsed: [0:32:37.727443]  batch: 400  d_loss: -0.131524  g_loss: 0.311203\n",
      "Elapsed: [0:32:41.345479]  batch: 450  d_loss: -0.163383  g_loss: 0.240346\n",
      "Elapsed: [0:32:44.936109]  batch: 500  d_loss: -0.059766  g_loss: 0.186926\n",
      "Time taken for epoch: 45.000 secs\n",
      "ticker =  7501\n",
      "\n",
      "Epoch: 16\n",
      "Elapsed: [0:32:54.428107]  batch: 1  d_loss: -0.121703  g_loss: 0.247880\n",
      "Elapsed: [0:32:57.913966]  batch: 50  d_loss: -0.014079  g_loss: 0.185961\n",
      "Elapsed: [0:33:01.451657]  batch: 100  d_loss: -0.077608  g_loss: 0.247634\n",
      "Elapsed: [0:33:05.024187]  batch: 150  d_loss: 0.080691  g_loss: 0.175538\n",
      "Elapsed: [0:33:08.655047]  batch: 200  d_loss: -0.062646  g_loss: 0.130046\n",
      "Elapsed: [0:33:12.356834]  batch: 250  d_loss: -0.098286  g_loss: 0.178741\n",
      "Elapsed: [0:33:15.963231]  batch: 300  d_loss: 0.023899  g_loss: 0.102223\n",
      "Elapsed: [0:33:19.535019]  batch: 350  d_loss: -0.076626  g_loss: 0.275213\n",
      "Elapsed: [0:33:23.119116]  batch: 400  d_loss: -0.018732  g_loss: 0.215893\n",
      "Elapsed: [0:33:26.736933]  batch: 450  d_loss: -0.059691  g_loss: 0.261431\n",
      "Elapsed: [0:33:30.324700]  batch: 500  d_loss: -0.099296  g_loss: 0.301334\n",
      "Time taken for epoch: 45.302 secs\n",
      "ticker =  8001\n",
      "\n",
      "Epoch: 17\n",
      "Elapsed: [0:33:39.733258]  batch: 1  d_loss: -0.035008  g_loss: 0.180278\n",
      "Elapsed: [0:33:43.249166]  batch: 50  d_loss: 0.023564  g_loss: -0.007327\n",
      "Elapsed: [0:33:46.842582]  batch: 100  d_loss: -0.110735  g_loss: 0.389589\n",
      "Elapsed: [0:33:50.433095]  batch: 150  d_loss: -0.037647  g_loss: 0.265037\n",
      "Elapsed: [0:33:53.987615]  batch: 200  d_loss: 0.054604  g_loss: 0.159928\n",
      "Elapsed: [0:33:57.585057]  batch: 250  d_loss: -0.041593  g_loss: 0.216022\n",
      "Elapsed: [0:34:01.166986]  batch: 300  d_loss: -0.030967  g_loss: 0.232021\n",
      "Elapsed: [0:34:04.752920]  batch: 350  d_loss: -0.117543  g_loss: 0.347184\n",
      "Elapsed: [0:34:08.346950]  batch: 400  d_loss: 0.004805  g_loss: 0.348062\n",
      "Elapsed: [0:34:12.010193]  batch: 450  d_loss: -0.127793  g_loss: 0.242687\n",
      "Elapsed: [0:34:15.584109]  batch: 500  d_loss: -0.038014  g_loss: 0.176356\n",
      "Time taken for epoch: 45.147 secs\n",
      "ticker =  8501\n",
      "\n",
      "Epoch: 18\n",
      "Elapsed: [0:34:24.845242]  batch: 1  d_loss: -0.201704  g_loss: 0.384243\n",
      "Elapsed: [0:34:28.370331]  batch: 50  d_loss: 0.072380  g_loss: 0.218322\n",
      "Elapsed: [0:34:31.940928]  batch: 100  d_loss: -0.055030  g_loss: 0.254702\n",
      "Elapsed: [0:34:35.557980]  batch: 150  d_loss: -0.013680  g_loss: 0.109969\n",
      "Elapsed: [0:34:39.128257]  batch: 200  d_loss: -0.043298  g_loss: 0.217499\n",
      "Elapsed: [0:34:42.702755]  batch: 250  d_loss: -0.001802  g_loss: 0.115569\n",
      "Elapsed: [0:34:46.311379]  batch: 300  d_loss: -0.088358  g_loss: 0.209430\n",
      "Elapsed: [0:34:49.889661]  batch: 350  d_loss: -0.085876  g_loss: 0.237538\n",
      "Elapsed: [0:34:53.557570]  batch: 400  d_loss: -0.015288  g_loss: 0.214083\n",
      "Elapsed: [0:34:57.149775]  batch: 450  d_loss: -0.143076  g_loss: 0.252754\n",
      "Elapsed: [0:35:00.755861]  batch: 500  d_loss: -0.068500  g_loss: 0.238808\n",
      "Time taken for epoch: 45.081 secs\n",
      "ticker =  9001\n",
      "\n",
      "Epoch: 19\n",
      "Elapsed: [0:35:10.152181]  batch: 1  d_loss: 0.010474  g_loss: 0.198018\n",
      "Elapsed: [0:35:13.723598]  batch: 50  d_loss: -0.007504  g_loss: 0.120475\n",
      "Elapsed: [0:35:17.337737]  batch: 100  d_loss: -0.118233  g_loss: 0.304010\n",
      "Elapsed: [0:35:20.948085]  batch: 150  d_loss: 0.048084  g_loss: 0.124779\n",
      "Elapsed: [0:35:24.570913]  batch: 200  d_loss: -0.107565  g_loss: 0.126016\n",
      "Elapsed: [0:35:28.193703]  batch: 250  d_loss: -0.012242  g_loss: 0.099451\n",
      "Elapsed: [0:35:31.778966]  batch: 300  d_loss: 0.005312  g_loss: 0.060888\n",
      "Elapsed: [0:35:35.393420]  batch: 350  d_loss: -0.094226  g_loss: 0.194782\n",
      "Elapsed: [0:35:38.991697]  batch: 400  d_loss: -0.126172  g_loss: 0.336455\n",
      "Elapsed: [0:35:42.812145]  batch: 450  d_loss: -0.033456  g_loss: 0.121647\n",
      "Elapsed: [0:35:46.619294]  batch: 500  d_loss: 0.048374  g_loss: 0.149740\n",
      "Time taken for epoch: 45.817 secs\n",
      "ticker =  9501\n",
      "\n",
      "Epoch: 20\n",
      "Elapsed: [0:35:56.254863]  batch: 1  d_loss: -0.050025  g_loss: 0.130015\n",
      "Elapsed: [0:35:59.757942]  batch: 50  d_loss: -0.009690  g_loss: 0.126567\n",
      "Elapsed: [0:36:03.316690]  batch: 100  d_loss: -0.015513  g_loss: 0.177290\n",
      "Elapsed: [0:36:06.880114]  batch: 150  d_loss: -0.020385  g_loss: 0.184342\n",
      "Elapsed: [0:36:10.446320]  batch: 200  d_loss: 0.090011  g_loss: 0.053263\n",
      "Elapsed: [0:36:14.055390]  batch: 250  d_loss: 0.009746  g_loss: 0.271002\n",
      "Elapsed: [0:36:17.667788]  batch: 300  d_loss: -0.062503  g_loss: 0.284548\n",
      "Elapsed: [0:36:21.221968]  batch: 350  d_loss: 0.064082  g_loss: 0.238940\n",
      "Elapsed: [0:36:24.810777]  batch: 400  d_loss: -0.018998  g_loss: 0.071526\n",
      "Elapsed: [0:36:28.365910]  batch: 450  d_loss: -0.053707  g_loss: 0.191486\n",
      "Elapsed: [0:36:31.939784]  batch: 500  d_loss: -0.056318  g_loss: 0.225276\n",
      "Time taken for epoch: 45.172 secs\n",
      "ticker =  10001\n",
      "\n",
      "Epoch: 21\n",
      "Elapsed: [0:36:41.276720]  batch: 1  d_loss: -0.100776  g_loss: 0.201592\n",
      "Elapsed: [0:36:44.819412]  batch: 50  d_loss: -0.087708  g_loss: 0.208525\n",
      "Elapsed: [0:36:48.410264]  batch: 100  d_loss: 0.106559  g_loss: 0.219639\n",
      "Elapsed: [0:36:51.971640]  batch: 150  d_loss: -0.116326  g_loss: 0.257428\n",
      "Elapsed: [0:36:55.550360]  batch: 200  d_loss: 0.018659  g_loss: 0.112805\n",
      "Elapsed: [0:36:59.162797]  batch: 250  d_loss: -0.025440  g_loss: 0.204411\n",
      "Elapsed: [0:37:02.813066]  batch: 300  d_loss: -0.109559  g_loss: 0.222745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: [0:37:06.425317]  batch: 350  d_loss: -0.092230  g_loss: 0.162785\n",
      "Elapsed: [0:37:10.033333]  batch: 400  d_loss: -0.005325  g_loss: 0.067522\n",
      "Elapsed: [0:37:13.641144]  batch: 450  d_loss: -0.048725  g_loss: 0.169767\n",
      "Elapsed: [0:37:17.169788]  batch: 500  d_loss: 0.036916  g_loss: 0.044423\n",
      "Time taken for epoch: 45.166 secs\n",
      "ticker =  10501\n",
      "\n",
      "Epoch: 22\n",
      "Elapsed: [0:37:26.559984]  batch: 1  d_loss: -0.120890  g_loss: 0.152200\n",
      "Elapsed: [0:37:30.119473]  batch: 50  d_loss: -0.076269  g_loss: 0.247635\n",
      "Elapsed: [0:37:33.673249]  batch: 100  d_loss: -0.119219  g_loss: 0.139429\n",
      "Elapsed: [0:37:37.166737]  batch: 150  d_loss: -0.067780  g_loss: 0.134537\n",
      "Elapsed: [0:37:40.739729]  batch: 200  d_loss: 0.001495  g_loss: 0.150853\n",
      "Elapsed: [0:37:44.310158]  batch: 250  d_loss: 0.088237  g_loss: 0.075841\n",
      "Elapsed: [0:37:47.862259]  batch: 300  d_loss: 0.045798  g_loss: 0.056956\n",
      "Elapsed: [0:37:51.460641]  batch: 350  d_loss: -0.093784  g_loss: 0.250483\n",
      "Elapsed: [0:37:55.078337]  batch: 400  d_loss: 0.096442  g_loss: 0.090947\n",
      "Elapsed: [0:37:58.728323]  batch: 450  d_loss: -0.019462  g_loss: 0.278437\n",
      "Elapsed: [0:38:02.259608]  batch: 500  d_loss: -0.089745  g_loss: 0.389631\n",
      "Time taken for epoch: 44.986 secs\n",
      "ticker =  11001\n",
      "\n",
      "Epoch: 23\n",
      "Elapsed: [0:38:11.575193]  batch: 1  d_loss: -0.072454  g_loss: 0.417734\n",
      "Elapsed: [0:38:15.031652]  batch: 50  d_loss: 0.001059  g_loss: 0.106184\n",
      "Elapsed: [0:38:18.570098]  batch: 100  d_loss: -0.127351  g_loss: 0.084811\n",
      "Elapsed: [0:38:22.123818]  batch: 150  d_loss: 0.006162  g_loss: 0.147443\n",
      "Elapsed: [0:38:25.662819]  batch: 200  d_loss: 0.035408  g_loss: 0.184046\n",
      "Elapsed: [0:38:29.256526]  batch: 250  d_loss: -0.039301  g_loss: 0.196120\n",
      "Elapsed: [0:38:32.867505]  batch: 300  d_loss: 0.040182  g_loss: 0.190547\n",
      "Elapsed: [0:38:36.457237]  batch: 350  d_loss: -0.060953  g_loss: 0.065888\n",
      "Elapsed: [0:38:39.994213]  batch: 400  d_loss: -0.044179  g_loss: 0.092542\n",
      "Elapsed: [0:38:43.566492]  batch: 450  d_loss: -0.075264  g_loss: 0.250535\n",
      "Elapsed: [0:38:47.122310]  batch: 500  d_loss: -0.047780  g_loss: 0.095001\n",
      "Time taken for epoch: 44.784 secs\n",
      "ticker =  11501\n",
      "\n",
      "Epoch: 24\n",
      "Elapsed: [0:38:56.380307]  batch: 1  d_loss: -0.041945  g_loss: 0.042860\n",
      "Elapsed: [0:38:59.887447]  batch: 50  d_loss: -0.064538  g_loss: 0.101625\n",
      "Elapsed: [0:39:03.449354]  batch: 100  d_loss: -0.107775  g_loss: 0.144824\n",
      "Elapsed: [0:39:07.012601]  batch: 150  d_loss: -0.091431  g_loss: 0.195930\n",
      "Elapsed: [0:39:10.654477]  batch: 200  d_loss: 0.012047  g_loss: 0.261157\n",
      "Elapsed: [0:39:14.236342]  batch: 250  d_loss: 0.024291  g_loss: 0.117286\n",
      "Elapsed: [0:39:17.760445]  batch: 300  d_loss: 0.019480  g_loss: 0.172497\n",
      "Elapsed: [0:39:21.288336]  batch: 350  d_loss: 0.027962  g_loss: 0.165880\n",
      "Elapsed: [0:39:24.897387]  batch: 400  d_loss: 0.022207  g_loss: 0.150340\n",
      "Elapsed: [0:39:28.462364]  batch: 450  d_loss: -0.045210  g_loss: 0.070412\n",
      "Elapsed: [0:39:32.004491]  batch: 500  d_loss: -0.020191  g_loss: 0.016168\n",
      "Time taken for epoch: 44.799 secs\n",
      "ticker =  12001\n",
      "\n",
      "Epoch: 25\n",
      "Elapsed: [0:39:41.281369]  batch: 1  d_loss: -0.076741  g_loss: 0.138104\n",
      "Elapsed: [0:39:44.781277]  batch: 50  d_loss: -0.016739  g_loss: 0.236189\n",
      "Elapsed: [0:39:48.291903]  batch: 100  d_loss: 0.015152  g_loss: 0.111070\n",
      "Elapsed: [0:39:52.049743]  batch: 150  d_loss: -0.078524  g_loss: 0.244798\n",
      "Elapsed: [0:39:55.621984]  batch: 200  d_loss: 0.056763  g_loss: 0.145396\n",
      "Elapsed: [0:39:59.142025]  batch: 250  d_loss: -0.013631  g_loss: 0.269815\n",
      "Elapsed: [0:40:02.707732]  batch: 300  d_loss: -0.071474  g_loss: 0.194899\n",
      "Elapsed: [0:40:06.206623]  batch: 350  d_loss: -0.063571  g_loss: 0.097504\n",
      "Elapsed: [0:40:09.755041]  batch: 400  d_loss: -0.048034  g_loss: 0.217369\n",
      "Elapsed: [0:40:13.306411]  batch: 450  d_loss: 0.050124  g_loss: 0.111989\n",
      "Elapsed: [0:40:16.852973]  batch: 500  d_loss: -0.081332  g_loss: 0.184003\n",
      "Time taken for epoch: 44.775 secs\n",
      "ticker =  12501\n",
      "\n",
      "Epoch: 26\n",
      "Elapsed: [0:40:26.086540]  batch: 1  d_loss: -0.007166  g_loss: 0.096781\n",
      "Elapsed: [0:40:29.596602]  batch: 50  d_loss: -0.021587  g_loss: 0.145771\n",
      "Elapsed: [0:40:33.114488]  batch: 100  d_loss: -0.058929  g_loss: 0.067722\n",
      "Elapsed: [0:40:36.673862]  batch: 150  d_loss: -0.056940  g_loss: 0.154741\n",
      "Elapsed: [0:40:40.216409]  batch: 200  d_loss: 0.002491  g_loss: 0.161890\n",
      "Elapsed: [0:40:43.781571]  batch: 250  d_loss: -0.097227  g_loss: 0.257889\n",
      "Elapsed: [0:40:47.332897]  batch: 300  d_loss: -0.000923  g_loss: 0.061235\n",
      "Elapsed: [0:40:50.889124]  batch: 350  d_loss: -0.036116  g_loss: 0.135149\n",
      "Elapsed: [0:40:54.580857]  batch: 400  d_loss: -0.060821  g_loss: 0.159099\n",
      "Elapsed: [0:40:58.158420]  batch: 450  d_loss: -0.043954  g_loss: 0.140827\n",
      "Elapsed: [0:41:01.736430]  batch: 500  d_loss: -0.038167  g_loss: 0.102864\n",
      "Time taken for epoch: 44.808 secs\n",
      "ticker =  13001\n",
      "\n",
      "Epoch: 27\n",
      "Elapsed: [0:41:11.000424]  batch: 1  d_loss: -0.022721  g_loss: 0.113879\n",
      "Elapsed: [0:41:14.527993]  batch: 50  d_loss: -0.118744  g_loss: 0.106239\n",
      "Elapsed: [0:41:18.014865]  batch: 100  d_loss: -0.052435  g_loss: 0.156465\n",
      "Elapsed: [0:41:21.489138]  batch: 150  d_loss: 0.020380  g_loss: 0.128000\n",
      "Elapsed: [0:41:24.990151]  batch: 200  d_loss: -0.047011  g_loss: 0.071701\n",
      "Elapsed: [0:41:28.505901]  batch: 250  d_loss: 0.046511  g_loss: 0.127428\n",
      "Elapsed: [0:41:32.054646]  batch: 300  d_loss: -0.037584  g_loss: 0.130761\n",
      "Elapsed: [0:41:35.567915]  batch: 350  d_loss: 0.021010  g_loss: 0.102134\n",
      "Elapsed: [0:41:39.081490]  batch: 400  d_loss: -0.041464  g_loss: 0.127223\n",
      "Elapsed: [0:41:42.614773]  batch: 450  d_loss: -0.007492  g_loss: 0.168388\n",
      "Elapsed: [0:41:46.128038]  batch: 500  d_loss: 0.002124  g_loss: 0.116017\n",
      "Time taken for epoch: 44.294 secs\n",
      "ticker =  13501\n",
      "\n",
      "Epoch: 28\n",
      "Elapsed: [0:41:55.307407]  batch: 1  d_loss: -0.028429  g_loss: 0.165822\n",
      "Elapsed: [0:41:58.942204]  batch: 50  d_loss: -0.102238  g_loss: 0.229733\n",
      "Elapsed: [0:42:02.597301]  batch: 100  d_loss: 0.019786  g_loss: 0.027707\n",
      "Elapsed: [0:42:06.164027]  batch: 150  d_loss: 0.012818  g_loss: 0.133047\n",
      "Elapsed: [0:42:09.788019]  batch: 200  d_loss: -0.024820  g_loss: 0.121058\n",
      "Elapsed: [0:42:13.451673]  batch: 250  d_loss: -0.003230  g_loss: 0.126763\n",
      "Elapsed: [0:42:17.025508]  batch: 300  d_loss: 0.035156  g_loss: 0.105806\n",
      "Elapsed: [0:42:20.617326]  batch: 350  d_loss: -0.058529  g_loss: 0.154649\n",
      "Elapsed: [0:42:24.233324]  batch: 400  d_loss: -0.048481  g_loss: 0.170735\n",
      "Elapsed: [0:42:27.780406]  batch: 450  d_loss: 0.056053  g_loss: 0.097098\n",
      "Elapsed: [0:42:31.350269]  batch: 500  d_loss: -0.006894  g_loss: 0.060370\n",
      "Time taken for epoch: 45.126 secs\n",
      "ticker =  14001\n",
      "\n",
      "Epoch: 29\n",
      "Elapsed: [0:42:40.612529]  batch: 1  d_loss: -0.024839  g_loss: 0.145111\n",
      "Elapsed: [0:42:44.103057]  batch: 50  d_loss: 0.000905  g_loss: 0.049055\n",
      "Elapsed: [0:42:47.659588]  batch: 100  d_loss: -0.008519  g_loss: 0.078127\n",
      "Elapsed: [0:42:51.182109]  batch: 150  d_loss: -0.032855  g_loss: 0.105697\n",
      "Elapsed: [0:42:54.689395]  batch: 200  d_loss: 0.049989  g_loss: 0.064064\n",
      "Elapsed: [0:42:58.254791]  batch: 250  d_loss: -0.063485  g_loss: 0.117213\n",
      "Elapsed: [0:43:01.845360]  batch: 300  d_loss: -0.052410  g_loss: 0.074132\n",
      "Elapsed: [0:43:05.546782]  batch: 350  d_loss: -0.004318  g_loss: 0.141763\n",
      "Elapsed: [0:43:09.145465]  batch: 400  d_loss: -0.004075  g_loss: 0.102021\n",
      "Elapsed: [0:43:12.850070]  batch: 450  d_loss: 0.022159  g_loss: 0.103469\n",
      "Elapsed: [0:43:16.398071]  batch: 500  d_loss: -0.104213  g_loss: 0.130615\n",
      "Time taken for epoch: 44.977 secs\n",
      "ticker =  14501\n",
      "\n",
      "Epoch: 30\n",
      "Elapsed: [0:43:25.687368]  batch: 1  d_loss: 0.011454  g_loss: 0.031640\n",
      "Elapsed: [0:43:29.148561]  batch: 50  d_loss: 0.007982  g_loss: 0.093718\n",
      "Elapsed: [0:43:32.760638]  batch: 100  d_loss: -0.037162  g_loss: 0.157031\n",
      "Elapsed: [0:43:36.282038]  batch: 150  d_loss: -0.013497  g_loss: 0.116116\n",
      "Elapsed: [0:43:39.905753]  batch: 200  d_loss: 0.006163  g_loss: 0.111423\n",
      "Elapsed: [0:43:43.447009]  batch: 250  d_loss: 0.006834  g_loss: 0.105694\n",
      "Elapsed: [0:43:47.056614]  batch: 300  d_loss: 0.015138  g_loss: 0.063274\n",
      "Elapsed: [0:43:50.615524]  batch: 350  d_loss: -0.065284  g_loss: 0.125231\n",
      "Elapsed: [0:43:54.157237]  batch: 400  d_loss: -0.038454  g_loss: 0.107978\n",
      "Elapsed: [0:43:57.765527]  batch: 450  d_loss: -0.055405  g_loss: 0.160406\n",
      "Elapsed: [0:44:01.410275]  batch: 500  d_loss: 0.032946  g_loss: 0.034619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for epoch: 44.925 secs\n",
      "ticker =  15001\n",
      "\n",
      "Epoch: 31\n",
      "Elapsed: [0:44:10.789604]  batch: 1  d_loss: -0.022638  g_loss: 0.050372\n",
      "Elapsed: [0:44:14.271452]  batch: 50  d_loss: -0.058183  g_loss: 0.076016\n",
      "Elapsed: [0:44:17.827355]  batch: 100  d_loss: -0.044577  g_loss: 0.169207\n",
      "Elapsed: [0:44:21.361224]  batch: 150  d_loss: -0.001566  g_loss: 0.047826\n",
      "Elapsed: [0:44:24.933508]  batch: 200  d_loss: -0.009678  g_loss: 0.122778\n",
      "Elapsed: [0:44:28.466142]  batch: 250  d_loss: -0.059768  g_loss: 0.153710\n",
      "Elapsed: [0:44:31.979506]  batch: 300  d_loss: -0.016196  g_loss: 0.065966\n",
      "Elapsed: [0:44:35.528024]  batch: 350  d_loss: -0.036178  g_loss: 0.127399\n",
      "Elapsed: [0:44:39.105097]  batch: 400  d_loss: -0.003368  g_loss: 0.135611\n",
      "Elapsed: [0:44:42.640713]  batch: 450  d_loss: -0.025253  g_loss: 0.105364\n",
      "Elapsed: [0:44:46.191248]  batch: 500  d_loss: -0.026102  g_loss: 0.099146\n",
      "Time taken for epoch: 44.690 secs\n",
      "ticker =  15501\n",
      "\n",
      "Epoch: 32\n",
      "Elapsed: [0:44:55.490413]  batch: 1  d_loss: -0.018892  g_loss: 0.134439\n",
      "Elapsed: [0:44:58.960313]  batch: 50  d_loss: 0.038368  g_loss: 0.077675\n",
      "Elapsed: [0:45:02.532127]  batch: 100  d_loss: -0.033180  g_loss: 0.177503\n",
      "Elapsed: [0:45:06.324252]  batch: 150  d_loss: -0.078503  g_loss: 0.228903\n",
      "Elapsed: [0:45:10.022125]  batch: 200  d_loss: 0.001617  g_loss: 0.154281\n",
      "Elapsed: [0:45:13.666061]  batch: 250  d_loss: -0.024708  g_loss: 0.108974\n",
      "Elapsed: [0:45:17.223621]  batch: 300  d_loss: -0.020026  g_loss: 0.136438\n",
      "Elapsed: [0:45:20.825564]  batch: 350  d_loss: -0.001699  g_loss: 0.082979\n",
      "Elapsed: [0:45:24.326690]  batch: 400  d_loss: 0.010374  g_loss: 0.038032\n",
      "Elapsed: [0:45:27.876691]  batch: 450  d_loss: -0.001222  g_loss: 0.078234\n",
      "Elapsed: [0:45:31.387545]  batch: 500  d_loss: -0.008036  g_loss: 0.198968\n",
      "Time taken for epoch: 45.114 secs\n",
      "ticker =  16001\n",
      "\n",
      "Epoch: 33\n",
      "Elapsed: [0:45:40.597025]  batch: 1  d_loss: 0.056264  g_loss: 0.031402\n",
      "Elapsed: [0:45:44.068016]  batch: 50  d_loss: -0.062375  g_loss: 0.074155\n",
      "Elapsed: [0:45:47.563041]  batch: 100  d_loss: -0.052837  g_loss: 0.106012\n",
      "Elapsed: [0:45:51.066473]  batch: 150  d_loss: -0.062088  g_loss: 0.163459\n",
      "Elapsed: [0:45:54.567570]  batch: 200  d_loss: -0.017279  g_loss: 0.161886\n",
      "Elapsed: [0:45:58.116223]  batch: 250  d_loss: 0.023454  g_loss: 0.043871\n",
      "Elapsed: [0:46:01.685589]  batch: 300  d_loss: -0.002605  g_loss: 0.054904\n",
      "Elapsed: [0:46:05.248029]  batch: 350  d_loss: -0.025488  g_loss: 0.129615\n",
      "Elapsed: [0:46:08.786584]  batch: 400  d_loss: -0.076242  g_loss: 0.141686\n",
      "Elapsed: [0:46:12.386039]  batch: 450  d_loss: 0.024105  g_loss: 0.093698\n",
      "Elapsed: [0:46:15.914088]  batch: 500  d_loss: 0.007438  g_loss: 0.021725\n",
      "Time taken for epoch: 44.437 secs\n",
      "ticker =  16501\n",
      "\n",
      "Epoch: 34\n",
      "Elapsed: [0:46:25.097037]  batch: 1  d_loss: -0.077176  g_loss: 0.111427\n",
      "Elapsed: [0:46:28.511342]  batch: 50  d_loss: 0.015188  g_loss: 0.035299\n",
      "Elapsed: [0:46:32.026736]  batch: 100  d_loss: -0.089376  g_loss: 0.110262\n",
      "Elapsed: [0:46:35.555788]  batch: 150  d_loss: -0.028355  g_loss: 0.129470\n",
      "Elapsed: [0:46:39.036618]  batch: 200  d_loss: -0.027124  g_loss: 0.084573\n",
      "Elapsed: [0:46:42.595756]  batch: 250  d_loss: -0.049505  g_loss: 0.142324\n",
      "Elapsed: [0:46:46.120597]  batch: 300  d_loss: -0.064294  g_loss: 0.151086\n",
      "Elapsed: [0:46:49.619328]  batch: 350  d_loss: -0.018709  g_loss: 0.085990\n",
      "Elapsed: [0:46:53.166317]  batch: 400  d_loss: 0.027105  g_loss: 0.048237\n",
      "Elapsed: [0:46:56.712654]  batch: 450  d_loss: -0.023462  g_loss: 0.166703\n",
      "Elapsed: [0:47:00.227140]  batch: 500  d_loss: 0.016626  g_loss: 0.111262\n",
      "Time taken for epoch: 44.225 secs\n",
      "ticker =  17001\n",
      "\n",
      "Epoch: 35\n",
      "Elapsed: [0:47:09.508638]  batch: 1  d_loss: -0.013827  g_loss: 0.143589\n",
      "Elapsed: [0:47:13.139303]  batch: 50  d_loss: -0.058346  g_loss: 0.103754\n",
      "Elapsed: [0:47:16.636326]  batch: 100  d_loss: -0.010462  g_loss: 0.064834\n",
      "Elapsed: [0:47:20.120738]  batch: 150  d_loss: -0.020257  g_loss: 0.095936\n",
      "Elapsed: [0:47:23.630245]  batch: 200  d_loss: -0.013194  g_loss: 0.123826\n",
      "Elapsed: [0:47:27.146692]  batch: 250  d_loss: -0.026416  g_loss: 0.090013\n",
      "Elapsed: [0:47:30.656545]  batch: 300  d_loss: 0.008034  g_loss: 0.034494\n",
      "Elapsed: [0:47:34.194176]  batch: 350  d_loss: -0.012595  g_loss: 0.056870\n",
      "Elapsed: [0:47:37.733616]  batch: 400  d_loss: 0.063425  g_loss: 0.045714\n",
      "Elapsed: [0:47:41.252045]  batch: 450  d_loss: 0.018324  g_loss: -0.004519\n",
      "Elapsed: [0:47:44.754066]  batch: 500  d_loss: 0.088413  g_loss: 0.071082\n",
      "Time taken for epoch: 44.437 secs\n",
      "ticker =  17501\n",
      "\n",
      "\n",
      "Currently working on Depth:  3\n",
      "Current resolution: 16 x 16\n",
      "Ticker 1\n",
      "\n",
      "Epoch: 1\n",
      "Elapsed: [0:47:53.941711]  batch: 1  d_loss: 2.706206  g_loss: 0.112496\n",
      "Elapsed: [0:47:58.043683]  batch: 50  d_loss: 0.005369  g_loss: 0.186907\n",
      "Elapsed: [0:48:02.166650]  batch: 100  d_loss: 0.032818  g_loss: 0.068315\n",
      "Elapsed: [0:48:06.284767]  batch: 150  d_loss: -0.097967  g_loss: 0.241183\n",
      "Elapsed: [0:48:10.400959]  batch: 200  d_loss: -0.180138  g_loss: 0.337706\n",
      "Elapsed: [0:48:14.530607]  batch: 250  d_loss: -0.148882  g_loss: 0.339849\n",
      "Elapsed: [0:48:18.596138]  batch: 300  d_loss: -0.001315  g_loss: 0.187428\n",
      "Elapsed: [0:48:22.821089]  batch: 350  d_loss: -0.027525  g_loss: 0.160785\n",
      "Elapsed: [0:48:27.036762]  batch: 400  d_loss: -0.096893  g_loss: 0.245103\n",
      "Elapsed: [0:48:31.203938]  batch: 450  d_loss: 0.034797  g_loss: 0.051730\n",
      "Elapsed: [0:48:35.401581]  batch: 500  d_loss: -0.211155  g_loss: 0.350865\n",
      "Time taken for epoch: 50.563 secs\n",
      "ticker =  501\n",
      "\n",
      "Epoch: 2\n",
      "Elapsed: [0:48:44.815363]  batch: 1  d_loss: 0.027658  g_loss: 0.069954\n",
      "Elapsed: [0:48:48.892976]  batch: 50  d_loss: -0.141384  g_loss: 0.284851\n",
      "Elapsed: [0:48:53.059900]  batch: 100  d_loss: -0.172955  g_loss: 0.352854\n",
      "Elapsed: [0:48:57.234920]  batch: 150  d_loss: -0.056641  g_loss: 0.286283\n",
      "Elapsed: [0:49:01.445034]  batch: 200  d_loss: -0.131378  g_loss: 0.252915\n",
      "Elapsed: [0:49:05.602671]  batch: 250  d_loss: -0.181390  g_loss: 0.284894\n",
      "Elapsed: [0:49:09.847382]  batch: 300  d_loss: -0.140226  g_loss: 0.299293\n",
      "Elapsed: [0:49:14.135043]  batch: 350  d_loss: -0.069618  g_loss: 0.161249\n",
      "Elapsed: [0:49:18.328629]  batch: 400  d_loss: -0.145874  g_loss: 0.166896\n",
      "Elapsed: [0:49:22.515157]  batch: 450  d_loss: -0.090347  g_loss: 0.189902\n",
      "Elapsed: [0:49:26.720462]  batch: 500  d_loss: -0.264237  g_loss: 0.373523\n",
      "Time taken for epoch: 51.222 secs\n",
      "ticker =  1001\n",
      "\n",
      "Epoch: 3\n",
      "Elapsed: [0:49:36.656683]  batch: 1  d_loss: -0.034680  g_loss: 0.055698\n",
      "Elapsed: [0:49:40.749966]  batch: 50  d_loss: -0.203789  g_loss: 0.311606\n",
      "Elapsed: [0:49:44.973429]  batch: 100  d_loss: -0.032130  g_loss: 0.093542\n",
      "Elapsed: [0:49:49.179462]  batch: 150  d_loss: 0.019433  g_loss: 0.045017\n",
      "Elapsed: [0:49:53.414922]  batch: 200  d_loss: -0.174438  g_loss: 0.259472\n",
      "Elapsed: [0:49:57.573666]  batch: 250  d_loss: -0.050513  g_loss: 0.155325\n",
      "Elapsed: [0:50:01.731189]  batch: 300  d_loss: -0.051968  g_loss: 0.120889\n",
      "Elapsed: [0:50:05.905419]  batch: 350  d_loss: -0.103316  g_loss: 0.218461\n",
      "Elapsed: [0:50:10.087278]  batch: 400  d_loss: -0.227991  g_loss: 0.336286\n",
      "Elapsed: [0:50:14.329822]  batch: 450  d_loss: -0.165652  g_loss: 0.346123\n",
      "Elapsed: [0:50:18.535549]  batch: 500  d_loss: 0.020491  g_loss: 0.040824\n",
      "Time taken for epoch: 51.715 secs\n",
      "ticker =  1501\n",
      "\n",
      "Epoch: 4\n",
      "Elapsed: [0:50:28.114244]  batch: 1  d_loss: -0.096403  g_loss: 0.174793\n",
      "Elapsed: [0:50:32.221940]  batch: 50  d_loss: -0.187650  g_loss: 0.402653\n",
      "Elapsed: [0:50:36.382816]  batch: 100  d_loss: -0.293628  g_loss: 0.483107\n",
      "Elapsed: [0:50:40.594752]  batch: 150  d_loss: -0.185890  g_loss: 0.334095\n",
      "Elapsed: [0:50:44.769998]  batch: 200  d_loss: 0.011936  g_loss: 0.070520\n",
      "Elapsed: [0:50:48.994946]  batch: 250  d_loss: 0.008387  g_loss: 0.060875\n",
      "Elapsed: [0:50:53.171182]  batch: 300  d_loss: -0.152611  g_loss: 0.272145\n",
      "Elapsed: [0:50:57.428513]  batch: 350  d_loss: -0.117056  g_loss: 0.299496\n",
      "Elapsed: [0:51:01.780960]  batch: 400  d_loss: -0.029548  g_loss: 0.164808\n",
      "Elapsed: [0:51:06.008403]  batch: 450  d_loss: -0.173056  g_loss: 0.357704\n",
      "Elapsed: [0:51:10.220897]  batch: 500  d_loss: -0.183380  g_loss: 0.349169\n",
      "Time taken for epoch: 51.624 secs\n",
      "ticker =  2001\n",
      "\n",
      "Epoch: 5\n",
      "Elapsed: [0:51:19.793779]  batch: 1  d_loss: -0.018349  g_loss: 0.190431\n",
      "Elapsed: [0:51:24.053240]  batch: 50  d_loss: -0.145519  g_loss: 0.305299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: [0:51:28.203882]  batch: 100  d_loss: -0.073385  g_loss: 0.173760\n",
      "Elapsed: [0:51:32.440567]  batch: 150  d_loss: 0.042886  g_loss: 0.088639\n",
      "Elapsed: [0:51:36.637445]  batch: 200  d_loss: -0.308890  g_loss: 0.380494\n",
      "Elapsed: [0:51:40.806044]  batch: 250  d_loss: -0.069040  g_loss: 0.221113\n",
      "Elapsed: [0:51:45.013816]  batch: 300  d_loss: -0.227829  g_loss: 0.402113\n",
      "Elapsed: [0:51:49.209640]  batch: 350  d_loss: -0.024455  g_loss: 0.040209\n",
      "Elapsed: [0:51:53.463864]  batch: 400  d_loss: 0.026730  g_loss: 0.015584\n",
      "Elapsed: [0:51:57.742804]  batch: 450  d_loss: 0.007045  g_loss: 0.029990\n",
      "Elapsed: [0:52:01.987527]  batch: 500  d_loss: -0.031192  g_loss: 0.206636\n",
      "Time taken for epoch: 51.643 secs\n",
      "ticker =  2501\n",
      "\n",
      "Epoch: 6\n",
      "Elapsed: [0:52:11.613823]  batch: 1  d_loss: -0.215566  g_loss: 0.387503\n",
      "Elapsed: [0:52:15.748176]  batch: 50  d_loss: -0.202290  g_loss: 0.418750\n",
      "Elapsed: [0:52:19.984614]  batch: 100  d_loss: -0.012825  g_loss: 0.182687\n",
      "Elapsed: [0:52:24.183165]  batch: 150  d_loss: -0.036784  g_loss: 0.037335\n",
      "Elapsed: [0:52:28.402670]  batch: 200  d_loss: -0.067494  g_loss: 0.183184\n",
      "Elapsed: [0:52:32.551638]  batch: 250  d_loss: -0.063556  g_loss: 0.256397\n",
      "Elapsed: [0:52:36.772553]  batch: 300  d_loss: -0.063286  g_loss: 0.191952\n",
      "Elapsed: [0:52:40.998239]  batch: 350  d_loss: -0.061137  g_loss: 0.217128\n",
      "Elapsed: [0:52:45.174897]  batch: 400  d_loss: -0.010275  g_loss: 0.106333\n",
      "Elapsed: [0:52:49.375919]  batch: 450  d_loss: -0.099371  g_loss: 0.240097\n",
      "Elapsed: [0:52:53.555378]  batch: 500  d_loss: -0.205688  g_loss: 0.411921\n",
      "Time taken for epoch: 51.461 secs\n",
      "ticker =  3001\n",
      "\n",
      "Epoch: 7\n",
      "Elapsed: [0:53:03.058677]  batch: 1  d_loss: 0.013432  g_loss: 0.180247\n",
      "Elapsed: [0:53:08.633878]  batch: 50  d_loss: -0.007653  g_loss: 0.146768\n",
      "Elapsed: [0:53:13.207400]  batch: 100  d_loss: -0.189968  g_loss: 0.345007\n",
      "Elapsed: [0:53:17.479983]  batch: 150  d_loss: -0.173892  g_loss: 0.308315\n",
      "Elapsed: [0:53:21.694639]  batch: 200  d_loss: -0.202362  g_loss: 0.306331\n",
      "Elapsed: [0:53:25.901853]  batch: 250  d_loss: -0.292546  g_loss: 0.483788\n",
      "Elapsed: [0:53:30.142751]  batch: 300  d_loss: -0.041812  g_loss: 0.225922\n",
      "Elapsed: [0:53:34.407322]  batch: 350  d_loss: -0.222915  g_loss: 0.334242\n",
      "Elapsed: [0:53:38.596301]  batch: 400  d_loss: 0.012765  g_loss: 0.176961\n",
      "Elapsed: [0:53:42.736080]  batch: 450  d_loss: -0.042847  g_loss: 0.149326\n",
      "Elapsed: [0:53:46.961714]  batch: 500  d_loss: -0.161773  g_loss: 0.346324\n",
      "Time taken for epoch: 53.322 secs\n",
      "ticker =  3501\n",
      "\n",
      "Epoch: 8\n",
      "Elapsed: [0:53:56.506974]  batch: 1  d_loss: -0.026894  g_loss: 0.186945\n",
      "Elapsed: [0:54:00.629683]  batch: 50  d_loss: -0.134610  g_loss: 0.335415\n",
      "Elapsed: [0:54:04.842257]  batch: 100  d_loss: -0.157915  g_loss: 0.278114\n",
      "Elapsed: [0:54:09.086463]  batch: 150  d_loss: -0.134851  g_loss: 0.333258\n",
      "Elapsed: [0:54:13.337601]  batch: 200  d_loss: -0.119311  g_loss: 0.577494\n",
      "Elapsed: [0:54:17.583414]  batch: 250  d_loss: -0.108379  g_loss: 0.275855\n",
      "Elapsed: [0:54:21.777576]  batch: 300  d_loss: -0.098060  g_loss: 0.193581\n",
      "Elapsed: [0:54:25.994822]  batch: 350  d_loss: -0.195279  g_loss: 0.372163\n",
      "Elapsed: [0:54:30.210901]  batch: 400  d_loss: -0.041021  g_loss: 0.169916\n",
      "Elapsed: [0:54:34.485321]  batch: 450  d_loss: -0.187353  g_loss: 0.269356\n",
      "Elapsed: [0:54:38.723138]  batch: 500  d_loss: -0.014183  g_loss: 0.016329\n",
      "Time taken for epoch: 51.645 secs\n",
      "ticker =  4001\n",
      "\n",
      "Epoch: 9\n",
      "Elapsed: [0:54:48.954065]  batch: 1  d_loss: -0.264423  g_loss: 0.401364\n",
      "Elapsed: [0:54:53.132869]  batch: 50  d_loss: -0.126855  g_loss: 0.264154\n",
      "Elapsed: [0:54:57.325135]  batch: 100  d_loss: -0.244488  g_loss: 0.291710\n",
      "Elapsed: [0:55:01.532123]  batch: 150  d_loss: -0.204079  g_loss: 0.347186\n",
      "Elapsed: [0:55:06.013981]  batch: 200  d_loss: 0.008282  g_loss: 0.151167\n",
      "Elapsed: [0:55:10.221174]  batch: 250  d_loss: -0.237076  g_loss: 0.515096\n",
      "Elapsed: [0:55:14.431662]  batch: 300  d_loss: 0.037963  g_loss: -0.027990\n",
      "Elapsed: [0:55:18.591722]  batch: 350  d_loss: -0.256927  g_loss: 0.365098\n",
      "Elapsed: [0:55:22.776098]  batch: 400  d_loss: -0.217242  g_loss: 0.375953\n",
      "Elapsed: [0:55:26.950653]  batch: 450  d_loss: -0.011764  g_loss: 0.195820\n",
      "Elapsed: [0:55:31.136931]  batch: 500  d_loss: 0.031635  g_loss: 0.065888\n",
      "Time taken for epoch: 52.338 secs\n",
      "ticker =  4501\n",
      "\n",
      "Epoch: 10\n",
      "Elapsed: [0:55:40.608387]  batch: 1  d_loss: -0.209052  g_loss: 0.225424\n",
      "Elapsed: [0:55:44.693561]  batch: 50  d_loss: -0.088731  g_loss: 0.224477\n",
      "Elapsed: [0:55:48.867991]  batch: 100  d_loss: 0.009474  g_loss: 0.090249\n",
      "Elapsed: [0:55:53.086520]  batch: 150  d_loss: -0.284337  g_loss: 0.292132\n",
      "Elapsed: [0:55:57.264986]  batch: 200  d_loss: -0.199300  g_loss: 0.321521\n",
      "Elapsed: [0:56:01.454158]  batch: 250  d_loss: -0.287289  g_loss: 0.374363\n",
      "Elapsed: [0:56:05.636019]  batch: 300  d_loss: -0.243609  g_loss: 0.361409\n",
      "Elapsed: [0:56:09.823033]  batch: 350  d_loss: -0.024843  g_loss: 0.131798\n",
      "Elapsed: [0:56:14.026100]  batch: 400  d_loss: -0.072048  g_loss: 0.212751\n",
      "Elapsed: [0:56:18.194275]  batch: 450  d_loss: -0.195865  g_loss: 0.317226\n",
      "Elapsed: [0:56:22.392109]  batch: 500  d_loss: -0.082920  g_loss: 0.248752\n",
      "Time taken for epoch: 51.191 secs\n",
      "ticker =  5001\n",
      "\n",
      "Epoch: 11\n",
      "Elapsed: [0:56:32.054907]  batch: 1  d_loss: -0.096019  g_loss: 0.256715\n",
      "Elapsed: [0:56:36.523041]  batch: 50  d_loss: 0.057575  g_loss: 0.091972\n",
      "Elapsed: [0:56:41.669981]  batch: 100  d_loss: -0.046734  g_loss: 0.102262\n",
      "Elapsed: [0:56:45.825181]  batch: 150  d_loss: -0.000384  g_loss: 0.078582\n",
      "Elapsed: [0:56:50.079801]  batch: 200  d_loss: 0.034112  g_loss: 0.063115\n",
      "Elapsed: [0:56:54.273496]  batch: 250  d_loss: -0.021175  g_loss: 0.151826\n",
      "Elapsed: [0:56:58.450977]  batch: 300  d_loss: -0.141890  g_loss: 0.277589\n",
      "Elapsed: [0:57:02.650922]  batch: 350  d_loss: -0.204508  g_loss: 0.382586\n",
      "Elapsed: [0:57:06.874364]  batch: 400  d_loss: -0.051530  g_loss: 0.049216\n",
      "Elapsed: [0:57:11.203483]  batch: 450  d_loss: -0.190236  g_loss: 0.234591\n",
      "Elapsed: [0:57:15.391327]  batch: 500  d_loss: 0.040801  g_loss: 0.032247\n",
      "Time taken for epoch: 52.851 secs\n",
      "ticker =  5501\n",
      "\n",
      "Epoch: 12\n",
      "Elapsed: [0:57:24.917754]  batch: 1  d_loss: -0.176236  g_loss: 0.339719\n",
      "Elapsed: [0:57:28.956244]  batch: 50  d_loss: -0.063384  g_loss: 0.164687\n",
      "Elapsed: [0:57:33.216994]  batch: 100  d_loss: -0.068328  g_loss: 0.233986\n",
      "Elapsed: [0:57:37.626367]  batch: 150  d_loss: -0.111691  g_loss: 0.244978\n",
      "Elapsed: [0:57:42.096600]  batch: 200  d_loss: -0.165684  g_loss: 0.277505\n",
      "Elapsed: [0:57:46.538835]  batch: 250  d_loss: 0.051270  g_loss: 0.000920\n",
      "Elapsed: [0:57:50.936835]  batch: 300  d_loss: -0.076369  g_loss: 0.207872\n",
      "Elapsed: [0:57:55.173526]  batch: 350  d_loss: -0.024016  g_loss: 0.167352\n",
      "Elapsed: [0:57:59.350725]  batch: 400  d_loss: -0.172232  g_loss: 0.280688\n",
      "Elapsed: [0:58:03.484492]  batch: 450  d_loss: -0.179053  g_loss: 0.336584\n",
      "Elapsed: [0:58:07.647269]  batch: 500  d_loss: -0.029471  g_loss: 0.159859\n",
      "Time taken for epoch: 52.168 secs\n",
      "ticker =  6001\n",
      "\n",
      "Epoch: 13\n",
      "Elapsed: [0:58:17.352270]  batch: 1  d_loss: -0.118829  g_loss: 0.195600\n",
      "Elapsed: [0:58:21.565871]  batch: 50  d_loss: -0.056061  g_loss: 0.187963\n",
      "Elapsed: [0:58:28.861358]  batch: 100  d_loss: -0.059858  g_loss: 0.176251\n",
      "Elapsed: [0:58:37.212340]  batch: 150  d_loss: -0.142555  g_loss: 0.306424\n",
      "Elapsed: [0:58:45.682458]  batch: 200  d_loss: -0.061109  g_loss: 0.190252\n",
      "Elapsed: [0:58:58.566145]  batch: 250  d_loss: -0.153868  g_loss: 0.288384\n",
      "Elapsed: [0:59:07.849709]  batch: 300  d_loss: -0.199397  g_loss: 0.289080\n",
      "Elapsed: [0:59:17.337570]  batch: 350  d_loss: -0.105260  g_loss: 0.245032\n",
      "Elapsed: [0:59:25.674638]  batch: 400  d_loss: -0.098805  g_loss: 0.238193\n",
      "Elapsed: [0:59:33.420768]  batch: 450  d_loss: -0.125592  g_loss: 0.264132\n",
      "Elapsed: [0:59:41.269763]  batch: 500  d_loss: -0.120728  g_loss: 0.260344\n",
      "Time taken for epoch: 93.746 secs\n",
      "ticker =  6501\n",
      "\n",
      "Epoch: 14\n",
      "Elapsed: [0:59:56.422412]  batch: 1  d_loss: 0.015170  g_loss: 0.131479\n",
      "Elapsed: [1:00:00.767387]  batch: 50  d_loss: -0.096942  g_loss: 0.243773\n",
      "Elapsed: [1:00:05.260849]  batch: 100  d_loss: -0.011171  g_loss: 0.190942\n",
      "Elapsed: [1:00:09.788550]  batch: 150  d_loss: -0.018161  g_loss: 0.124454\n",
      "Elapsed: [1:00:14.725618]  batch: 200  d_loss: -0.235138  g_loss: 0.381358\n",
      "Elapsed: [1:00:19.498615]  batch: 250  d_loss: -0.116079  g_loss: 0.262024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: [1:00:24.116942]  batch: 300  d_loss: -0.069547  g_loss: 0.085531\n",
      "Elapsed: [1:00:28.622416]  batch: 350  d_loss: -0.162815  g_loss: 0.257599\n",
      "Elapsed: [1:00:33.469037]  batch: 400  d_loss: -0.034720  g_loss: 0.071543\n",
      "Elapsed: [1:00:38.281176]  batch: 450  d_loss: -0.169874  g_loss: 0.312906\n",
      "Elapsed: [1:00:42.832575]  batch: 500  d_loss: -0.088851  g_loss: 0.275220\n",
      "Time taken for epoch: 61.141 secs\n",
      "ticker =  7001\n",
      "\n",
      "Epoch: 15\n",
      "Elapsed: [1:00:53.340030]  batch: 1  d_loss: 0.021481  g_loss: 0.031952\n",
      "Elapsed: [1:00:57.966805]  batch: 50  d_loss: -0.011406  g_loss: 0.084952\n",
      "Elapsed: [1:01:03.010162]  batch: 100  d_loss: -0.104981  g_loss: 0.259176\n",
      "Elapsed: [1:01:08.820568]  batch: 150  d_loss: -0.053735  g_loss: 0.137808\n",
      "Elapsed: [1:01:15.932208]  batch: 200  d_loss: 0.003273  g_loss: 0.131789\n",
      "Elapsed: [1:01:22.691470]  batch: 250  d_loss: -0.031865  g_loss: 0.134871\n",
      "Elapsed: [1:01:32.289071]  batch: 300  d_loss: -0.060720  g_loss: 0.349096\n",
      "Elapsed: [1:01:38.402992]  batch: 350  d_loss: -0.100884  g_loss: 0.268041\n",
      "Elapsed: [1:01:46.624164]  batch: 400  d_loss: -0.100736  g_loss: 0.213405\n",
      "Elapsed: [1:02:03.811437]  batch: 450  d_loss: -0.028397  g_loss: 0.154973\n",
      "Elapsed: [1:02:11.903398]  batch: 500  d_loss: -0.126103  g_loss: 0.293767\n",
      "Time taken for epoch: 89.729 secs\n",
      "ticker =  7501\n",
      "\n",
      "Epoch: 16\n",
      "Elapsed: [1:03:27.067979]  batch: 1  d_loss: -0.009770  g_loss: 0.043636\n",
      "Elapsed: [1:03:42.690331]  batch: 50  d_loss: -0.048488  g_loss: 0.183545\n",
      "Elapsed: [1:04:00.921540]  batch: 100  d_loss: -0.165464  g_loss: 0.285535\n",
      "Elapsed: [1:04:18.255385]  batch: 150  d_loss: 0.057620  g_loss: -0.023379\n",
      "Elapsed: [1:04:36.540248]  batch: 200  d_loss: -0.022538  g_loss: 0.134131\n",
      "Elapsed: [1:04:52.572247]  batch: 250  d_loss: -0.029470  g_loss: 0.178009\n",
      "Elapsed: [1:05:10.694277]  batch: 300  d_loss: 0.031413  g_loss: 0.133743\n",
      "Elapsed: [1:05:19.658491]  batch: 350  d_loss: -0.099010  g_loss: 0.234328\n",
      "Elapsed: [1:05:28.312440]  batch: 400  d_loss: -0.203289  g_loss: 0.281784\n",
      "Elapsed: [1:05:36.871772]  batch: 450  d_loss: 0.072183  g_loss: -0.010222\n",
      "Elapsed: [1:05:46.985665]  batch: 500  d_loss: -0.019950  g_loss: 0.108071\n",
      "Time taken for epoch: 214.416 secs\n",
      "ticker =  8001\n",
      "\n",
      "Epoch: 17\n",
      "Elapsed: [1:06:11.515667]  batch: 1  d_loss: -0.123358  g_loss: 0.285125\n",
      "Elapsed: [1:06:20.408695]  batch: 50  d_loss: 0.005225  g_loss: 0.109987\n",
      "Elapsed: [1:06:29.883582]  batch: 100  d_loss: -0.066360  g_loss: 0.203507\n",
      "Elapsed: [1:06:41.195845]  batch: 150  d_loss: -0.011791  g_loss: 0.077678\n",
      "Elapsed: [1:06:52.695953]  batch: 200  d_loss: -0.133008  g_loss: 0.190563\n",
      "Elapsed: [1:07:05.145752]  batch: 250  d_loss: -0.082894  g_loss: 0.222580\n",
      "Elapsed: [1:07:17.287638]  batch: 300  d_loss: 0.066180  g_loss: 0.083417\n",
      "Elapsed: [1:07:31.297961]  batch: 350  d_loss: 0.005425  g_loss: 0.114004\n",
      "Elapsed: [1:07:44.386397]  batch: 400  d_loss: -0.050860  g_loss: 0.089516\n",
      "Elapsed: [1:07:58.198340]  batch: 450  d_loss: 0.025737  g_loss: 0.004147\n",
      "Elapsed: [1:08:11.138865]  batch: 500  d_loss: -0.008352  g_loss: 0.118331\n",
      "Time taken for epoch: 144.154 secs\n",
      "ticker =  8501\n",
      "\n",
      "Epoch: 18\n",
      "Elapsed: [1:08:50.928527]  batch: 1  d_loss: -0.107207  g_loss: 0.214893\n",
      "Elapsed: [1:09:06.334443]  batch: 50  d_loss: -0.033895  g_loss: 0.082700\n",
      "Elapsed: [1:09:20.585949]  batch: 100  d_loss: 0.017009  g_loss: 0.020270\n",
      "Elapsed: [1:09:31.170648]  batch: 150  d_loss: -0.134345  g_loss: 0.193963\n",
      "Elapsed: [1:09:42.000671]  batch: 200  d_loss: 0.029964  g_loss: 0.125887\n",
      "Elapsed: [1:09:51.608736]  batch: 250  d_loss: 0.039266  g_loss: 0.083739\n",
      "Elapsed: [1:10:01.409464]  batch: 300  d_loss: -0.072700  g_loss: 0.089652\n",
      "Elapsed: [1:10:12.172402]  batch: 350  d_loss: -0.116649  g_loss: 0.232964\n",
      "Elapsed: [1:10:31.209017]  batch: 400  d_loss: -0.062626  g_loss: 0.209062\n",
      "Elapsed: [1:10:57.924066]  batch: 450  d_loss: -0.184519  g_loss: 0.287981\n",
      "Elapsed: [1:11:05.198210]  batch: 500  d_loss: -0.060602  g_loss: 0.192136\n",
      "Time taken for epoch: 173.350 secs\n",
      "ticker =  9001\n",
      "\n",
      "Epoch: 19\n",
      "Elapsed: [1:11:16.994983]  batch: 1  d_loss: 0.017106  g_loss: 0.042350\n",
      "Elapsed: [1:11:22.347342]  batch: 50  d_loss: 0.020552  g_loss: 0.005111\n",
      "Elapsed: [1:11:29.156053]  batch: 100  d_loss: 0.038303  g_loss: 0.006392\n",
      "Elapsed: [1:11:35.063817]  batch: 150  d_loss: -0.005268  g_loss: 0.092391\n",
      "Elapsed: [1:11:40.803538]  batch: 200  d_loss: -0.107817  g_loss: 0.256971\n",
      "Elapsed: [1:11:48.726941]  batch: 250  d_loss: -0.026220  g_loss: 0.162993\n",
      "Elapsed: [1:12:01.732324]  batch: 300  d_loss: -0.011185  g_loss: 0.129814\n",
      "Elapsed: [1:12:12.803497]  batch: 350  d_loss: 0.029238  g_loss: 0.012546\n",
      "Elapsed: [1:12:26.000659]  batch: 400  d_loss: -0.086955  g_loss: 0.218814\n",
      "Elapsed: [1:12:36.545463]  batch: 450  d_loss: -0.060177  g_loss: 0.201268\n",
      "Elapsed: [1:12:46.849516]  batch: 500  d_loss: -0.000369  g_loss: 0.137510\n",
      "Time taken for epoch: 101.539 secs\n",
      "ticker =  9501\n",
      "\n",
      "Epoch: 20\n",
      "Elapsed: [1:13:05.115197]  batch: 1  d_loss: -0.079719  g_loss: 0.208098\n",
      "Elapsed: [1:13:14.039405]  batch: 50  d_loss: 0.151465  g_loss: 0.055841\n",
      "Elapsed: [1:13:18.867953]  batch: 100  d_loss: -0.022784  g_loss: 0.125281\n",
      "Elapsed: [1:13:24.370151]  batch: 150  d_loss: 0.016243  g_loss: 0.112135\n",
      "Elapsed: [1:13:32.914623]  batch: 200  d_loss: -0.086238  g_loss: 0.218977\n",
      "Elapsed: [1:13:40.134125]  batch: 250  d_loss: 0.062527  g_loss: 0.068101\n",
      "Elapsed: [1:13:46.751563]  batch: 300  d_loss: -0.162589  g_loss: 0.279631\n",
      "Elapsed: [1:13:52.750975]  batch: 350  d_loss: 0.017955  g_loss: 0.096437\n",
      "Elapsed: [1:13:58.694719]  batch: 400  d_loss: 0.020684  g_loss: 0.091846\n",
      "Elapsed: [1:14:03.987404]  batch: 450  d_loss: -0.018931  g_loss: 0.029253\n",
      "Elapsed: [1:14:09.252082]  batch: 500  d_loss: -0.137833  g_loss: 0.281137\n",
      "Time taken for epoch: 82.338 secs\n",
      "ticker =  10001\n",
      "\n",
      "Epoch: 21\n",
      "Elapsed: [1:14:25.761014]  batch: 1  d_loss: -0.026281  g_loss: 0.060764\n",
      "Elapsed: [1:14:32.741580]  batch: 50  d_loss: 0.001830  g_loss: 0.094679\n",
      "Elapsed: [1:14:41.193340]  batch: 100  d_loss: -0.075760  g_loss: 0.204087\n",
      "Elapsed: [1:14:48.300581]  batch: 150  d_loss: -0.011737  g_loss: 0.039221\n",
      "Elapsed: [1:14:53.927076]  batch: 200  d_loss: -0.049937  g_loss: 0.202615\n",
      "Elapsed: [1:15:01.254404]  batch: 250  d_loss: -0.025619  g_loss: 0.079309\n",
      "Elapsed: [1:15:08.483746]  batch: 300  d_loss: -0.011076  g_loss: 0.067350\n",
      "Elapsed: [1:15:15.127141]  batch: 350  d_loss: 0.013031  g_loss: 0.025900\n",
      "Elapsed: [1:15:22.547364]  batch: 400  d_loss: -0.018104  g_loss: 0.143582\n",
      "Elapsed: [1:15:30.713919]  batch: 450  d_loss: 0.228535  g_loss: 0.252188\n",
      "Elapsed: [1:15:38.358374]  batch: 500  d_loss: 0.029772  g_loss: 0.055600\n",
      "Time taken for epoch: 89.138 secs\n",
      "ticker =  10501\n",
      "\n",
      "Epoch: 22\n",
      "Elapsed: [1:15:58.717366]  batch: 1  d_loss: -0.053269  g_loss: 0.164732\n",
      "Elapsed: [1:16:06.371000]  batch: 50  d_loss: -0.005706  g_loss: 0.001223\n",
      "Elapsed: [1:16:15.461777]  batch: 100  d_loss: -0.145323  g_loss: 0.156534\n",
      "Elapsed: [1:16:21.605149]  batch: 150  d_loss: -0.121342  g_loss: 0.200220\n",
      "Elapsed: [1:16:27.005642]  batch: 200  d_loss: 0.008039  g_loss: 0.083465\n",
      "Elapsed: [1:16:34.381130]  batch: 250  d_loss: -0.029188  g_loss: 0.207962\n",
      "Elapsed: [1:16:41.790599]  batch: 300  d_loss: 0.002135  g_loss: 0.004430\n",
      "Elapsed: [1:16:49.422484]  batch: 350  d_loss: -0.038488  g_loss: 0.214713\n",
      "Elapsed: [1:16:56.190788]  batch: 400  d_loss: 0.004014  g_loss: 0.206438\n",
      "Elapsed: [1:17:05.900604]  batch: 450  d_loss: -0.096402  g_loss: 0.155454\n",
      "Elapsed: [1:17:11.559407]  batch: 500  d_loss: -0.061330  g_loss: 0.079925\n",
      "Time taken for epoch: 92.791 secs\n",
      "ticker =  11001\n",
      "\n",
      "Epoch: 23\n",
      "Elapsed: [1:17:24.973671]  batch: 1  d_loss: -0.069634  g_loss: 0.231513\n",
      "Elapsed: [1:17:31.599044]  batch: 50  d_loss: 0.032111  g_loss: 0.082067\n",
      "Elapsed: [1:17:36.727708]  batch: 100  d_loss: 0.122031  g_loss: 0.287984\n",
      "Elapsed: [1:17:42.170404]  batch: 150  d_loss: 0.013003  g_loss: -0.003472\n",
      "Elapsed: [1:17:47.649338]  batch: 200  d_loss: 0.035161  g_loss: -0.018278\n",
      "Elapsed: [1:17:53.212323]  batch: 250  d_loss: -0.073629  g_loss: 0.214893\n",
      "Elapsed: [1:17:58.638877]  batch: 300  d_loss: 0.044977  g_loss: 0.025903\n",
      "Elapsed: [1:18:03.889691]  batch: 350  d_loss: -0.038212  g_loss: 0.049027\n",
      "Elapsed: [1:18:09.085897]  batch: 400  d_loss: 0.080843  g_loss: 0.194503\n",
      "Elapsed: [1:18:14.322959]  batch: 450  d_loss: 0.003840  g_loss: 0.007148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: [1:18:19.471662]  batch: 500  d_loss: 0.038397  g_loss: 0.110918\n",
      "Time taken for epoch: 67.792 secs\n",
      "ticker =  11501\n",
      "\n",
      "Epoch: 24\n",
      "Elapsed: [1:18:31.412114]  batch: 1  d_loss: -0.057761  g_loss: 0.221189\n",
      "Elapsed: [1:18:36.615762]  batch: 50  d_loss: -0.008422  g_loss: 0.211684\n",
      "Elapsed: [1:18:41.719431]  batch: 100  d_loss: 0.049187  g_loss: 0.144626\n",
      "Elapsed: [1:18:46.862606]  batch: 150  d_loss: 0.079324  g_loss: 0.162565\n",
      "Elapsed: [1:18:51.917799]  batch: 200  d_loss: 0.011217  g_loss: 0.026578\n",
      "Elapsed: [1:18:57.397300]  batch: 250  d_loss: 0.019440  g_loss: 0.017113\n",
      "Elapsed: [1:19:02.525536]  batch: 300  d_loss: 0.004220  g_loss: -0.030140\n",
      "Elapsed: [1:19:07.686290]  batch: 350  d_loss: 0.262390  g_loss: 0.208720\n",
      "Elapsed: [1:19:12.853769]  batch: 400  d_loss: -0.028820  g_loss: 0.269008\n",
      "Elapsed: [1:19:18.069643]  batch: 450  d_loss: 0.070013  g_loss: 0.093926\n",
      "Elapsed: [1:19:23.464401]  batch: 500  d_loss: -0.032573  g_loss: 0.221117\n",
      "Time taken for epoch: 63.873 secs\n",
      "ticker =  12001\n",
      "\n",
      "Epoch: 25\n",
      "Elapsed: [1:19:35.713389]  batch: 1  d_loss: 0.017999  g_loss: 0.031666\n",
      "Elapsed: [1:19:40.807447]  batch: 50  d_loss: 0.131091  g_loss: 0.030152\n",
      "Elapsed: [1:19:46.078857]  batch: 100  d_loss: 0.289717  g_loss: 0.121111\n",
      "Elapsed: [1:19:51.270460]  batch: 150  d_loss: 0.131982  g_loss: -0.002359\n",
      "Elapsed: [1:19:56.497224]  batch: 200  d_loss: 0.034967  g_loss: -0.010993\n",
      "Elapsed: [1:20:01.965805]  batch: 250  d_loss: 0.045320  g_loss: 0.020204\n",
      "Elapsed: [1:20:07.157997]  batch: 300  d_loss: 0.061988  g_loss: 0.179093\n",
      "Elapsed: [1:20:12.419731]  batch: 350  d_loss: 0.083517  g_loss: 0.161814\n",
      "Elapsed: [1:20:18.542336]  batch: 400  d_loss: 0.069117  g_loss: 0.063970\n",
      "Elapsed: [1:20:25.866465]  batch: 450  d_loss: 0.052684  g_loss: -0.125520\n",
      "Elapsed: [1:20:31.378733]  batch: 500  d_loss: -0.003519  g_loss: 0.144388\n",
      "Time taken for epoch: 67.790 secs\n",
      "ticker =  12501\n",
      "\n",
      "Epoch: 26\n",
      "Elapsed: [1:20:50.605278]  batch: 1  d_loss: 0.159315  g_loss: 0.005221\n",
      "Elapsed: [1:20:57.517159]  batch: 50  d_loss: 0.081243  g_loss: 0.171363\n",
      "Elapsed: [1:21:05.357928]  batch: 100  d_loss: 0.013023  g_loss: 0.141283\n",
      "Elapsed: [1:21:12.352671]  batch: 150  d_loss: 0.071500  g_loss: -0.019517\n",
      "Elapsed: [1:21:19.240802]  batch: 200  d_loss: 0.244169  g_loss: -0.049096\n",
      "Elapsed: [1:21:25.852044]  batch: 250  d_loss: 0.068983  g_loss: 0.191777\n",
      "Elapsed: [1:21:38.254237]  batch: 300  d_loss: 0.083979  g_loss: 0.070949\n",
      "Elapsed: [1:21:43.145498]  batch: 350  d_loss: 0.025947  g_loss: -0.001979\n",
      "Elapsed: [1:21:48.005247]  batch: 400  d_loss: 0.037597  g_loss: 0.139505\n",
      "Elapsed: [1:21:52.911475]  batch: 450  d_loss: 0.044182  g_loss: 0.100286\n",
      "Elapsed: [1:21:57.790480]  batch: 500  d_loss: 0.069788  g_loss: 0.020679\n",
      "Time taken for epoch: 86.275 secs\n",
      "ticker =  13001\n",
      "\n",
      "Epoch: 27\n",
      "Elapsed: [1:22:08.875759]  batch: 1  d_loss: 0.014823  g_loss: -0.013695\n",
      "Elapsed: [1:22:13.769040]  batch: 50  d_loss: 0.058563  g_loss: 0.008391\n",
      "Elapsed: [1:22:18.567110]  batch: 100  d_loss: 0.016963  g_loss: 0.018314\n",
      "Elapsed: [1:22:23.498114]  batch: 150  d_loss: 0.023327  g_loss: 0.012449\n",
      "Elapsed: [1:22:35.352498]  batch: 200  d_loss: 0.036822  g_loss: -0.037020\n",
      "Elapsed: [1:22:45.825919]  batch: 250  d_loss: 0.055010  g_loss: 0.097174\n",
      "Elapsed: [1:22:57.439251]  batch: 300  d_loss: 0.018999  g_loss: 0.018911\n",
      "Elapsed: [1:23:15.522038]  batch: 350  d_loss: -0.013071  g_loss: 0.088564\n",
      "Elapsed: [1:23:35.841276]  batch: 400  d_loss: 0.017917  g_loss: 0.012466\n",
      "Elapsed: [1:23:55.747432]  batch: 450  d_loss: 0.118815  g_loss: 0.031265\n",
      "Elapsed: [1:24:15.439142]  batch: 500  d_loss: 0.085859  g_loss: -0.036246\n",
      "Time taken for epoch: 137.956 secs\n",
      "ticker =  13501\n",
      "\n",
      "Epoch: 28\n",
      "Elapsed: [1:25:00.237421]  batch: 1  d_loss: 0.032769  g_loss: 0.029859\n",
      "Elapsed: [1:25:21.975835]  batch: 50  d_loss: -0.013645  g_loss: 0.071127\n",
      "Elapsed: [1:25:42.689016]  batch: 100  d_loss: 0.053521  g_loss: 0.016913\n",
      "Elapsed: [1:26:04.599153]  batch: 150  d_loss: 0.069871  g_loss: 0.093648\n",
      "Elapsed: [1:26:26.799512]  batch: 200  d_loss: 0.033860  g_loss: 0.026950\n",
      "Elapsed: [1:26:51.235363]  batch: 250  d_loss: 0.051407  g_loss: 0.032644\n",
      "Elapsed: [1:27:15.463146]  batch: 300  d_loss: -0.015786  g_loss: 0.158027\n",
      "Elapsed: [1:27:35.221380]  batch: 350  d_loss: 0.103076  g_loss: 0.055052\n",
      "Elapsed: [1:27:58.062032]  batch: 400  d_loss: 0.045902  g_loss: 0.012603\n",
      "Elapsed: [1:28:18.775034]  batch: 450  d_loss: -0.017208  g_loss: 0.080493\n",
      "Elapsed: [1:28:44.011455]  batch: 500  d_loss: 0.108815  g_loss: 0.076514\n",
      "Time taken for epoch: 268.539 secs\n",
      "ticker =  14001\n",
      "\n",
      "Epoch: 29\n",
      "Elapsed: [1:29:25.895941]  batch: 1  d_loss: -0.000162  g_loss: 0.103460\n",
      "Elapsed: [1:29:48.842525]  batch: 50  d_loss: -0.024286  g_loss: -0.005738\n",
      "Elapsed: [1:30:11.527277]  batch: 100  d_loss: -0.000682  g_loss: 0.123295\n",
      "Elapsed: [1:30:32.589790]  batch: 150  d_loss: 0.037037  g_loss: -0.013597\n",
      "Elapsed: [1:30:52.716224]  batch: 200  d_loss: 0.031935  g_loss: 0.005652\n",
      "Elapsed: [1:31:18.822364]  batch: 250  d_loss: 0.038205  g_loss: -0.018534\n",
      "Elapsed: [1:31:41.324513]  batch: 300  d_loss: 0.009002  g_loss: -0.031828\n",
      "Elapsed: [1:32:05.630988]  batch: 350  d_loss: 0.008662  g_loss: 0.040623\n",
      "Elapsed: [1:32:25.145962]  batch: 400  d_loss: 0.007655  g_loss: 0.083211\n",
      "Elapsed: [1:32:43.209113]  batch: 450  d_loss: 0.017679  g_loss: 0.009396\n",
      "Elapsed: [1:33:05.191265]  batch: 500  d_loss: 0.023685  g_loss: 0.085630\n",
      "Time taken for epoch: 261.064 secs\n",
      "ticker =  14501\n",
      "\n",
      "Epoch: 30\n",
      "Elapsed: [1:33:52.270818]  batch: 1  d_loss: -0.018834  g_loss: 0.060509\n",
      "Elapsed: [1:34:16.213572]  batch: 50  d_loss: 0.008031  g_loss: 0.060547\n",
      "Elapsed: [1:34:34.446259]  batch: 100  d_loss: 0.018376  g_loss: -0.061427\n",
      "Elapsed: [1:34:56.106899]  batch: 150  d_loss: 0.018516  g_loss: 0.090966\n",
      "Elapsed: [1:35:24.901615]  batch: 200  d_loss: 0.007897  g_loss: 0.085524\n",
      "Elapsed: [1:35:50.915914]  batch: 250  d_loss: 0.026492  g_loss: -0.024753\n",
      "Elapsed: [1:36:13.118898]  batch: 300  d_loss: 0.073195  g_loss: 0.022553\n",
      "Elapsed: [1:36:35.697624]  batch: 350  d_loss: 0.023320  g_loss: 0.043072\n",
      "Elapsed: [1:37:02.314251]  batch: 400  d_loss: 0.060391  g_loss: 0.145773\n",
      "Elapsed: [1:37:21.411417]  batch: 450  d_loss: 0.027841  g_loss: -0.035099\n",
      "Elapsed: [1:37:45.637780]  batch: 500  d_loss: -0.011017  g_loss: 0.104829\n",
      "Time taken for epoch: 279.943 secs\n",
      "ticker =  15001\n",
      "\n",
      "Epoch: 31\n",
      "Elapsed: [1:38:32.066782]  batch: 1  d_loss: 0.014407  g_loss: 0.032853\n",
      "Elapsed: [1:38:54.854249]  batch: 50  d_loss: 0.184171  g_loss: -0.067912\n",
      "Elapsed: [1:39:19.666826]  batch: 100  d_loss: -0.022984  g_loss: 0.094783\n",
      "Elapsed: [1:39:41.851126]  batch: 150  d_loss: -0.007433  g_loss: 0.124517\n",
      "Elapsed: [1:40:04.859525]  batch: 200  d_loss: 0.059005  g_loss: 0.085798\n",
      "Elapsed: [1:40:28.627307]  batch: 250  d_loss: 0.021066  g_loss: -0.003010\n",
      "Elapsed: [1:40:58.762842]  batch: 300  d_loss: -0.025540  g_loss: 0.097569\n",
      "Elapsed: [1:41:22.598600]  batch: 350  d_loss: -0.007945  g_loss: 0.083698\n",
      "Elapsed: [1:41:42.992711]  batch: 400  d_loss: 0.042038  g_loss: -0.017982\n",
      "Elapsed: [1:42:07.312619]  batch: 450  d_loss: 0.012445  g_loss: 0.010946\n",
      "Elapsed: [1:42:29.961463]  batch: 500  d_loss: 0.000284  g_loss: -0.026953\n",
      "Time taken for epoch: 284.013 secs\n",
      "ticker =  15501\n",
      "\n",
      "Epoch: 32\n",
      "Elapsed: [1:43:13.453441]  batch: 1  d_loss: -0.012589  g_loss: -0.001209\n",
      "Elapsed: [1:43:36.547062]  batch: 50  d_loss: -0.011415  g_loss: 0.040635\n",
      "Elapsed: [1:44:00.175121]  batch: 100  d_loss: 0.005344  g_loss: 0.028019\n",
      "Elapsed: [1:44:30.490127]  batch: 150  d_loss: 0.048292  g_loss: 0.017525\n",
      "Elapsed: [1:44:55.110036]  batch: 200  d_loss: 0.024328  g_loss: -0.014966\n",
      "Elapsed: [1:45:17.736851]  batch: 250  d_loss: 0.074965  g_loss: 0.023706\n",
      "Elapsed: [1:45:40.473712]  batch: 300  d_loss: 0.017365  g_loss: 0.040189\n",
      "Elapsed: [1:45:59.843704]  batch: 350  d_loss: 0.034840  g_loss: 0.042972\n",
      "Elapsed: [1:46:23.194817]  batch: 400  d_loss: 0.001825  g_loss: 0.050792\n",
      "Elapsed: [1:46:43.847577]  batch: 450  d_loss: -0.003037  g_loss: 0.077518\n",
      "Elapsed: [1:47:08.995547]  batch: 500  d_loss: 0.003869  g_loss: 0.038814\n",
      "Time taken for epoch: 278.574 secs\n",
      "ticker =  16001\n",
      "\n",
      "Epoch: 33\n",
      "Elapsed: [1:47:50.599971]  batch: 1  d_loss: 0.003097  g_loss: 0.039293\n",
      "Elapsed: [1:48:15.458618]  batch: 50  d_loss: 0.045304  g_loss: 0.004569\n",
      "Elapsed: [1:48:37.009984]  batch: 100  d_loss: -0.012580  g_loss: 0.056172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: [1:49:00.507076]  batch: 150  d_loss: 0.028241  g_loss: -0.041658\n",
      "Elapsed: [1:49:19.742977]  batch: 200  d_loss: 0.023713  g_loss: -0.001476\n",
      "Elapsed: [1:49:39.016448]  batch: 250  d_loss: 0.015410  g_loss: 0.120770\n",
      "Elapsed: [1:49:59.029673]  batch: 300  d_loss: -0.006494  g_loss: 0.068353\n",
      "Elapsed: [1:50:23.171457]  batch: 350  d_loss: 0.024742  g_loss: 0.048528\n",
      "Elapsed: [1:50:47.072267]  batch: 400  d_loss: 0.008123  g_loss: 0.005208\n",
      "Elapsed: [1:51:12.200345]  batch: 450  d_loss: 0.053171  g_loss: 0.055395\n",
      "Elapsed: [1:51:38.924115]  batch: 500  d_loss: -0.020369  g_loss: -0.044877\n",
      "Time taken for epoch: 269.665 secs\n",
      "ticker =  16501\n",
      "\n",
      "Epoch: 34\n",
      "Elapsed: [1:52:22.312848]  batch: 1  d_loss: -0.012753  g_loss: -0.033019\n",
      "Elapsed: [1:52:45.034466]  batch: 50  d_loss: 0.000192  g_loss: 0.028192\n",
      "Elapsed: [1:53:09.345086]  batch: 100  d_loss: 0.048279  g_loss: -0.036548\n",
      "Elapsed: [1:53:32.050142]  batch: 150  d_loss: 0.056181  g_loss: 0.046081\n",
      "Elapsed: [1:53:56.145102]  batch: 200  d_loss: 0.024806  g_loss: 0.045662\n",
      "Elapsed: [1:54:20.117346]  batch: 250  d_loss: -0.026761  g_loss: -0.010049\n",
      "Elapsed: [1:54:44.032335]  batch: 300  d_loss: 0.011459  g_loss: 0.060598\n",
      "Elapsed: [1:55:08.090773]  batch: 350  d_loss: 0.028466  g_loss: 0.014316\n",
      "Elapsed: [1:55:32.133166]  batch: 400  d_loss: -0.000854  g_loss: 0.010456\n",
      "Elapsed: [1:55:58.289326]  batch: 450  d_loss: 0.056831  g_loss: -0.011434\n",
      "Elapsed: [1:56:25.264137]  batch: 500  d_loss: 0.069684  g_loss: -0.042759\n",
      "Time taken for epoch: 285.869 secs\n",
      "ticker =  17001\n",
      "\n",
      "Epoch: 35\n",
      "Elapsed: [1:57:10.592444]  batch: 1  d_loss: 0.108818  g_loss: -0.075975\n",
      "Elapsed: [1:57:34.351746]  batch: 50  d_loss: 0.039565  g_loss: 0.047221\n",
      "Elapsed: [1:58:00.959017]  batch: 100  d_loss: 0.021571  g_loss: 0.056882\n",
      "Elapsed: [1:58:27.877892]  batch: 150  d_loss: -0.027174  g_loss: 0.064588\n",
      "Elapsed: [1:58:56.667431]  batch: 200  d_loss: -0.029367  g_loss: 0.035513\n",
      "Elapsed: [1:59:15.961709]  batch: 250  d_loss: 0.034586  g_loss: 0.036213\n",
      "Elapsed: [1:59:28.298427]  batch: 300  d_loss: -0.013950  g_loss: 0.042015\n",
      "Elapsed: [1:59:46.752947]  batch: 350  d_loss: 0.043317  g_loss: 0.016905\n",
      "Elapsed: [1:59:55.113495]  batch: 400  d_loss: 0.018273  g_loss: 0.000226\n",
      "Elapsed: [2:00:07.707712]  batch: 450  d_loss: 0.049721  g_loss: -0.008735\n",
      "Elapsed: [2:00:18.997150]  batch: 500  d_loss: -0.051846  g_loss: 0.226220\n",
      "Time taken for epoch: 232.950 secs\n",
      "ticker =  17501\n",
      "\n",
      "Epoch: 36\n",
      "Elapsed: [2:00:33.126748]  batch: 1  d_loss: -0.024170  g_loss: 0.154240\n",
      "Elapsed: [2:00:37.701772]  batch: 50  d_loss: 0.120201  g_loss: -0.005403\n",
      "Elapsed: [2:00:42.166534]  batch: 100  d_loss: 0.002983  g_loss: 0.058176\n",
      "Elapsed: [2:00:46.807553]  batch: 150  d_loss: 0.023523  g_loss: -0.012928\n",
      "Elapsed: [2:00:51.292765]  batch: 200  d_loss: 0.034342  g_loss: 0.016341\n",
      "Elapsed: [2:00:55.771141]  batch: 250  d_loss: 0.074022  g_loss: 0.008148\n",
      "Elapsed: [2:01:00.303777]  batch: 300  d_loss: 0.030411  g_loss: 0.059533\n",
      "Elapsed: [2:01:04.932150]  batch: 350  d_loss: 0.003196  g_loss: 0.036966\n",
      "Elapsed: [2:01:09.419441]  batch: 400  d_loss: 0.022111  g_loss: 0.039055\n",
      "Elapsed: [2:01:13.923862]  batch: 450  d_loss: 0.030386  g_loss: 0.098503\n",
      "Elapsed: [2:01:18.486015]  batch: 500  d_loss: -0.018080  g_loss: 0.078207\n",
      "Time taken for epoch: 59.021 secs\n",
      "ticker =  18001\n",
      "\n",
      "Epoch: 37\n",
      "Elapsed: [2:01:28.051052]  batch: 1  d_loss: 0.040775  g_loss: 0.076442\n",
      "Elapsed: [2:01:32.499702]  batch: 50  d_loss: 0.003095  g_loss: 0.071879\n",
      "Elapsed: [2:01:36.965912]  batch: 100  d_loss: 0.007969  g_loss: 0.059093\n",
      "Elapsed: [2:01:41.471963]  batch: 150  d_loss: 0.056223  g_loss: 0.034084\n",
      "Elapsed: [2:01:46.011830]  batch: 200  d_loss: 0.010314  g_loss: -0.003383\n",
      "Elapsed: [2:01:50.467456]  batch: 250  d_loss: -0.000631  g_loss: 0.076164\n",
      "Elapsed: [2:01:54.939410]  batch: 300  d_loss: 0.011512  g_loss: -0.003881\n",
      "Elapsed: [2:01:59.446930]  batch: 350  d_loss: -0.002296  g_loss: 0.029812\n",
      "Elapsed: [2:02:03.919642]  batch: 400  d_loss: -0.002795  g_loss: -0.001685\n",
      "Elapsed: [2:02:08.419665]  batch: 450  d_loss: -0.015763  g_loss: 0.011553\n",
      "Elapsed: [2:02:13.008533]  batch: 500  d_loss: 0.010792  g_loss: -0.086073\n",
      "Time taken for epoch: 54.418 secs\n",
      "ticker =  18501\n",
      "\n",
      "Epoch: 38\n",
      "Elapsed: [2:02:22.643019]  batch: 1  d_loss: 0.023717  g_loss: -0.081400\n",
      "Elapsed: [2:02:27.139063]  batch: 50  d_loss: -0.021637  g_loss: 0.076198\n",
      "Elapsed: [2:02:31.640830]  batch: 100  d_loss: 0.058972  g_loss: 0.040351\n",
      "Elapsed: [2:02:36.178130]  batch: 150  d_loss: 0.005099  g_loss: 0.015159\n",
      "Elapsed: [2:02:40.706342]  batch: 200  d_loss: 0.041411  g_loss: 0.049705\n",
      "Elapsed: [2:02:45.190112]  batch: 250  d_loss: 0.046189  g_loss: 0.006344\n",
      "Elapsed: [2:02:49.692721]  batch: 300  d_loss: -0.007989  g_loss: 0.131458\n",
      "Elapsed: [2:02:54.173701]  batch: 350  d_loss: 0.017392  g_loss: 0.017329\n",
      "Elapsed: [2:02:58.711892]  batch: 400  d_loss: 0.029895  g_loss: 0.070640\n",
      "Elapsed: [2:03:03.218876]  batch: 450  d_loss: 0.008546  g_loss: -0.006505\n",
      "Elapsed: [2:03:07.746060]  batch: 500  d_loss: 0.033572  g_loss: -0.013411\n",
      "Time taken for epoch: 54.632 secs\n",
      "ticker =  19001\n",
      "\n",
      "Epoch: 39\n",
      "Elapsed: [2:03:17.225235]  batch: 1  d_loss: -0.009620  g_loss: 0.034097\n",
      "Elapsed: [2:03:21.840359]  batch: 50  d_loss: 0.050463  g_loss: 0.067792\n",
      "Elapsed: [2:03:26.444775]  batch: 100  d_loss: 0.032974  g_loss: -0.005556\n",
      "Elapsed: [2:03:30.984715]  batch: 150  d_loss: 0.041100  g_loss: 0.005378\n",
      "Elapsed: [2:03:35.564482]  batch: 200  d_loss: 0.039393  g_loss: -0.038763\n",
      "Elapsed: [2:03:40.191151]  batch: 250  d_loss: -0.001267  g_loss: 0.089108\n",
      "Elapsed: [2:03:45.675797]  batch: 300  d_loss: 0.023902  g_loss: 0.142753\n",
      "Elapsed: [2:03:50.675908]  batch: 350  d_loss: 0.035956  g_loss: -0.022069\n",
      "Elapsed: [2:03:55.213793]  batch: 400  d_loss: -0.043897  g_loss: -0.004249\n",
      "Elapsed: [2:03:59.892201]  batch: 450  d_loss: 0.014701  g_loss: 0.110198\n",
      "Elapsed: [2:04:04.613293]  batch: 500  d_loss: 0.036962  g_loss: -0.034330\n",
      "Time taken for epoch: 56.794 secs\n",
      "ticker =  19501\n",
      "\n",
      "Epoch: 40\n",
      "Elapsed: [2:04:15.388173]  batch: 1  d_loss: 0.019448  g_loss: 0.001775\n",
      "Elapsed: [2:04:20.053918]  batch: 50  d_loss: -0.000058  g_loss: 0.027132\n",
      "Elapsed: [2:04:24.645756]  batch: 100  d_loss: 0.001420  g_loss: 0.021355\n",
      "Elapsed: [2:04:29.265196]  batch: 150  d_loss: -0.001530  g_loss: -0.002043\n",
      "Elapsed: [2:04:33.877831]  batch: 200  d_loss: 0.022768  g_loss: 0.045380\n",
      "Elapsed: [2:04:38.412254]  batch: 250  d_loss: -0.008720  g_loss: 0.000997\n",
      "Elapsed: [2:04:43.161169]  batch: 300  d_loss: -0.011057  g_loss: 0.072140\n",
      "Elapsed: [2:04:47.714728]  batch: 350  d_loss: 0.025521  g_loss: 0.029066\n",
      "Elapsed: [2:04:52.296991]  batch: 400  d_loss: 0.053174  g_loss: 0.036156\n",
      "Elapsed: [2:04:56.905332]  batch: 450  d_loss: 0.022510  g_loss: -0.001460\n",
      "Elapsed: [2:05:01.554694]  batch: 500  d_loss: 0.005488  g_loss: 0.062412\n",
      "Time taken for epoch: 56.817 secs\n",
      "ticker =  20001\n",
      "\n",
      "Epoch: 41\n",
      "Elapsed: [2:05:11.162619]  batch: 1  d_loss: 0.010167  g_loss: 0.039258\n",
      "Elapsed: [2:05:15.681074]  batch: 50  d_loss: 0.027738  g_loss: -0.019635\n",
      "Elapsed: [2:05:20.211854]  batch: 100  d_loss: 0.050785  g_loss: -0.020113\n",
      "Elapsed: [2:05:24.725525]  batch: 150  d_loss: 0.058694  g_loss: 0.046948\n",
      "Elapsed: [2:05:29.200232]  batch: 200  d_loss: 0.013354  g_loss: 0.019964\n",
      "Elapsed: [2:05:33.614896]  batch: 250  d_loss: 0.014614  g_loss: -0.048026\n",
      "Elapsed: [2:05:38.180349]  batch: 300  d_loss: 0.026027  g_loss: 0.027302\n",
      "Elapsed: [2:05:42.628547]  batch: 350  d_loss: 0.102223  g_loss: -0.107481\n",
      "Elapsed: [2:05:47.090022]  batch: 400  d_loss: -0.058499  g_loss: 0.294915\n",
      "Elapsed: [2:05:51.558701]  batch: 450  d_loss: 0.050862  g_loss: 0.034507\n",
      "Elapsed: [2:05:56.052509]  batch: 500  d_loss: 0.030800  g_loss: 0.013754\n",
      "Time taken for epoch: 54.386 secs\n",
      "ticker =  20501\n",
      "\n",
      "Epoch: 42\n",
      "Elapsed: [2:06:05.499236]  batch: 1  d_loss: 0.018973  g_loss: 0.016923\n",
      "Elapsed: [2:06:10.010306]  batch: 50  d_loss: 0.044635  g_loss: 0.026241\n",
      "Elapsed: [2:06:14.668481]  batch: 100  d_loss: 0.027414  g_loss: -0.070453\n",
      "Elapsed: [2:06:19.240352]  batch: 150  d_loss: 0.020497  g_loss: 0.036706\n",
      "Elapsed: [2:06:23.733327]  batch: 200  d_loss: -0.017068  g_loss: 0.068058\n",
      "Elapsed: [2:06:28.194529]  batch: 250  d_loss: 0.050854  g_loss: 0.071977\n",
      "Elapsed: [2:06:32.705964]  batch: 300  d_loss: 0.016174  g_loss: -0.030425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: [2:06:37.184259]  batch: 350  d_loss: 0.117703  g_loss: -0.082715\n",
      "Elapsed: [2:06:41.647450]  batch: 400  d_loss: 0.000424  g_loss: 0.167324\n",
      "Elapsed: [2:06:46.090781]  batch: 450  d_loss: 0.016573  g_loss: -0.062392\n",
      "Elapsed: [2:06:50.582284]  batch: 500  d_loss: -0.019023  g_loss: 0.004111\n",
      "Time taken for epoch: 54.427 secs\n",
      "ticker =  21001\n",
      "\n",
      "Epoch: 43\n",
      "Elapsed: [2:07:00.027323]  batch: 1  d_loss: -0.033422  g_loss: 0.027748\n",
      "Elapsed: [2:07:04.486825]  batch: 50  d_loss: 0.013473  g_loss: 0.050636\n",
      "Elapsed: [2:07:08.887109]  batch: 100  d_loss: -0.006238  g_loss: 0.086658\n",
      "Elapsed: [2:07:13.355757]  batch: 150  d_loss: -0.023206  g_loss: 0.007937\n",
      "Elapsed: [2:07:17.769928]  batch: 200  d_loss: -0.026877  g_loss: 0.063897\n",
      "Elapsed: [2:07:22.218243]  batch: 250  d_loss: 0.016493  g_loss: 0.033780\n",
      "Elapsed: [2:07:26.648941]  batch: 300  d_loss: 0.005970  g_loss: 0.047347\n",
      "Elapsed: [2:07:31.030463]  batch: 350  d_loss: 0.034983  g_loss: -0.010245\n",
      "Elapsed: [2:07:35.447525]  batch: 400  d_loss: 0.023066  g_loss: 0.026155\n",
      "Elapsed: [2:07:39.926177]  batch: 450  d_loss: -0.007802  g_loss: 0.017336\n",
      "Elapsed: [2:07:44.311941]  batch: 500  d_loss: 0.063391  g_loss: -0.038158\n",
      "Time taken for epoch: 53.640 secs\n",
      "ticker =  21501\n",
      "\n",
      "Epoch: 44\n",
      "Elapsed: [2:07:53.950799]  batch: 1  d_loss: 0.028344  g_loss: -0.024865\n",
      "Elapsed: [2:07:58.319783]  batch: 50  d_loss: 0.056714  g_loss: -0.013956\n",
      "Elapsed: [2:08:02.716404]  batch: 100  d_loss: 0.015540  g_loss: 0.002587\n",
      "Elapsed: [2:08:07.139099]  batch: 150  d_loss: 0.020719  g_loss: 0.007550\n",
      "Elapsed: [2:08:11.553229]  batch: 200  d_loss: 0.018835  g_loss: 0.035475\n",
      "Elapsed: [2:08:15.994587]  batch: 250  d_loss: 0.008870  g_loss: 0.013936\n",
      "Elapsed: [2:08:20.473507]  batch: 300  d_loss: 0.012470  g_loss: 0.061637\n",
      "Elapsed: [2:08:24.900004]  batch: 350  d_loss: 0.087502  g_loss: 0.010985\n",
      "Elapsed: [2:08:29.338331]  batch: 400  d_loss: 0.009119  g_loss: -0.027493\n",
      "Elapsed: [2:08:33.790618]  batch: 450  d_loss: -0.014273  g_loss: 0.066874\n",
      "Elapsed: [2:08:38.260676]  batch: 500  d_loss: -0.009161  g_loss: 0.053787\n",
      "Time taken for epoch: 53.845 secs\n",
      "ticker =  22001\n",
      "\n",
      "Epoch: 45\n",
      "Elapsed: [2:08:47.699764]  batch: 1  d_loss: 0.011510  g_loss: 0.024866\n",
      "Elapsed: [2:08:52.119431]  batch: 50  d_loss: -0.000989  g_loss: 0.003003\n",
      "Elapsed: [2:08:56.524850]  batch: 100  d_loss: 0.001293  g_loss: -0.018552\n",
      "Elapsed: [2:09:01.131349]  batch: 150  d_loss: -0.008085  g_loss: 0.062533\n",
      "Elapsed: [2:09:05.836227]  batch: 200  d_loss: 0.032344  g_loss: -0.024178\n",
      "Elapsed: [2:09:10.634673]  batch: 250  d_loss: 0.052332  g_loss: -0.019776\n",
      "Elapsed: [2:09:15.404832]  batch: 300  d_loss: 0.029242  g_loss: 0.061357\n",
      "Elapsed: [2:09:20.130698]  batch: 350  d_loss: 0.006143  g_loss: 0.028283\n",
      "Elapsed: [2:09:24.607233]  batch: 400  d_loss: 0.069142  g_loss: 0.129044\n",
      "Elapsed: [2:09:29.041085]  batch: 450  d_loss: 0.014740  g_loss: 0.043160\n",
      "Elapsed: [2:09:33.389723]  batch: 500  d_loss: 0.065512  g_loss: -0.024865\n",
      "Time taken for epoch: 55.028 secs\n",
      "ticker =  22501\n",
      "\n",
      "Epoch: 46\n",
      "Elapsed: [2:09:42.826009]  batch: 1  d_loss: 0.018085  g_loss: -0.017839\n",
      "Elapsed: [2:09:47.219887]  batch: 50  d_loss: 0.028880  g_loss: 0.088656\n",
      "Elapsed: [2:09:51.649470]  batch: 100  d_loss: -0.017940  g_loss: 0.038299\n",
      "Elapsed: [2:09:56.051435]  batch: 150  d_loss: 0.023712  g_loss: 0.076097\n",
      "Elapsed: [2:10:00.458221]  batch: 200  d_loss: -0.017635  g_loss: 0.009594\n",
      "Elapsed: [2:10:04.910809]  batch: 250  d_loss: 0.041436  g_loss: 0.004621\n",
      "Elapsed: [2:10:09.335229]  batch: 300  d_loss: 0.055468  g_loss: 0.007829\n",
      "Elapsed: [2:10:13.842270]  batch: 350  d_loss: 0.008073  g_loss: -0.015344\n",
      "Elapsed: [2:10:18.279099]  batch: 400  d_loss: 0.041394  g_loss: 0.027250\n",
      "Elapsed: [2:10:22.726012]  batch: 450  d_loss: -0.016382  g_loss: 0.061186\n",
      "Elapsed: [2:10:27.115238]  batch: 500  d_loss: 0.045289  g_loss: -0.027512\n",
      "Time taken for epoch: 53.623 secs\n",
      "ticker =  23001\n",
      "\n",
      "Epoch: 47\n",
      "Elapsed: [2:10:36.737332]  batch: 1  d_loss: 0.002069  g_loss: -0.029120\n",
      "Elapsed: [2:10:45.020930]  batch: 50  d_loss: 0.047444  g_loss: 0.024815\n",
      "Elapsed: [2:10:52.983913]  batch: 100  d_loss: -0.043933  g_loss: 0.081904\n",
      "Elapsed: [2:11:00.292457]  batch: 150  d_loss: 0.032464  g_loss: -0.016220\n",
      "Elapsed: [2:11:07.832766]  batch: 200  d_loss: -0.004631  g_loss: 0.127365\n",
      "Elapsed: [2:11:12.717595]  batch: 250  d_loss: 0.026499  g_loss: 0.012076\n",
      "Elapsed: [2:11:17.302963]  batch: 300  d_loss: -0.013199  g_loss: 0.029647\n",
      "Elapsed: [2:11:21.888856]  batch: 350  d_loss: -0.013848  g_loss: 0.019715\n",
      "Elapsed: [2:11:28.704058]  batch: 400  d_loss: 0.022274  g_loss: 0.054777\n",
      "Elapsed: [2:11:36.740315]  batch: 450  d_loss: -0.023412  g_loss: 0.013246\n",
      "Elapsed: [2:11:44.724834]  batch: 500  d_loss: 0.035136  g_loss: 0.058711\n",
      "Time taken for epoch: 77.586 secs\n",
      "ticker =  23501\n",
      "\n",
      "Epoch: 48\n",
      "Elapsed: [2:11:57.584919]  batch: 1  d_loss: 0.041212  g_loss: 0.041235\n",
      "Elapsed: [2:12:01.893216]  batch: 50  d_loss: -0.009256  g_loss: 0.075508\n",
      "Elapsed: [2:12:06.319318]  batch: 100  d_loss: 0.044278  g_loss: -0.130090\n",
      "Elapsed: [2:12:10.788340]  batch: 150  d_loss: -0.025129  g_loss: 0.100578\n",
      "Elapsed: [2:12:15.184500]  batch: 200  d_loss: 0.015772  g_loss: 0.034725\n",
      "Elapsed: [2:12:19.587173]  batch: 250  d_loss: 0.036012  g_loss: 0.018341\n",
      "Elapsed: [2:12:23.999945]  batch: 300  d_loss: 0.082860  g_loss: 0.080271\n",
      "Elapsed: [2:12:28.412312]  batch: 350  d_loss: 0.077146  g_loss: -0.076417\n",
      "Elapsed: [2:12:32.790667]  batch: 400  d_loss: 0.021030  g_loss: -0.021700\n",
      "Elapsed: [2:12:37.253193]  batch: 450  d_loss: 0.035328  g_loss: 0.019513\n",
      "Elapsed: [2:12:41.658942]  batch: 500  d_loss: -0.004100  g_loss: 0.039860\n",
      "Time taken for epoch: 56.708 secs\n",
      "ticker =  24001\n",
      "\n",
      "Epoch: 49\n",
      "Elapsed: [2:12:51.036503]  batch: 1  d_loss: -0.020734  g_loss: 0.043034\n",
      "Elapsed: [2:12:55.473030]  batch: 50  d_loss: -0.003447  g_loss: 0.078872\n",
      "Elapsed: [2:12:59.935842]  batch: 100  d_loss: 0.008502  g_loss: 0.038459\n",
      "Elapsed: [2:13:04.386927]  batch: 150  d_loss: 0.167645  g_loss: 0.028908\n",
      "Elapsed: [2:13:08.809467]  batch: 200  d_loss: 0.004632  g_loss: 0.067503\n",
      "Elapsed: [2:13:13.232231]  batch: 250  d_loss: -0.010876  g_loss: 0.071457\n",
      "Elapsed: [2:13:17.608180]  batch: 300  d_loss: 0.004541  g_loss: -0.029044\n",
      "Elapsed: [2:13:22.310956]  batch: 350  d_loss: 0.034949  g_loss: 0.047319\n",
      "Elapsed: [2:13:27.059405]  batch: 400  d_loss: -0.006480  g_loss: 0.076395\n",
      "Elapsed: [2:13:31.520996]  batch: 450  d_loss: 0.021541  g_loss: 0.031067\n",
      "Elapsed: [2:13:35.973020]  batch: 500  d_loss: -0.004129  g_loss: -0.005342\n",
      "Time taken for epoch: 54.201 secs\n",
      "ticker =  24501\n",
      "\n",
      "Epoch: 50\n",
      "Elapsed: [2:13:45.398224]  batch: 1  d_loss: -0.024323  g_loss: 0.075363\n",
      "Elapsed: [2:13:49.777169]  batch: 50  d_loss: -0.000168  g_loss: 0.035602\n",
      "Elapsed: [2:13:54.240190]  batch: 100  d_loss: 0.025477  g_loss: 0.061958\n",
      "Elapsed: [2:13:58.665977]  batch: 150  d_loss: 0.013398  g_loss: 0.026396\n",
      "Elapsed: [2:14:03.090904]  batch: 200  d_loss: -0.000487  g_loss: 0.048828\n",
      "Elapsed: [2:14:07.528705]  batch: 250  d_loss: 0.012755  g_loss: 0.077712\n",
      "Elapsed: [2:14:12.086733]  batch: 300  d_loss: 0.045961  g_loss: 0.004765\n",
      "Elapsed: [2:14:16.519804]  batch: 350  d_loss: 0.050663  g_loss: -0.011020\n",
      "Elapsed: [2:14:20.926575]  batch: 400  d_loss: -0.016563  g_loss: 0.107651\n",
      "Elapsed: [2:14:25.525431]  batch: 450  d_loss: 0.053426  g_loss: 0.038207\n",
      "Elapsed: [2:14:29.922653]  batch: 500  d_loss: 0.023778  g_loss: 0.022547\n",
      "Time taken for epoch: 53.833 secs\n",
      "ticker =  25001\n",
      "\n",
      "\n",
      "Currently working on Depth:  4\n",
      "Current resolution: 32 x 32\n",
      "Ticker 1\n",
      "\n",
      "Epoch: 1\n",
      "Elapsed: [2:14:39.420568]  batch: 1  d_loss: 2.430351  g_loss: 0.039235\n",
      "Elapsed: [2:14:45.490860]  batch: 50  d_loss: -0.045928  g_loss: 0.179725\n",
      "Elapsed: [2:14:51.638991]  batch: 100  d_loss: 0.032025  g_loss: 0.115282\n",
      "Elapsed: [2:14:57.830729]  batch: 150  d_loss: 0.068944  g_loss: 0.070045\n",
      "Elapsed: [2:15:03.994790]  batch: 200  d_loss: 0.016060  g_loss: 0.095652\n",
      "Elapsed: [2:15:10.129937]  batch: 250  d_loss: 0.024502  g_loss: 0.051261\n",
      "Elapsed: [2:15:16.304113]  batch: 300  d_loss: 0.014631  g_loss: 0.051597\n",
      "Elapsed: [2:15:22.484593]  batch: 350  d_loss: 0.011591  g_loss: 0.076662\n",
      "Elapsed: [2:15:28.669566]  batch: 400  d_loss: 0.033414  g_loss: 0.081249\n",
      "Elapsed: [2:15:34.821649]  batch: 450  d_loss: -0.027314  g_loss: 0.073634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: [2:15:40.980827]  batch: 500  d_loss: 0.019000  g_loss: 0.142247\n",
      "Time taken for epoch: 70.968 secs\n",
      "ticker =  501\n",
      "\n",
      "Epoch: 2\n",
      "Elapsed: [2:15:51.147716]  batch: 1  d_loss: 0.046761  g_loss: 0.049863\n",
      "Elapsed: [2:15:57.236075]  batch: 50  d_loss: -0.013255  g_loss: 0.058722\n",
      "Elapsed: [2:16:03.411481]  batch: 100  d_loss: -0.026834  g_loss: 0.104875\n",
      "Elapsed: [2:16:09.603434]  batch: 150  d_loss: 0.061415  g_loss: 0.152654\n",
      "Elapsed: [2:16:15.785766]  batch: 200  d_loss: -0.083009  g_loss: 0.272235\n",
      "Elapsed: [2:16:21.969760]  batch: 250  d_loss: -0.007591  g_loss: 0.039450\n",
      "Elapsed: [2:16:28.170457]  batch: 300  d_loss: 0.005643  g_loss: 0.137576\n",
      "Elapsed: [2:16:34.331649]  batch: 350  d_loss: 0.405828  g_loss: 0.098949\n",
      "Elapsed: [2:16:40.490384]  batch: 400  d_loss: -0.152123  g_loss: 0.173913\n",
      "Elapsed: [2:16:46.664046]  batch: 450  d_loss: -0.011928  g_loss: 0.151595\n",
      "Elapsed: [2:16:52.848428]  batch: 500  d_loss: -0.095096  g_loss: 0.257011\n",
      "Time taken for epoch: 71.767 secs\n",
      "ticker =  1001\n",
      "\n",
      "Epoch: 3\n",
      "Elapsed: [2:17:02.706174]  batch: 1  d_loss: 0.006991  g_loss: 0.110679\n",
      "Elapsed: [2:17:08.794796]  batch: 50  d_loss: 0.061869  g_loss: 0.072596\n",
      "Elapsed: [2:17:14.964509]  batch: 100  d_loss: -0.007397  g_loss: 0.139105\n",
      "Elapsed: [2:17:21.182929]  batch: 150  d_loss: 0.041818  g_loss: 0.085541\n",
      "Elapsed: [2:17:27.375587]  batch: 200  d_loss: 0.057385  g_loss: -0.002068\n",
      "Elapsed: [2:17:33.554472]  batch: 250  d_loss: 0.026174  g_loss: 0.080701\n",
      "Elapsed: [2:17:39.755711]  batch: 300  d_loss: -0.094445  g_loss: 0.171308\n",
      "Elapsed: [2:17:45.887779]  batch: 350  d_loss: 0.031588  g_loss: -0.043814\n",
      "Elapsed: [2:17:52.053956]  batch: 400  d_loss: -0.016262  g_loss: 0.186789\n",
      "Elapsed: [2:17:58.216787]  batch: 450  d_loss: -0.128917  g_loss: 0.220777\n",
      "Elapsed: [2:18:04.386165]  batch: 500  d_loss: 0.056577  g_loss: 0.028087\n",
      "Time taken for epoch: 71.410 secs\n",
      "ticker =  1501\n",
      "\n",
      "Epoch: 4\n",
      "Elapsed: [2:18:14.271669]  batch: 1  d_loss: -0.013915  g_loss: 0.069631\n",
      "Elapsed: [2:18:20.386200]  batch: 50  d_loss: -0.045083  g_loss: 0.192053\n",
      "Elapsed: [2:18:26.984120]  batch: 100  d_loss: -0.049228  g_loss: 0.212419\n",
      "Elapsed: [2:18:33.199145]  batch: 150  d_loss: -0.231862  g_loss: 0.424234\n",
      "Elapsed: [2:18:39.409059]  batch: 200  d_loss: -0.062239  g_loss: 0.221731\n",
      "Elapsed: [2:18:45.622448]  batch: 250  d_loss: -0.118664  g_loss: 0.226129\n",
      "Elapsed: [2:18:51.809639]  batch: 300  d_loss: -0.031505  g_loss: 0.227378\n",
      "Elapsed: [2:18:58.021627]  batch: 350  d_loss: -0.062339  g_loss: 0.251736\n",
      "Elapsed: [2:19:04.225079]  batch: 400  d_loss: -0.191812  g_loss: 0.293837\n",
      "Elapsed: [2:19:10.421129]  batch: 450  d_loss: 0.005792  g_loss: 0.121667\n",
      "Elapsed: [2:19:16.615550]  batch: 500  d_loss: -0.221229  g_loss: 0.328078\n",
      "Time taken for epoch: 72.134 secs\n",
      "ticker =  2001\n",
      "\n",
      "Epoch: 5\n",
      "Elapsed: [2:19:26.623611]  batch: 1  d_loss: 0.036371  g_loss: 0.081055\n",
      "Elapsed: [2:19:33.103123]  batch: 50  d_loss: 0.015503  g_loss: 0.042063\n",
      "Elapsed: [2:19:39.332961]  batch: 100  d_loss: 0.073119  g_loss: 0.281089\n",
      "Elapsed: [2:19:45.575196]  batch: 150  d_loss: -0.024141  g_loss: 0.295796\n",
      "Elapsed: [2:19:51.747916]  batch: 200  d_loss: 0.014381  g_loss: 0.121535\n",
      "Elapsed: [2:19:57.947033]  batch: 250  d_loss: -0.226727  g_loss: 0.425466\n",
      "Elapsed: [2:20:04.163560]  batch: 300  d_loss: 0.008831  g_loss: 0.090710\n",
      "Elapsed: [2:20:10.430078]  batch: 350  d_loss: -0.051991  g_loss: 0.082849\n",
      "Elapsed: [2:20:16.639664]  batch: 400  d_loss: -0.157365  g_loss: 0.290209\n",
      "Elapsed: [2:20:22.843419]  batch: 450  d_loss: -0.255368  g_loss: 0.399371\n",
      "Elapsed: [2:20:29.035492]  batch: 500  d_loss: 0.042206  g_loss: 0.080739\n",
      "Time taken for epoch: 72.286 secs\n",
      "ticker =  2501\n",
      "\n",
      "Epoch: 6\n",
      "Elapsed: [2:20:38.992582]  batch: 1  d_loss: -0.222336  g_loss: 0.318914\n",
      "Elapsed: [2:20:45.086286]  batch: 50  d_loss: -0.159830  g_loss: 0.276237\n",
      "Elapsed: [2:20:51.272862]  batch: 100  d_loss: -0.075564  g_loss: 0.161103\n",
      "Elapsed: [2:20:57.485927]  batch: 150  d_loss: -0.019716  g_loss: 0.219686\n",
      "Elapsed: [2:21:03.690004]  batch: 200  d_loss: -0.152472  g_loss: 0.262083\n",
      "Elapsed: [2:21:09.942537]  batch: 250  d_loss: -0.137089  g_loss: 0.276563\n",
      "Elapsed: [2:21:16.228750]  batch: 300  d_loss: -0.185943  g_loss: 0.319486\n",
      "Elapsed: [2:21:22.777338]  batch: 350  d_loss: -0.366804  g_loss: 0.501213\n",
      "Elapsed: [2:21:30.262524]  batch: 400  d_loss: -0.080023  g_loss: 0.126506\n",
      "Elapsed: [2:21:36.870329]  batch: 450  d_loss: 0.017715  g_loss: 0.115423\n",
      "Elapsed: [2:21:43.098233]  batch: 500  d_loss: -0.280824  g_loss: 0.438549\n",
      "Time taken for epoch: 73.968 secs\n",
      "ticker =  3001\n",
      "\n",
      "Epoch: 7\n",
      "Elapsed: [2:21:53.758393]  batch: 1  d_loss: -0.055909  g_loss: 0.260507\n",
      "Elapsed: [2:21:59.931173]  batch: 50  d_loss: -0.379185  g_loss: 0.524032\n",
      "Elapsed: [2:22:09.614538]  batch: 100  d_loss: -0.218273  g_loss: 0.310725\n",
      "Elapsed: [2:22:20.653907]  batch: 150  d_loss: 0.111999  g_loss: 0.090790\n",
      "Elapsed: [2:22:31.346433]  batch: 200  d_loss: -0.130704  g_loss: 0.299314\n",
      "Elapsed: [2:22:44.590686]  batch: 250  d_loss: -0.286528  g_loss: 0.543988\n",
      "Elapsed: [2:22:54.807451]  batch: 300  d_loss: -0.134180  g_loss: 0.188752\n",
      "Elapsed: [2:23:04.347159]  batch: 350  d_loss: -0.300497  g_loss: 0.501893\n",
      "Elapsed: [2:23:13.780966]  batch: 400  d_loss: -0.166569  g_loss: 0.309622\n",
      "Elapsed: [2:23:23.317235]  batch: 450  d_loss: -0.165198  g_loss: 0.225044\n",
      "Elapsed: [2:23:33.367095]  batch: 500  d_loss: 0.098124  g_loss: 0.078332\n",
      "Time taken for epoch: 110.284 secs\n",
      "ticker =  3501\n",
      "\n",
      "Epoch: 8\n",
      "Elapsed: [2:23:51.575003]  batch: 1  d_loss: -0.151617  g_loss: 0.235539\n",
      "Elapsed: [2:24:00.230136]  batch: 50  d_loss: -0.133313  g_loss: 0.309786\n",
      "Elapsed: [2:24:10.048383]  batch: 100  d_loss: -0.023151  g_loss: 0.158824\n",
      "Elapsed: [2:24:19.558038]  batch: 150  d_loss: 0.035128  g_loss: 0.173578\n",
      "Elapsed: [2:24:28.747944]  batch: 200  d_loss: -0.213030  g_loss: 0.416850\n",
      "Elapsed: [2:24:38.406990]  batch: 250  d_loss: 0.077864  g_loss: 0.090346\n",
      "Elapsed: [2:24:47.891314]  batch: 300  d_loss: -0.260647  g_loss: 0.470081\n",
      "Elapsed: [2:24:57.389719]  batch: 350  d_loss: -0.254719  g_loss: 0.459882\n",
      "Elapsed: [2:25:06.919486]  batch: 400  d_loss: -0.226464  g_loss: 0.365007\n",
      "Elapsed: [2:25:16.332819]  batch: 450  d_loss: -0.108975  g_loss: 0.177918\n",
      "Elapsed: [2:25:25.805961]  batch: 500  d_loss: -0.343043  g_loss: 0.532461\n",
      "Time taken for epoch: 112.338 secs\n",
      "ticker =  4001\n",
      "\n",
      "Epoch: 9\n",
      "Elapsed: [2:25:42.804091]  batch: 1  d_loss: -0.104949  g_loss: 0.296651\n",
      "Elapsed: [2:25:51.791372]  batch: 50  d_loss: -0.057509  g_loss: 0.150976\n",
      "Elapsed: [2:26:01.318640]  batch: 100  d_loss: -0.349236  g_loss: 0.590734\n",
      "Elapsed: [2:26:10.312017]  batch: 150  d_loss: -0.321751  g_loss: 0.488188\n",
      "Elapsed: [2:26:20.109823]  batch: 200  d_loss: -0.345570  g_loss: 0.538535\n",
      "Elapsed: [2:26:29.547859]  batch: 250  d_loss: -0.300736  g_loss: 0.424958\n",
      "Elapsed: [2:26:38.875715]  batch: 300  d_loss: -0.019008  g_loss: 0.179394\n",
      "Elapsed: [2:26:48.484982]  batch: 350  d_loss: 0.064928  g_loss: 0.202379\n",
      "Elapsed: [2:26:58.347618]  batch: 400  d_loss: -0.191076  g_loss: 0.357357\n",
      "Elapsed: [2:27:08.032901]  batch: 450  d_loss: -0.225146  g_loss: 0.305033\n",
      "Elapsed: [2:27:17.676862]  batch: 500  d_loss: -0.040523  g_loss: 0.285710\n",
      "Time taken for epoch: 111.535 secs\n",
      "ticker =  4501\n",
      "\n",
      "Epoch: 10\n",
      "Elapsed: [2:27:34.687630]  batch: 1  d_loss: -0.126615  g_loss: 0.447591\n",
      "Elapsed: [2:27:42.590226]  batch: 50  d_loss: -0.134827  g_loss: 0.268352\n",
      "Elapsed: [2:27:48.868167]  batch: 100  d_loss: 0.027138  g_loss: 0.035439\n",
      "Elapsed: [2:27:58.383321]  batch: 150  d_loss: -0.336504  g_loss: 0.517672\n",
      "Elapsed: [2:28:08.006814]  batch: 200  d_loss: -0.047208  g_loss: 0.400955\n",
      "Elapsed: [2:28:17.601146]  batch: 250  d_loss: -0.006501  g_loss: 0.171557\n",
      "Elapsed: [2:28:27.259437]  batch: 300  d_loss: -0.069190  g_loss: 0.189374\n",
      "Elapsed: [2:28:36.716197]  batch: 350  d_loss: -0.115801  g_loss: 0.305935\n",
      "Elapsed: [2:28:45.807555]  batch: 400  d_loss: -0.215665  g_loss: 0.367537\n",
      "Elapsed: [2:28:55.575716]  batch: 450  d_loss: -0.364776  g_loss: 0.513303\n",
      "Elapsed: [2:29:07.580491]  batch: 500  d_loss: -0.030245  g_loss: 0.311496\n",
      "Time taken for epoch: 109.949 secs\n",
      "ticker =  5001\n",
      "\n",
      "Epoch: 11\n",
      "Elapsed: [2:29:27.883884]  batch: 1  d_loss: -0.336250  g_loss: 0.567035\n",
      "Elapsed: [2:29:34.091234]  batch: 50  d_loss: 0.018208  g_loss: 0.147704\n",
      "Elapsed: [2:29:39.945068]  batch: 100  d_loss: 0.027107  g_loss: 0.209431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: [2:29:46.074236]  batch: 150  d_loss: -0.054770  g_loss: 0.211110\n",
      "Elapsed: [2:29:52.229980]  batch: 200  d_loss: -0.312001  g_loss: 0.451433\n",
      "Elapsed: [2:29:58.418600]  batch: 250  d_loss: 0.175528  g_loss: -0.002627\n",
      "Elapsed: [2:30:04.583733]  batch: 300  d_loss: 0.015449  g_loss: 0.003321\n",
      "Elapsed: [2:30:10.749799]  batch: 350  d_loss: 0.026741  g_loss: 0.171894\n",
      "Elapsed: [2:30:16.980312]  batch: 400  d_loss: -0.270151  g_loss: 0.401866\n",
      "Elapsed: [2:30:23.141696]  batch: 450  d_loss: -0.207067  g_loss: 0.397556\n",
      "Elapsed: [2:30:29.297606]  batch: 500  d_loss: -0.031700  g_loss: 0.305878\n",
      "Time taken for epoch: 81.157 secs\n",
      "ticker =  5501\n",
      "\n",
      "Epoch: 12\n",
      "Elapsed: [2:30:39.576615]  batch: 1  d_loss: -0.368275  g_loss: 0.579800\n",
      "Elapsed: [2:30:45.663948]  batch: 50  d_loss: -0.139299  g_loss: 0.298937\n",
      "Elapsed: [2:30:51.852849]  batch: 100  d_loss: -0.208773  g_loss: 0.407213\n",
      "Elapsed: [2:30:58.053041]  batch: 150  d_loss: -0.210439  g_loss: 0.523214\n",
      "Elapsed: [2:31:04.268483]  batch: 200  d_loss: -0.059987  g_loss: 0.314269\n",
      "Elapsed: [2:31:10.451487]  batch: 250  d_loss: 0.069007  g_loss: 0.053842\n",
      "Elapsed: [2:31:16.640190]  batch: 300  d_loss: -0.274248  g_loss: 0.487868\n",
      "Elapsed: [2:31:23.005131]  batch: 350  d_loss: -0.024136  g_loss: 0.261523\n",
      "Elapsed: [2:31:29.233026]  batch: 400  d_loss: -0.265001  g_loss: 0.470202\n",
      "Elapsed: [2:31:35.435414]  batch: 450  d_loss: -0.368175  g_loss: 0.533220\n",
      "Elapsed: [2:31:41.655071]  batch: 500  d_loss: 0.054578  g_loss: 0.228403\n",
      "Time taken for epoch: 72.245 secs\n",
      "ticker =  6001\n",
      "\n",
      "Epoch: 13\n",
      "Elapsed: [2:31:51.938852]  batch: 1  d_loss: -0.332002  g_loss: 0.632783\n",
      "Elapsed: [2:31:58.026637]  batch: 50  d_loss: 0.119540  g_loss: 0.190499\n",
      "Elapsed: [2:32:04.245888]  batch: 100  d_loss: -0.378902  g_loss: 0.525139\n",
      "Elapsed: [2:32:10.473181]  batch: 150  d_loss: 0.291491  g_loss: 0.021336\n",
      "Elapsed: [2:32:16.823548]  batch: 200  d_loss: -0.375310  g_loss: 0.607182\n",
      "Elapsed: [2:32:23.020035]  batch: 250  d_loss: -0.074004  g_loss: 0.288711\n",
      "Elapsed: [2:32:29.271499]  batch: 300  d_loss: -0.396632  g_loss: 0.835677\n",
      "Elapsed: [2:32:35.620959]  batch: 350  d_loss: -0.255812  g_loss: 0.420420\n",
      "Elapsed: [2:32:41.851501]  batch: 400  d_loss: -0.469765  g_loss: 0.628063\n",
      "Elapsed: [2:32:48.058126]  batch: 450  d_loss: 0.059732  g_loss: 0.211289\n",
      "Elapsed: [2:32:54.265724]  batch: 500  d_loss: -0.347425  g_loss: 0.593050\n",
      "Time taken for epoch: 72.496 secs\n",
      "ticker =  6501\n",
      "\n",
      "Epoch: 14\n",
      "Elapsed: [2:33:04.674285]  batch: 1  d_loss: 0.343405  g_loss: 0.184638\n",
      "Elapsed: [2:33:10.745604]  batch: 50  d_loss: -0.296664  g_loss: 0.494842\n",
      "Elapsed: [2:33:16.980966]  batch: 100  d_loss: -0.264626  g_loss: 0.330974\n",
      "Elapsed: [2:33:23.254025]  batch: 150  d_loss: 0.142446  g_loss: 0.203427\n",
      "Elapsed: [2:33:29.541543]  batch: 200  d_loss: -0.448083  g_loss: 0.578853\n",
      "Elapsed: [2:33:35.767588]  batch: 250  d_loss: 0.034603  g_loss: 0.026862\n",
      "Elapsed: [2:33:42.016184]  batch: 300  d_loss: 0.011822  g_loss: 0.182548\n",
      "Elapsed: [2:33:48.239917]  batch: 350  d_loss: -0.456971  g_loss: 0.697029\n",
      "Elapsed: [2:33:54.499506]  batch: 400  d_loss: -0.433804  g_loss: 0.614329\n",
      "Elapsed: [2:34:00.745909]  batch: 450  d_loss: -0.354797  g_loss: 0.539688\n",
      "Elapsed: [2:34:06.973036]  batch: 500  d_loss: 0.033167  g_loss: 0.101711\n",
      "Time taken for epoch: 72.621 secs\n",
      "ticker =  7001\n",
      "\n",
      "Epoch: 15\n",
      "Elapsed: [2:34:17.454664]  batch: 1  d_loss: -0.614061  g_loss: 0.645087\n",
      "Elapsed: [2:34:23.569503]  batch: 50  d_loss: -0.447488  g_loss: 0.676914\n",
      "Elapsed: [2:34:29.803074]  batch: 100  d_loss: -0.629474  g_loss: 0.738069\n",
      "Elapsed: [2:34:36.055079]  batch: 150  d_loss: -0.282381  g_loss: 0.508347\n",
      "Elapsed: [2:34:42.290185]  batch: 200  d_loss: -0.126691  g_loss: 0.322507\n",
      "Elapsed: [2:34:48.531199]  batch: 250  d_loss: 0.141067  g_loss: 0.271465\n",
      "Elapsed: [2:34:54.790049]  batch: 300  d_loss: 0.156984  g_loss: 0.100263\n",
      "Elapsed: [2:35:01.029005]  batch: 350  d_loss: -0.176051  g_loss: 0.329952\n",
      "Elapsed: [2:35:07.265186]  batch: 400  d_loss: 0.055045  g_loss: 0.036691\n",
      "Elapsed: [2:35:13.483638]  batch: 450  d_loss: -0.417491  g_loss: 0.595115\n",
      "Elapsed: [2:35:19.714222]  batch: 500  d_loss: -0.013438  g_loss: 0.152944\n",
      "Time taken for epoch: 72.615 secs\n",
      "ticker =  7501\n",
      "\n",
      "Epoch: 16\n",
      "Elapsed: [2:35:30.180112]  batch: 1  d_loss: -0.633455  g_loss: 0.812724\n",
      "Elapsed: [2:35:36.327416]  batch: 50  d_loss: 0.043457  g_loss: 0.103265\n",
      "Elapsed: [2:35:42.579625]  batch: 100  d_loss: -0.326193  g_loss: 0.479121\n",
      "Elapsed: [2:35:48.877181]  batch: 150  d_loss: 0.108505  g_loss: 0.097048\n",
      "Elapsed: [2:35:55.113120]  batch: 200  d_loss: 0.004334  g_loss: 0.313601\n",
      "Elapsed: [2:36:01.357035]  batch: 250  d_loss: 0.218159  g_loss: 0.211554\n",
      "Elapsed: [2:36:07.611411]  batch: 300  d_loss: -0.235205  g_loss: 0.430862\n",
      "Elapsed: [2:36:13.848770]  batch: 350  d_loss: -0.003015  g_loss: 0.223744\n",
      "Elapsed: [2:36:20.071026]  batch: 400  d_loss: -0.563552  g_loss: 0.822242\n",
      "Elapsed: [2:36:26.504819]  batch: 450  d_loss: -0.333162  g_loss: 0.597407\n",
      "Elapsed: [2:36:32.884973]  batch: 500  d_loss: -0.250362  g_loss: 0.475536\n",
      "Time taken for epoch: 73.069 secs\n",
      "ticker =  8001\n",
      "\n",
      "Epoch: 17\n",
      "Elapsed: [2:36:43.339665]  batch: 1  d_loss: -0.040358  g_loss: 0.341240\n",
      "Elapsed: [2:36:49.458918]  batch: 50  d_loss: -0.515752  g_loss: 0.803109\n",
      "Elapsed: [2:36:55.667397]  batch: 100  d_loss: -0.591910  g_loss: 0.741528\n",
      "Elapsed: [2:37:01.954790]  batch: 150  d_loss: 0.073331  g_loss: -0.012873\n",
      "Elapsed: [2:37:08.200455]  batch: 200  d_loss: -0.417423  g_loss: 0.706473\n",
      "Elapsed: [2:37:14.437028]  batch: 250  d_loss: -0.600642  g_loss: 0.804119\n",
      "Elapsed: [2:37:20.679172]  batch: 300  d_loss: -0.506208  g_loss: 0.703972\n",
      "Elapsed: [2:37:26.957969]  batch: 350  d_loss: -0.511037  g_loss: 0.606434\n",
      "Elapsed: [2:37:33.199959]  batch: 400  d_loss: 0.217218  g_loss: -0.078118\n",
      "Elapsed: [2:37:39.434329]  batch: 450  d_loss: -0.425560  g_loss: 0.570812\n",
      "Elapsed: [2:37:46.341592]  batch: 500  d_loss: 0.179175  g_loss: 0.279430\n",
      "Time taken for epoch: 73.309 secs\n",
      "ticker =  8501\n",
      "\n",
      "Epoch: 18\n",
      "Elapsed: [2:37:56.707329]  batch: 1  d_loss: -0.333774  g_loss: 0.599372\n",
      "Elapsed: [2:38:02.836783]  batch: 50  d_loss: 0.216204  g_loss: 0.132873\n",
      "Elapsed: [2:38:09.051536]  batch: 100  d_loss: -0.330202  g_loss: 0.544213\n",
      "Elapsed: [2:38:15.331184]  batch: 150  d_loss: -0.120578  g_loss: 0.292333\n",
      "Elapsed: [2:38:21.558117]  batch: 200  d_loss: -0.481781  g_loss: 0.833189\n",
      "Elapsed: [2:38:27.806376]  batch: 250  d_loss: -0.392459  g_loss: 0.634292\n",
      "Elapsed: [2:38:34.053369]  batch: 300  d_loss: -0.340287  g_loss: 0.803374\n",
      "Elapsed: [2:38:40.316279]  batch: 350  d_loss: -0.057010  g_loss: 0.283545\n",
      "Elapsed: [2:38:46.584558]  batch: 400  d_loss: -0.261115  g_loss: 0.351595\n",
      "Elapsed: [2:38:53.438398]  batch: 450  d_loss: 0.013928  g_loss: 0.155876\n",
      "Elapsed: [2:39:00.110818]  batch: 500  d_loss: -0.045992  g_loss: 0.328003\n",
      "Time taken for epoch: 73.755 secs\n",
      "ticker =  9001\n",
      "\n",
      "Epoch: 19\n",
      "Elapsed: [2:39:11.316861]  batch: 1  d_loss: -0.530910  g_loss: 0.700123\n",
      "Elapsed: [2:39:17.420712]  batch: 50  d_loss: -0.359490  g_loss: 0.501346\n",
      "Elapsed: [2:39:23.675523]  batch: 100  d_loss: -0.350030  g_loss: 0.591742\n",
      "Elapsed: [2:39:29.947635]  batch: 150  d_loss: -0.115032  g_loss: 0.297329\n",
      "Elapsed: [2:39:38.751357]  batch: 200  d_loss: 0.030308  g_loss: 0.141773\n",
      "Elapsed: [2:39:49.739460]  batch: 250  d_loss: -0.282063  g_loss: 0.540585\n",
      "Elapsed: [2:40:00.265035]  batch: 300  d_loss: -0.148507  g_loss: 0.345038\n",
      "Elapsed: [2:40:10.386949]  batch: 350  d_loss: 0.092864  g_loss: 0.056626\n",
      "Elapsed: [2:40:19.805511]  batch: 400  d_loss: -0.214719  g_loss: 0.318129\n",
      "Elapsed: [2:40:29.693854]  batch: 450  d_loss: -0.443218  g_loss: 0.463824\n",
      "Elapsed: [2:40:39.417391]  batch: 500  d_loss: 0.210606  g_loss: 0.117514\n",
      "Time taken for epoch: 99.169 secs\n",
      "ticker =  9501\n",
      "\n",
      "Epoch: 20\n",
      "Elapsed: [2:40:56.253275]  batch: 1  d_loss: -0.416701  g_loss: 0.759570\n",
      "Elapsed: [2:41:05.276295]  batch: 50  d_loss: -0.183606  g_loss: 0.504918\n",
      "Elapsed: [2:41:14.544480]  batch: 100  d_loss: -0.494167  g_loss: 0.651524\n",
      "Elapsed: [2:41:23.845678]  batch: 150  d_loss: -0.160175  g_loss: 0.486318\n",
      "Elapsed: [2:41:33.424923]  batch: 200  d_loss: 0.073098  g_loss: 0.336764\n",
      "Elapsed: [2:41:42.509752]  batch: 250  d_loss: -0.012413  g_loss: 0.185694\n",
      "Elapsed: [2:41:52.057556]  batch: 300  d_loss: -0.583105  g_loss: 0.691909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: [2:42:02.011961]  batch: 350  d_loss: -0.067220  g_loss: 0.302561\n",
      "Elapsed: [2:42:11.313973]  batch: 400  d_loss: -0.196971  g_loss: 0.472227\n",
      "Elapsed: [2:42:20.620409]  batch: 450  d_loss: -0.270722  g_loss: 0.433554\n",
      "Elapsed: [2:42:30.236071]  batch: 500  d_loss: -0.171418  g_loss: 0.443241\n",
      "Time taken for epoch: 110.633 secs\n",
      "ticker =  10001\n",
      "\n",
      "Epoch: 21\n",
      "Elapsed: [2:42:47.947673]  batch: 1  d_loss: -0.298443  g_loss: 0.520122\n",
      "Elapsed: [2:42:56.813110]  batch: 50  d_loss: -0.279021  g_loss: 0.577275\n",
      "Elapsed: [2:43:07.322522]  batch: 100  d_loss: -0.075209  g_loss: 0.254467\n",
      "Elapsed: [2:43:19.057387]  batch: 150  d_loss: -0.403831  g_loss: 0.647974\n",
      "Elapsed: [2:43:29.490289]  batch: 200  d_loss: -0.181836  g_loss: 0.503678\n",
      "Elapsed: [2:43:39.135102]  batch: 250  d_loss: 0.010408  g_loss: 0.287490\n",
      "Elapsed: [2:43:48.458257]  batch: 300  d_loss: -0.170178  g_loss: 0.395941\n",
      "Elapsed: [2:43:59.081268]  batch: 350  d_loss: -0.190033  g_loss: 0.405163\n",
      "Elapsed: [2:44:08.955059]  batch: 400  d_loss: 0.008428  g_loss: 0.120840\n",
      "Elapsed: [2:44:18.757582]  batch: 450  d_loss: -0.360986  g_loss: 0.584786\n",
      "Elapsed: [2:44:31.076279]  batch: 500  d_loss: -0.449938  g_loss: 0.605214\n",
      "Time taken for epoch: 120.850 secs\n",
      "ticker =  10501\n",
      "\n",
      "Epoch: 22\n",
      "Elapsed: [2:44:48.267554]  batch: 1  d_loss: 0.149522  g_loss: 0.043095\n",
      "Elapsed: [2:44:56.655931]  batch: 50  d_loss: -0.351346  g_loss: 0.584468\n",
      "Elapsed: [2:45:03.272622]  batch: 100  d_loss: -0.216073  g_loss: 0.372971\n",
      "Elapsed: [2:45:12.551928]  batch: 150  d_loss: -0.260346  g_loss: 0.483471\n",
      "Elapsed: [2:45:22.392057]  batch: 200  d_loss: -0.036680  g_loss: 0.274257\n",
      "Elapsed: [2:45:32.034698]  batch: 250  d_loss: 0.010694  g_loss: 0.190475\n",
      "Elapsed: [2:45:41.946118]  batch: 300  d_loss: -0.104202  g_loss: 0.301985\n",
      "Elapsed: [2:45:51.534805]  batch: 350  d_loss: -0.230723  g_loss: 0.438172\n",
      "Elapsed: [2:46:01.369477]  batch: 400  d_loss: -0.119727  g_loss: 0.106256\n",
      "Elapsed: [2:46:10.746672]  batch: 450  d_loss: -0.451913  g_loss: 0.668983\n",
      "Elapsed: [2:46:24.941803]  batch: 500  d_loss: -0.393259  g_loss: 0.468914\n",
      "Time taken for epoch: 113.682 secs\n",
      "ticker =  11001\n",
      "\n",
      "Epoch: 23\n",
      "Elapsed: [2:46:41.791792]  batch: 1  d_loss: 0.141613  g_loss: -0.119648\n",
      "Elapsed: [2:46:49.070158]  batch: 50  d_loss: -0.250474  g_loss: 0.431473\n",
      "Elapsed: [2:46:55.036757]  batch: 100  d_loss: -0.400241  g_loss: 0.581270\n",
      "Elapsed: [2:47:02.350697]  batch: 150  d_loss: 0.080559  g_loss: 0.123396\n",
      "Elapsed: [2:47:11.869277]  batch: 200  d_loss: 0.049348  g_loss: 0.042134\n",
      "Elapsed: [2:47:21.896678]  batch: 250  d_loss: 0.138450  g_loss: 0.036428\n",
      "Elapsed: [2:47:32.020822]  batch: 300  d_loss: 0.188610  g_loss: 0.171083\n",
      "Elapsed: [2:47:41.472607]  batch: 350  d_loss: 0.005306  g_loss: 0.156005\n",
      "Elapsed: [2:47:51.031730]  batch: 400  d_loss: -0.221029  g_loss: 0.474472\n",
      "Elapsed: [2:48:01.139805]  batch: 450  d_loss: -0.050313  g_loss: 0.260983\n",
      "Elapsed: [2:48:10.719486]  batch: 500  d_loss: -0.227399  g_loss: 0.435278\n",
      "Time taken for epoch: 105.336 secs\n",
      "ticker =  11501\n",
      "\n",
      "Epoch: 24\n",
      "Elapsed: [2:48:28.992608]  batch: 1  d_loss: -0.150246  g_loss: 0.320569\n",
      "Elapsed: [2:48:36.768111]  batch: 50  d_loss: -0.207227  g_loss: 0.531088\n",
      "Elapsed: [2:48:43.022042]  batch: 100  d_loss: -0.025041  g_loss: 0.239540\n",
      "Elapsed: [2:48:49.381927]  batch: 150  d_loss: -0.383678  g_loss: 0.569695\n",
      "Elapsed: [2:49:00.257496]  batch: 200  d_loss: -0.046999  g_loss: 0.283360\n",
      "Elapsed: [2:49:09.801821]  batch: 250  d_loss: -0.029685  g_loss: 0.095804\n",
      "Elapsed: [2:49:18.940170]  batch: 300  d_loss: -0.083836  g_loss: 0.403858\n",
      "Elapsed: [2:49:28.914773]  batch: 350  d_loss: -0.435753  g_loss: 0.635642\n",
      "Elapsed: [2:49:38.027149]  batch: 400  d_loss: 0.112232  g_loss: 0.196605\n",
      "Elapsed: [2:49:47.604733]  batch: 450  d_loss: 0.011073  g_loss: 0.207128\n",
      "Elapsed: [2:49:56.516615]  batch: 500  d_loss: 0.128401  g_loss: 0.122950\n",
      "Time taken for epoch: 105.654 secs\n",
      "ticker =  12001\n",
      "\n",
      "Epoch: 25\n",
      "Elapsed: [2:50:12.966213]  batch: 1  d_loss: -0.549884  g_loss: 0.724159\n",
      "Elapsed: [2:50:20.469590]  batch: 50  d_loss: -0.389227  g_loss: 0.628760\n",
      "Elapsed: [2:50:26.450649]  batch: 100  d_loss: 0.185727  g_loss: 0.079174\n",
      "Elapsed: [2:50:33.970977]  batch: 150  d_loss: -0.113305  g_loss: 0.288413\n",
      "Elapsed: [2:50:43.122517]  batch: 200  d_loss: -0.226264  g_loss: 0.417690\n",
      "Elapsed: [2:50:52.559803]  batch: 250  d_loss: -0.401653  g_loss: 0.660148\n",
      "Elapsed: [2:51:02.106645]  batch: 300  d_loss: -0.351850  g_loss: 0.709816\n",
      "Elapsed: [2:51:10.847355]  batch: 350  d_loss: -0.055486  g_loss: 0.321072\n",
      "Elapsed: [2:51:20.336354]  batch: 400  d_loss: -0.193024  g_loss: 0.387743\n",
      "Elapsed: [2:51:29.784916]  batch: 450  d_loss: 0.130594  g_loss: 0.229590\n",
      "Elapsed: [2:51:38.759147]  batch: 500  d_loss: -0.042813  g_loss: 0.271600\n",
      "Time taken for epoch: 102.169 secs\n",
      "ticker =  12501\n",
      "\n",
      "Epoch: 26\n",
      "Elapsed: [2:51:55.718041]  batch: 1  d_loss: -0.326782  g_loss: 0.568858\n",
      "Elapsed: [2:52:02.478307]  batch: 50  d_loss: 0.024410  g_loss: 0.054141\n",
      "Elapsed: [2:52:08.466762]  batch: 100  d_loss: -0.128084  g_loss: 0.334695\n",
      "Elapsed: [2:52:14.688167]  batch: 150  d_loss: 0.078563  g_loss: 0.064816\n",
      "Elapsed: [2:52:22.890053]  batch: 200  d_loss: -0.332164  g_loss: 0.565129\n",
      "Elapsed: [2:52:32.456105]  batch: 250  d_loss: -0.310990  g_loss: 0.509368\n",
      "Elapsed: [2:52:41.761535]  batch: 300  d_loss: -0.227095  g_loss: 0.454004\n",
      "Elapsed: [2:52:50.638045]  batch: 350  d_loss: 0.108075  g_loss: 0.059045\n",
      "Elapsed: [2:53:01.271888]  batch: 400  d_loss: -0.169347  g_loss: 0.377479\n",
      "Elapsed: [2:53:10.642168]  batch: 450  d_loss: -0.020609  g_loss: 0.138679\n",
      "Elapsed: [2:53:20.141740]  batch: 500  d_loss: -0.183019  g_loss: 0.493225\n",
      "Time taken for epoch: 101.053 secs\n",
      "ticker =  13001\n",
      "\n",
      "Epoch: 27\n",
      "Elapsed: [2:53:37.426857]  batch: 1  d_loss: -0.168216  g_loss: 0.443657\n",
      "Elapsed: [2:53:43.544235]  batch: 50  d_loss: -0.104103  g_loss: 0.435334\n",
      "Elapsed: [2:53:49.505756]  batch: 100  d_loss: 0.038631  g_loss: 0.258679\n",
      "Elapsed: [2:53:55.703803]  batch: 150  d_loss: -0.100077  g_loss: 0.356041\n",
      "Elapsed: [2:54:01.967044]  batch: 200  d_loss: -0.191062  g_loss: 0.565378\n",
      "Elapsed: [2:54:10.635713]  batch: 250  d_loss: 0.056772  g_loss: 0.283264\n",
      "Elapsed: [2:54:20.439725]  batch: 300  d_loss: -0.212440  g_loss: 0.319324\n",
      "Elapsed: [2:54:30.153695]  batch: 350  d_loss: 0.647629  g_loss: -0.004870\n",
      "Elapsed: [2:54:39.248040]  batch: 400  d_loss: -0.275868  g_loss: 0.496501\n",
      "Elapsed: [2:54:49.992268]  batch: 450  d_loss: 0.074393  g_loss: 0.080582\n",
      "Elapsed: [2:54:59.665450]  batch: 500  d_loss: 0.073774  g_loss: 0.284936\n",
      "Time taken for epoch: 99.363 secs\n",
      "ticker =  13501\n",
      "\n",
      "Epoch: 28\n",
      "Elapsed: [2:55:16.061377]  batch: 1  d_loss: -0.324338  g_loss: 0.557082\n",
      "Elapsed: [2:55:22.174233]  batch: 50  d_loss: -0.149509  g_loss: 0.568479\n",
      "Elapsed: [2:55:28.167340]  batch: 100  d_loss: 0.019579  g_loss: 0.049877\n",
      "Elapsed: [2:55:34.299852]  batch: 150  d_loss: -0.064153  g_loss: 0.344272\n",
      "Elapsed: [2:55:40.472721]  batch: 200  d_loss: -0.168711  g_loss: 0.483841\n",
      "Elapsed: [2:55:49.502465]  batch: 250  d_loss: 0.044041  g_loss: 0.084699\n",
      "Elapsed: [2:55:58.477113]  batch: 300  d_loss: -0.330781  g_loss: 0.596184\n",
      "Elapsed: [2:56:07.926835]  batch: 350  d_loss: -0.074131  g_loss: 0.210074\n",
      "Elapsed: [2:56:17.547140]  batch: 400  d_loss: -0.155661  g_loss: 0.368134\n",
      "Elapsed: [2:56:27.635977]  batch: 450  d_loss: -0.401587  g_loss: 0.453202\n",
      "Elapsed: [2:56:36.669325]  batch: 500  d_loss: -0.300164  g_loss: 0.580252\n",
      "Time taken for epoch: 96.921 secs\n",
      "ticker =  14001\n",
      "\n",
      "Epoch: 29\n",
      "Elapsed: [2:56:53.642973]  batch: 1  d_loss: 0.101959  g_loss: 0.283786\n",
      "Elapsed: [2:56:59.439574]  batch: 50  d_loss: 0.125024  g_loss: 0.034877\n",
      "Elapsed: [2:57:05.382986]  batch: 100  d_loss: -0.411720  g_loss: 0.489851\n",
      "Elapsed: [2:57:11.505532]  batch: 150  d_loss: -0.285312  g_loss: 0.560135\n",
      "Elapsed: [2:57:17.678247]  batch: 200  d_loss: -0.074263  g_loss: 0.352379\n",
      "Elapsed: [2:57:24.755035]  batch: 250  d_loss: -0.018009  g_loss: 0.280859\n",
      "Elapsed: [2:57:34.625735]  batch: 300  d_loss: 0.076840  g_loss: 0.003023\n",
      "Elapsed: [2:57:43.630631]  batch: 350  d_loss: -0.135136  g_loss: 0.426191\n",
      "Elapsed: [2:57:53.577624]  batch: 400  d_loss: -0.361379  g_loss: 0.633965\n",
      "Elapsed: [2:58:03.084128]  batch: 450  d_loss: 0.015418  g_loss: 0.206441\n",
      "Elapsed: [2:58:11.800987]  batch: 500  d_loss: -0.118953  g_loss: 0.280080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for epoch: 95.049 secs\n",
      "ticker =  14501\n",
      "\n",
      "Epoch: 30\n",
      "Elapsed: [2:58:28.585900]  batch: 1  d_loss: 0.039302  g_loss: 0.161248\n",
      "Elapsed: [2:58:34.490894]  batch: 50  d_loss: 0.130502  g_loss: -0.028580\n",
      "Elapsed: [2:58:40.428106]  batch: 100  d_loss: 0.083567  g_loss: 0.109264\n",
      "Elapsed: [2:58:46.569345]  batch: 150  d_loss: -0.103944  g_loss: 0.441710\n",
      "Elapsed: [2:58:52.742232]  batch: 200  d_loss: -0.241692  g_loss: 0.460842\n",
      "Elapsed: [2:58:58.942905]  batch: 250  d_loss: 0.092440  g_loss: 0.113112\n",
      "Elapsed: [2:59:07.571330]  batch: 300  d_loss: -0.002529  g_loss: 0.255768\n",
      "Elapsed: [2:59:17.265743]  batch: 350  d_loss: -0.285421  g_loss: 0.435296\n",
      "Elapsed: [2:59:26.786276]  batch: 400  d_loss: 0.007598  g_loss: 0.167099\n",
      "Elapsed: [2:59:36.376047]  batch: 450  d_loss: 0.025964  g_loss: 0.360225\n",
      "Elapsed: [2:59:45.697500]  batch: 500  d_loss: -0.314348  g_loss: 0.542071\n",
      "Time taken for epoch: 93.529 secs\n",
      "ticker =  15001\n",
      "\n",
      "Epoch: 31\n",
      "Elapsed: [3:00:01.666991]  batch: 1  d_loss: 0.075962  g_loss: 0.119928\n",
      "Elapsed: [3:00:07.428901]  batch: 50  d_loss: 0.093297  g_loss: 0.305411\n",
      "Elapsed: [3:00:13.410378]  batch: 100  d_loss: -0.173024  g_loss: 0.408193\n",
      "Elapsed: [3:00:19.552016]  batch: 150  d_loss: -0.155121  g_loss: 0.357284\n",
      "Elapsed: [3:00:25.724996]  batch: 200  d_loss: 0.047543  g_loss: 0.095372\n",
      "Elapsed: [3:00:33.953708]  batch: 250  d_loss: -0.113365  g_loss: 0.311177\n",
      "Elapsed: [3:00:43.520616]  batch: 300  d_loss: 0.048368  g_loss: 0.238055\n",
      "Elapsed: [3:00:52.621992]  batch: 350  d_loss: -0.081655  g_loss: 0.376472\n",
      "Elapsed: [3:01:02.306606]  batch: 400  d_loss: 0.143180  g_loss: -0.007102\n",
      "Elapsed: [3:01:11.494019]  batch: 450  d_loss: 0.064227  g_loss: 0.002174\n",
      "Elapsed: [3:01:20.763832]  batch: 500  d_loss: 0.087927  g_loss: -0.018802\n",
      "Time taken for epoch: 94.780 secs\n",
      "ticker =  15501\n",
      "\n",
      "Epoch: 32\n",
      "Elapsed: [3:01:38.442293]  batch: 1  d_loss: 0.087429  g_loss: 0.052618\n",
      "Elapsed: [3:01:44.185898]  batch: 50  d_loss: -0.074007  g_loss: 0.330897\n",
      "Elapsed: [3:01:50.141945]  batch: 100  d_loss: -0.159526  g_loss: 0.442105\n",
      "Elapsed: [3:01:56.445271]  batch: 150  d_loss: 0.125690  g_loss: 0.236843\n",
      "Elapsed: [3:02:02.659473]  batch: 200  d_loss: -0.179599  g_loss: 0.426388\n",
      "Elapsed: [3:02:08.835639]  batch: 250  d_loss: 0.022260  g_loss: 0.175353\n",
      "Elapsed: [3:02:15.147841]  batch: 300  d_loss: -0.195666  g_loss: 0.442669\n",
      "Elapsed: [3:02:21.525890]  batch: 350  d_loss: 0.260906  g_loss: 0.081700\n",
      "Elapsed: [3:02:27.918544]  batch: 400  d_loss: -0.092777  g_loss: 0.416060\n",
      "Elapsed: [3:02:34.327734]  batch: 450  d_loss: 0.150889  g_loss: 0.073386\n",
      "Elapsed: [3:02:40.654713]  batch: 500  d_loss: 0.077058  g_loss: 0.118984\n",
      "Time taken for epoch: 79.665 secs\n",
      "ticker =  16001\n",
      "\n",
      "Epoch: 33\n",
      "Elapsed: [3:02:51.274786]  batch: 1  d_loss: -0.171150  g_loss: 0.376818\n",
      "Elapsed: [3:02:57.454304]  batch: 50  d_loss: -0.205496  g_loss: 0.418298\n",
      "Elapsed: [3:03:03.786016]  batch: 100  d_loss: -0.282685  g_loss: 0.433509\n",
      "Elapsed: [3:03:10.064204]  batch: 150  d_loss: 0.057574  g_loss: 0.058433\n",
      "Elapsed: [3:03:16.308319]  batch: 200  d_loss: -0.128017  g_loss: 0.448815\n",
      "Elapsed: [3:03:22.544506]  batch: 250  d_loss: -0.040925  g_loss: 0.118430\n",
      "Elapsed: [3:03:28.850101]  batch: 300  d_loss: 0.036403  g_loss: 0.122846\n",
      "Elapsed: [3:03:35.294684]  batch: 350  d_loss: -0.270705  g_loss: 0.445253\n",
      "Elapsed: [3:03:41.690746]  batch: 400  d_loss: 0.083384  g_loss: 0.112277\n",
      "Elapsed: [3:03:47.941902]  batch: 450  d_loss: 0.247866  g_loss: 0.228846\n",
      "Elapsed: [3:03:54.174306]  batch: 500  d_loss: -0.076345  g_loss: 0.331777\n",
      "Time taken for epoch: 73.389 secs\n",
      "ticker =  16501\n",
      "\n",
      "Epoch: 34\n",
      "Elapsed: [3:04:04.661886]  batch: 1  d_loss: -0.043860  g_loss: 0.339140\n",
      "Elapsed: [3:04:10.760158]  batch: 50  d_loss: -0.090909  g_loss: 0.287664\n",
      "Elapsed: [3:04:17.364671]  batch: 100  d_loss: 0.124671  g_loss: 0.025585\n",
      "Elapsed: [3:04:25.551199]  batch: 150  d_loss: -0.240413  g_loss: 0.618751\n",
      "Elapsed: [3:04:35.673054]  batch: 200  d_loss: 0.119861  g_loss: 0.308409\n",
      "Elapsed: [3:04:42.686408]  batch: 250  d_loss: 0.143401  g_loss: 0.311756\n",
      "Elapsed: [3:04:49.107578]  batch: 300  d_loss: -0.156576  g_loss: 0.342130\n",
      "Elapsed: [3:04:56.299206]  batch: 350  d_loss: 0.000367  g_loss: 0.253881\n",
      "Elapsed: [3:05:05.072380]  batch: 400  d_loss: -0.021773  g_loss: 0.319464\n",
      "Elapsed: [3:05:12.340963]  batch: 450  d_loss: 0.063966  g_loss: 0.348875\n",
      "Elapsed: [3:05:18.771316]  batch: 500  d_loss: 0.108323  g_loss: 0.179867\n",
      "Time taken for epoch: 84.550 secs\n",
      "ticker =  17001\n",
      "\n",
      "Epoch: 35\n",
      "Elapsed: [3:05:30.675630]  batch: 1  d_loss: -0.076470  g_loss: 0.372581\n",
      "Elapsed: [3:05:37.753875]  batch: 50  d_loss: -0.017474  g_loss: 0.236542\n",
      "Elapsed: [3:05:45.297521]  batch: 100  d_loss: 0.140915  g_loss: 0.199651\n",
      "Elapsed: [3:05:51.745955]  batch: 150  d_loss: 0.092834  g_loss: 0.176673\n",
      "Elapsed: [3:05:58.288168]  batch: 200  d_loss: 0.190245  g_loss: 0.174592\n",
      "Elapsed: [3:06:05.481703]  batch: 250  d_loss: 0.023814  g_loss: 0.252426\n",
      "Elapsed: [3:06:12.635463]  batch: 300  d_loss: 0.090378  g_loss: 0.248391\n",
      "Elapsed: [3:06:20.849005]  batch: 350  d_loss: 0.237310  g_loss: 0.217810\n",
      "Elapsed: [3:06:27.338165]  batch: 400  d_loss: 0.064582  g_loss: 0.364723\n",
      "Elapsed: [3:06:33.806206]  batch: 450  d_loss: 0.061444  g_loss: 0.302056\n",
      "Elapsed: [3:06:40.188750]  batch: 500  d_loss: 0.086440  g_loss: 0.151279\n",
      "Time taken for epoch: 81.270 secs\n",
      "ticker =  17501\n",
      "\n",
      "Epoch: 36\n",
      "Elapsed: [3:06:53.705428]  batch: 1  d_loss: -0.131353  g_loss: 0.375638\n",
      "Elapsed: [3:07:00.027384]  batch: 50  d_loss: 0.062425  g_loss: 0.261344\n",
      "Elapsed: [3:07:06.442251]  batch: 100  d_loss: 0.098813  g_loss: 0.038796\n",
      "Elapsed: [3:07:13.259565]  batch: 150  d_loss: -0.051915  g_loss: 0.283366\n",
      "Elapsed: [3:07:20.168323]  batch: 200  d_loss: -0.052247  g_loss: 0.252112\n",
      "Elapsed: [3:07:26.636030]  batch: 250  d_loss: -0.075051  g_loss: 0.402684\n",
      "Elapsed: [3:07:32.914102]  batch: 300  d_loss: 0.139939  g_loss: 0.020379\n",
      "Elapsed: [3:07:39.204284]  batch: 350  d_loss: 0.103356  g_loss: -0.044229\n",
      "Elapsed: [3:07:45.572803]  batch: 400  d_loss: 0.086500  g_loss: 0.085967\n",
      "Elapsed: [3:07:51.912510]  batch: 450  d_loss: 0.114901  g_loss: 0.386739\n",
      "Elapsed: [3:07:58.204750]  batch: 500  d_loss: 0.153325  g_loss: 0.020401\n",
      "Time taken for epoch: 77.909 secs\n",
      "ticker =  18001\n",
      "\n",
      "Epoch: 37\n",
      "Elapsed: [3:08:08.656844]  batch: 1  d_loss: 0.143208  g_loss: 0.118485\n",
      "Elapsed: [3:08:14.802380]  batch: 50  d_loss: -0.084878  g_loss: 0.316515\n",
      "Elapsed: [3:08:21.198990]  batch: 100  d_loss: -0.103550  g_loss: 0.448659\n",
      "Elapsed: [3:08:27.518919]  batch: 150  d_loss: 0.151024  g_loss: 0.025210\n",
      "Elapsed: [3:08:34.007461]  batch: 200  d_loss: -0.005360  g_loss: 0.430633\n",
      "Elapsed: [3:08:40.544390]  batch: 250  d_loss: -0.105465  g_loss: 0.462010\n",
      "Elapsed: [3:08:47.099809]  batch: 300  d_loss: -0.055845  g_loss: 0.266817\n",
      "Elapsed: [3:08:53.577842]  batch: 350  d_loss: -0.035348  g_loss: 0.244073\n",
      "Elapsed: [3:08:59.917889]  batch: 400  d_loss: 0.022367  g_loss: 0.197824\n",
      "Elapsed: [3:09:06.365218]  batch: 450  d_loss: 0.276831  g_loss: 0.258497\n",
      "Elapsed: [3:09:12.994324]  batch: 500  d_loss: 0.030433  g_loss: 0.193325\n",
      "Time taken for epoch: 74.695 secs\n",
      "ticker =  18501\n",
      "\n",
      "Epoch: 38\n",
      "Elapsed: [3:09:24.184124]  batch: 1  d_loss: -0.087386  g_loss: 0.197683\n",
      "Elapsed: [3:09:30.358279]  batch: 50  d_loss: -0.032367  g_loss: 0.318204\n",
      "Elapsed: [3:09:36.655592]  batch: 100  d_loss: 0.046134  g_loss: 0.209447\n",
      "Elapsed: [3:09:42.936788]  batch: 150  d_loss: -0.007727  g_loss: 0.175338\n",
      "Elapsed: [3:09:49.229749]  batch: 200  d_loss: -0.065490  g_loss: 0.437892\n",
      "Elapsed: [3:09:55.530319]  batch: 250  d_loss: 0.161397  g_loss: 0.188445\n",
      "Elapsed: [3:10:01.826390]  batch: 300  d_loss: -0.010288  g_loss: 0.222578\n",
      "Elapsed: [3:10:08.077631]  batch: 350  d_loss: -0.132587  g_loss: 0.109504\n",
      "Elapsed: [3:10:14.427503]  batch: 400  d_loss: 0.138178  g_loss: -0.073621\n",
      "Elapsed: [3:10:20.990182]  batch: 450  d_loss: 0.031930  g_loss: 0.142564\n",
      "Elapsed: [3:10:28.006180]  batch: 500  d_loss: -0.129041  g_loss: 0.302418\n",
      "Time taken for epoch: 74.881 secs\n",
      "ticker =  19001\n",
      "\n",
      "Epoch: 39\n",
      "Elapsed: [3:10:38.815374]  batch: 1  d_loss: 0.007652  g_loss: 0.195051\n",
      "Elapsed: [3:10:44.984413]  batch: 50  d_loss: -0.031477  g_loss: 0.158816\n",
      "Elapsed: [3:10:51.290379]  batch: 100  d_loss: -0.043574  g_loss: 0.274214\n",
      "Elapsed: [3:10:57.552706]  batch: 150  d_loss: 0.058448  g_loss: 0.063142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: [3:11:03.837125]  batch: 200  d_loss: -0.001651  g_loss: 0.237478\n",
      "Elapsed: [3:11:10.129517]  batch: 250  d_loss: 0.042915  g_loss: -0.112870\n",
      "Elapsed: [3:11:16.413995]  batch: 300  d_loss: 0.023648  g_loss: 0.262946\n",
      "Elapsed: [3:11:22.658298]  batch: 350  d_loss: 0.036001  g_loss: 0.158724\n",
      "Elapsed: [3:11:29.019625]  batch: 400  d_loss: -0.071351  g_loss: 0.193199\n",
      "Elapsed: [3:11:35.856562]  batch: 450  d_loss: -0.132837  g_loss: 0.283294\n",
      "Elapsed: [3:11:42.577981]  batch: 500  d_loss: 0.060405  g_loss: 0.232615\n",
      "Time taken for epoch: 74.452 secs\n",
      "ticker =  19501\n",
      "\n",
      "Epoch: 40\n",
      "Elapsed: [3:12:00.453447]  batch: 1  d_loss: -0.003683  g_loss: 0.237098\n",
      "Elapsed: [3:12:10.374297]  batch: 50  d_loss: -0.045290  g_loss: 0.266965\n",
      "Elapsed: [3:12:20.185205]  batch: 100  d_loss: -0.076063  g_loss: 0.353959\n",
      "Elapsed: [3:12:29.594156]  batch: 150  d_loss: 0.063747  g_loss: 0.195978\n",
      "Elapsed: [3:12:39.605568]  batch: 200  d_loss: 0.069358  g_loss: -0.020398\n",
      "Elapsed: [3:12:49.448553]  batch: 250  d_loss: -0.081256  g_loss: 0.326456\n",
      "Elapsed: [3:12:59.719121]  batch: 300  d_loss: 0.208480  g_loss: 0.052451\n",
      "Elapsed: [3:13:09.329948]  batch: 350  d_loss: -0.106123  g_loss: 0.201272\n",
      "Elapsed: [3:13:19.155850]  batch: 400  d_loss: 0.016014  g_loss: 0.184228\n",
      "Elapsed: [3:13:30.147053]  batch: 450  d_loss: 0.170050  g_loss: -0.023297\n",
      "Elapsed: [3:13:40.647280]  batch: 500  d_loss: -0.088675  g_loss: 0.350048\n",
      "Time taken for epoch: 118.412 secs\n",
      "ticker =  20001\n",
      "\n",
      "Epoch: 41\n",
      "Elapsed: [3:13:58.732566]  batch: 1  d_loss: 0.018468  g_loss: 0.129031\n",
      "Elapsed: [3:14:09.315086]  batch: 50  d_loss: 0.179576  g_loss: 0.020332\n",
      "Elapsed: [3:14:19.008480]  batch: 100  d_loss: -0.013175  g_loss: 0.204450\n",
      "Elapsed: [3:14:28.738367]  batch: 150  d_loss: 0.229497  g_loss: -0.039074\n",
      "Elapsed: [3:14:38.660316]  batch: 200  d_loss: -0.024429  g_loss: 0.131129\n",
      "Elapsed: [3:14:48.002110]  batch: 250  d_loss: 0.139687  g_loss: 0.145548\n",
      "Elapsed: [3:14:57.823436]  batch: 300  d_loss: 0.054545  g_loss: -0.009224\n",
      "Elapsed: [3:15:07.315413]  batch: 350  d_loss: 0.031360  g_loss: 0.103345\n",
      "Elapsed: [3:15:16.990756]  batch: 400  d_loss: 0.085856  g_loss: 0.219916\n",
      "Elapsed: [3:15:26.967892]  batch: 450  d_loss: 0.097680  g_loss: 0.049555\n",
      "Elapsed: [3:15:36.538285]  batch: 500  d_loss: 0.008033  g_loss: 0.242597\n",
      "Time taken for epoch: 115.381 secs\n",
      "ticker =  20501\n",
      "\n",
      "Epoch: 42\n",
      "Elapsed: [3:15:53.606543]  batch: 1  d_loss: -0.013580  g_loss: 0.141816\n",
      "Elapsed: [3:16:02.697975]  batch: 50  d_loss: 0.113938  g_loss: 0.249566\n",
      "Elapsed: [3:16:12.193607]  batch: 100  d_loss: -0.019516  g_loss: 0.163208\n",
      "Elapsed: [3:16:21.467905]  batch: 150  d_loss: -0.007847  g_loss: 0.281161\n",
      "Elapsed: [3:16:33.116343]  batch: 200  d_loss: -0.011718  g_loss: 0.196761\n",
      "Elapsed: [3:16:42.912043]  batch: 250  d_loss: -0.025678  g_loss: 0.311498\n",
      "Elapsed: [3:16:52.309357]  batch: 300  d_loss: 0.023148  g_loss: 0.071898\n",
      "Elapsed: [3:17:02.199385]  batch: 350  d_loss: 0.120079  g_loss: 0.048936\n",
      "Elapsed: [3:17:12.963238]  batch: 400  d_loss: 0.041951  g_loss: 0.146032\n",
      "Elapsed: [3:17:22.257107]  batch: 450  d_loss: 0.037799  g_loss: 0.135001\n",
      "Elapsed: [3:17:32.404512]  batch: 500  d_loss: -0.116463  g_loss: 0.381430\n",
      "Time taken for epoch: 115.541 secs\n",
      "ticker =  21001\n",
      "\n",
      "Epoch: 43\n",
      "Elapsed: [3:17:49.109601]  batch: 1  d_loss: 0.080106  g_loss: 0.043154\n",
      "Elapsed: [3:17:56.991710]  batch: 50  d_loss: 0.025406  g_loss: -0.007351\n",
      "Elapsed: [3:18:03.037118]  batch: 100  d_loss: -0.045367  g_loss: 0.359502\n",
      "Elapsed: [3:18:09.574609]  batch: 150  d_loss: 0.021316  g_loss: 0.162899\n",
      "Elapsed: [3:18:19.577217]  batch: 200  d_loss: 0.067648  g_loss: 0.198085\n",
      "Elapsed: [3:18:29.209030]  batch: 250  d_loss: -0.005789  g_loss: 0.238924\n",
      "Elapsed: [3:18:38.971554]  batch: 300  d_loss: 0.014766  g_loss: 0.204634\n",
      "Elapsed: [3:18:48.246904]  batch: 350  d_loss: -0.013751  g_loss: 0.338787\n",
      "Elapsed: [3:18:57.859613]  batch: 400  d_loss: -0.104821  g_loss: 0.282973\n",
      "Elapsed: [3:19:07.391070]  batch: 450  d_loss: 0.025460  g_loss: 0.161075\n",
      "Elapsed: [3:19:16.989835]  batch: 500  d_loss: 0.089692  g_loss: 0.152944\n",
      "Time taken for epoch: 104.412 secs\n",
      "ticker =  21501\n",
      "\n",
      "Epoch: 44\n",
      "Elapsed: [3:19:34.709820]  batch: 1  d_loss: -0.018279  g_loss: 0.212957\n",
      "Elapsed: [3:19:40.947231]  batch: 50  d_loss: -0.036494  g_loss: 0.103041\n",
      "Elapsed: [3:19:46.985501]  batch: 100  d_loss: -0.056860  g_loss: 0.038840\n",
      "Elapsed: [3:19:53.229741]  batch: 150  d_loss: -0.003386  g_loss: 0.195741\n",
      "Elapsed: [3:19:59.573258]  batch: 200  d_loss: -0.016878  g_loss: 0.176596\n",
      "Elapsed: [3:20:09.609645]  batch: 250  d_loss: 0.020458  g_loss: 0.106430\n",
      "Elapsed: [3:20:19.638754]  batch: 300  d_loss: 0.318645  g_loss: 0.063786\n",
      "Elapsed: [3:20:29.099878]  batch: 350  d_loss: 0.071435  g_loss: 0.300507\n",
      "Elapsed: [3:20:38.434512]  batch: 400  d_loss: 0.067646  g_loss: 0.027722\n",
      "Elapsed: [3:20:48.975037]  batch: 450  d_loss: -0.037556  g_loss: 0.282066\n",
      "Elapsed: [3:20:58.606578]  batch: 500  d_loss: -0.083133  g_loss: 0.146570\n",
      "Time taken for epoch: 101.480 secs\n",
      "ticker =  22001\n",
      "\n",
      "Epoch: 45\n",
      "Elapsed: [3:21:16.134544]  batch: 1  d_loss: 0.079848  g_loss: 0.021305\n",
      "Elapsed: [3:21:22.606810]  batch: 50  d_loss: 0.050563  g_loss: 0.112848\n",
      "Elapsed: [3:21:28.661226]  batch: 100  d_loss: 0.145532  g_loss: 0.230419\n",
      "Elapsed: [3:21:34.836665]  batch: 150  d_loss: -0.044400  g_loss: 0.292329\n",
      "Elapsed: [3:21:41.078295]  batch: 200  d_loss: -0.049302  g_loss: 0.193256\n",
      "Elapsed: [3:21:49.754677]  batch: 250  d_loss: -0.026235  g_loss: 0.194577\n",
      "Elapsed: [3:22:02.159659]  batch: 300  d_loss: 0.123065  g_loss: 0.033418\n",
      "Elapsed: [3:22:12.044461]  batch: 350  d_loss: 0.036178  g_loss: 0.105824\n",
      "Elapsed: [3:22:22.172894]  batch: 400  d_loss: 0.093303  g_loss: -0.024904\n",
      "Elapsed: [3:22:32.420598]  batch: 450  d_loss: 0.194546  g_loss: -0.000731\n",
      "Elapsed: [3:22:41.827009]  batch: 500  d_loss: 0.099874  g_loss: -0.066255\n",
      "Time taken for epoch: 103.115 secs\n",
      "ticker =  22501\n",
      "\n",
      "Epoch: 46\n",
      "Elapsed: [3:22:58.580568]  batch: 1  d_loss: 0.025200  g_loss: -0.086980\n",
      "Elapsed: [3:23:04.445524]  batch: 50  d_loss: -0.077805  g_loss: 0.164267\n",
      "Elapsed: [3:23:10.445591]  batch: 100  d_loss: -0.001574  g_loss: 0.065715\n",
      "Elapsed: [3:23:16.660806]  batch: 150  d_loss: 0.016101  g_loss: 0.305706\n",
      "Elapsed: [3:23:22.873224]  batch: 200  d_loss: -0.007741  g_loss: 0.092378\n",
      "Elapsed: [3:23:29.767618]  batch: 250  d_loss: -0.045626  g_loss: 0.395919\n",
      "Elapsed: [3:23:40.009115]  batch: 300  d_loss: 0.002391  g_loss: 0.125092\n",
      "Elapsed: [3:23:49.918033]  batch: 350  d_loss: 0.084801  g_loss: 0.168426\n",
      "Elapsed: [3:23:59.596890]  batch: 400  d_loss: 0.068670  g_loss: 0.242771\n",
      "Elapsed: [3:24:09.473936]  batch: 450  d_loss: -0.019731  g_loss: 0.299740\n",
      "Elapsed: [3:24:19.097300]  batch: 500  d_loss: -0.065030  g_loss: 0.129531\n",
      "Time taken for epoch: 97.035 secs\n",
      "ticker =  23001\n",
      "\n",
      "Epoch: 47\n",
      "Elapsed: [3:24:38.548116]  batch: 1  d_loss: 0.047066  g_loss: -0.006743\n",
      "Elapsed: [3:24:44.226906]  batch: 50  d_loss: -0.043998  g_loss: 0.127140\n",
      "Elapsed: [3:24:50.359836]  batch: 100  d_loss: -0.016767  g_loss: 0.124485\n",
      "Elapsed: [3:24:56.646864]  batch: 150  d_loss: -0.123217  g_loss: 0.247232\n",
      "Elapsed: [3:25:02.908944]  batch: 200  d_loss: 0.089207  g_loss: 0.012540\n",
      "Elapsed: [3:25:09.217292]  batch: 250  d_loss: 0.023823  g_loss: 0.114072\n",
      "Elapsed: [3:25:15.903507]  batch: 300  d_loss: -0.038666  g_loss: 0.139026\n",
      "Elapsed: [3:25:25.955483]  batch: 350  d_loss: 0.003062  g_loss: 0.109013\n",
      "Elapsed: [3:25:36.141420]  batch: 400  d_loss: -0.038715  g_loss: 0.008218\n",
      "Elapsed: [3:25:44.647547]  batch: 450  d_loss: 0.024152  g_loss: 0.048513\n",
      "Elapsed: [3:25:55.386063]  batch: 500  d_loss: 0.005288  g_loss: 0.290130\n",
      "Time taken for epoch: 96.337 secs\n",
      "ticker =  23501\n",
      "\n",
      "Epoch: 48\n",
      "Elapsed: [3:26:17.269544]  batch: 1  d_loss: 0.024609  g_loss: 0.262528\n",
      "Elapsed: [3:26:26.836371]  batch: 50  d_loss: 0.030204  g_loss: 0.116906\n",
      "Elapsed: [3:26:36.153913]  batch: 100  d_loss: -0.030829  g_loss: 0.239426\n",
      "Elapsed: [3:26:45.168216]  batch: 150  d_loss: -0.012926  g_loss: 0.227015\n",
      "Elapsed: [3:26:54.742221]  batch: 200  d_loss: 0.078101  g_loss: 0.132306\n",
      "Elapsed: [3:27:01.473473]  batch: 250  d_loss: -0.111289  g_loss: 0.152311\n",
      "Elapsed: [3:27:09.026736]  batch: 300  d_loss: -0.009575  g_loss: 0.229004\n",
      "Elapsed: [3:27:16.670023]  batch: 350  d_loss: -0.008885  g_loss: 0.124812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: [3:27:24.606960]  batch: 400  d_loss: 0.028637  g_loss: 0.121694\n",
      "Elapsed: [3:27:32.613415]  batch: 450  d_loss: 0.057440  g_loss: 0.229334\n",
      "Elapsed: [3:27:40.564084]  batch: 500  d_loss: 0.042644  g_loss: 0.052176\n",
      "Time taken for epoch: 104.571 secs\n",
      "ticker =  24001\n",
      "\n",
      "Epoch: 49\n",
      "Elapsed: [3:27:52.338242]  batch: 1  d_loss: -0.082120  g_loss: 0.137600\n",
      "Elapsed: [3:27:59.957325]  batch: 50  d_loss: 0.028263  g_loss: 0.146992\n",
      "Elapsed: [3:28:07.614737]  batch: 100  d_loss: -0.045093  g_loss: 0.221223\n",
      "Elapsed: [3:28:15.229095]  batch: 150  d_loss: -0.024831  g_loss: 0.138624\n",
      "Elapsed: [3:28:22.562529]  batch: 200  d_loss: -0.044812  g_loss: 0.230259\n",
      "Elapsed: [3:28:29.989450]  batch: 250  d_loss: 0.023875  g_loss: 0.114699\n",
      "Elapsed: [3:28:37.396420]  batch: 300  d_loss: -0.044283  g_loss: 0.070147\n",
      "Elapsed: [3:28:44.568601]  batch: 350  d_loss: 0.058134  g_loss: 0.064904\n",
      "Elapsed: [3:28:52.816278]  batch: 400  d_loss: -0.050016  g_loss: 0.211139\n",
      "Elapsed: [3:29:01.250485]  batch: 450  d_loss: 0.064952  g_loss: -0.122259\n",
      "Elapsed: [3:29:08.550144]  batch: 500  d_loss: -0.044829  g_loss: 0.210203\n",
      "Time taken for epoch: 87.841 secs\n",
      "ticker =  24501\n",
      "\n",
      "Epoch: 50\n",
      "Elapsed: [3:29:24.326514]  batch: 1  d_loss: 0.000321  g_loss: 0.141620\n",
      "Elapsed: [3:29:32.626310]  batch: 50  d_loss: 0.030765  g_loss: -0.059717\n",
      "Elapsed: [3:29:41.275062]  batch: 100  d_loss: 0.085922  g_loss: 0.095413\n",
      "Elapsed: [3:29:55.377966]  batch: 150  d_loss: -0.040462  g_loss: 0.124363\n",
      "Elapsed: [3:30:02.265241]  batch: 200  d_loss: 0.010688  g_loss: 0.136517\n",
      "Elapsed: [3:30:09.118612]  batch: 250  d_loss: 0.018661  g_loss: 0.079569\n",
      "Elapsed: [3:30:15.990718]  batch: 300  d_loss: -0.031229  g_loss: 0.185808\n",
      "Elapsed: [3:30:22.873151]  batch: 350  d_loss: -0.083018  g_loss: 0.218426\n",
      "Elapsed: [3:30:29.771053]  batch: 400  d_loss: -0.062538  g_loss: 0.309920\n",
      "Elapsed: [3:30:36.622632]  batch: 450  d_loss: -0.051788  g_loss: 0.115429\n",
      "Elapsed: [3:30:46.980386]  batch: 500  d_loss: 0.038224  g_loss: 0.076717\n",
      "Time taken for epoch: 98.373 secs\n",
      "ticker =  25001\n",
      "\n",
      "Epoch: 51\n",
      "Elapsed: [3:30:59.775046]  batch: 1  d_loss: 0.186090  g_loss: -0.047694\n",
      "Elapsed: [3:31:21.623500]  batch: 50  d_loss: -0.028038  g_loss: 0.169112\n",
      "Elapsed: [3:31:51.246055]  batch: 100  d_loss: 0.034861  g_loss: 0.094210\n",
      "Elapsed: [3:32:25.732472]  batch: 150  d_loss: 0.024790  g_loss: 0.045706\n",
      "Elapsed: [3:33:08.999547]  batch: 200  d_loss: -0.072834  g_loss: 0.224595\n",
      "Elapsed: [3:33:50.940585]  batch: 250  d_loss: 0.047745  g_loss: 0.079385\n",
      "Elapsed: [3:34:31.004920]  batch: 300  d_loss: 0.012599  g_loss: 0.196252\n",
      "Elapsed: [3:35:05.500474]  batch: 350  d_loss: -0.000677  g_loss: 0.181935\n",
      "Elapsed: [3:35:31.647712]  batch: 400  d_loss: -0.072613  g_loss: 0.158877\n",
      "Elapsed: [3:36:01.968376]  batch: 450  d_loss: -0.084584  g_loss: 0.282273\n",
      "Elapsed: [3:36:30.595747]  batch: 500  d_loss: -0.027552  g_loss: -0.003013\n",
      "Time taken for epoch: 344.237 secs\n",
      "ticker =  25501\n",
      "\n",
      "Epoch: 52\n",
      "Elapsed: [3:37:16.422771]  batch: 1  d_loss: -0.005347  g_loss: 0.189736\n",
      "Elapsed: [3:37:49.988836]  batch: 50  d_loss: -0.065547  g_loss: 0.261598\n",
      "Elapsed: [3:38:25.200763]  batch: 100  d_loss: -0.082497  g_loss: 0.169314\n",
      "Elapsed: [3:39:01.300664]  batch: 150  d_loss: 0.031666  g_loss: 0.094070\n",
      "Elapsed: [3:39:33.127601]  batch: 200  d_loss: 0.313601  g_loss: 0.149848\n",
      "Elapsed: [3:40:08.005356]  batch: 250  d_loss: 0.200993  g_loss: 0.100846\n",
      "Elapsed: [3:40:39.042692]  batch: 300  d_loss: 0.000461  g_loss: 0.264215\n",
      "Elapsed: [3:41:14.538839]  batch: 350  d_loss: -0.018985  g_loss: 0.139921\n",
      "Elapsed: [3:41:46.634188]  batch: 400  d_loss: -0.008138  g_loss: 0.122724\n",
      "Elapsed: [3:42:19.648849]  batch: 450  d_loss: -0.074086  g_loss: 0.226338\n",
      "Elapsed: [3:42:54.433511]  batch: 500  d_loss: -0.012710  g_loss: 0.131027\n",
      "Time taken for epoch: 383.470 secs\n",
      "ticker =  26001\n",
      "\n",
      "Epoch: 53\n",
      "Elapsed: [3:43:30.792105]  batch: 1  d_loss: -0.051234  g_loss: 0.128935\n",
      "Elapsed: [3:44:05.314209]  batch: 50  d_loss: -0.010502  g_loss: 0.179477\n",
      "Elapsed: [3:44:43.628425]  batch: 100  d_loss: 0.004127  g_loss: 0.163785\n",
      "Elapsed: [3:45:26.658794]  batch: 150  d_loss: -0.007438  g_loss: 0.249505\n",
      "Elapsed: [3:46:08.851963]  batch: 200  d_loss: 0.072926  g_loss: 0.061797\n",
      "Elapsed: [3:46:42.624371]  batch: 250  d_loss: 0.087058  g_loss: 0.108986\n",
      "Elapsed: [3:47:17.749879]  batch: 300  d_loss: -0.061434  g_loss: 0.274799\n",
      "Elapsed: [3:47:54.131335]  batch: 350  d_loss: -0.143907  g_loss: 0.359750\n",
      "Elapsed: [3:48:34.579875]  batch: 400  d_loss: 0.008625  g_loss: 0.091079\n",
      "Elapsed: [3:49:14.688812]  batch: 450  d_loss: -0.015272  g_loss: 0.156232\n",
      "Elapsed: [3:49:49.332438]  batch: 500  d_loss: 0.084867  g_loss: 0.118883\n",
      "Time taken for epoch: 414.204 secs\n",
      "ticker =  26501\n",
      "\n",
      "Epoch: 54\n",
      "Elapsed: [3:50:29.755757]  batch: 1  d_loss: 0.121261  g_loss: 0.125957\n",
      "Elapsed: [3:51:05.413456]  batch: 50  d_loss: -0.022755  g_loss: 0.104674\n",
      "Elapsed: [3:51:44.990234]  batch: 100  d_loss: -0.075961  g_loss: 0.230142\n",
      "Elapsed: [3:52:12.638782]  batch: 150  d_loss: 0.016072  g_loss: 0.086215\n",
      "Elapsed: [3:52:45.159565]  batch: 200  d_loss: -0.002878  g_loss: 0.100856\n",
      "Elapsed: [3:53:15.527515]  batch: 250  d_loss: 0.054355  g_loss: 0.069753\n",
      "Elapsed: [3:53:52.769041]  batch: 300  d_loss: -0.053123  g_loss: 0.197273\n",
      "Elapsed: [3:54:31.450284]  batch: 350  d_loss: 0.052199  g_loss: 0.171820\n",
      "Elapsed: [3:55:16.096915]  batch: 400  d_loss: 0.023367  g_loss: 0.026655\n",
      "Elapsed: [3:55:53.545102]  batch: 450  d_loss: 0.026319  g_loss: 0.076169\n",
      "Elapsed: [3:56:30.820856]  batch: 500  d_loss: -0.087857  g_loss: 0.185516\n",
      "Time taken for epoch: 401.312 secs\n",
      "ticker =  27001\n",
      "\n",
      "Epoch: 55\n",
      "Elapsed: [3:57:20.849393]  batch: 1  d_loss: 0.030041  g_loss: 0.106193\n",
      "Elapsed: [3:58:00.118166]  batch: 50  d_loss: 0.074347  g_loss: 0.146413\n",
      "Elapsed: [3:58:36.742624]  batch: 100  d_loss: -0.036028  g_loss: 0.190227\n",
      "Elapsed: [3:59:17.690891]  batch: 150  d_loss: 0.039015  g_loss: 0.080421\n",
      "Elapsed: [3:59:59.759139]  batch: 200  d_loss: 0.029926  g_loss: 0.042523\n",
      "Elapsed: [4:00:40.462073]  batch: 250  d_loss: 0.100326  g_loss: 0.072847\n",
      "Elapsed: [4:01:18.931185]  batch: 300  d_loss: 0.054313  g_loss: 0.018597\n",
      "Elapsed: [4:01:59.174496]  batch: 350  d_loss: 0.027879  g_loss: 0.052921\n",
      "Elapsed: [4:02:33.661766]  batch: 400  d_loss: -0.009857  g_loss: 0.126557\n",
      "Elapsed: [4:03:08.360098]  batch: 450  d_loss: 0.015032  g_loss: 0.122838\n",
      "Elapsed: [4:03:49.541736]  batch: 500  d_loss: -0.072362  g_loss: 0.220058\n",
      "Time taken for epoch: 438.229 secs\n",
      "ticker =  27501\n",
      "\n",
      "Epoch: 56\n",
      "Elapsed: [4:04:34.879727]  batch: 1  d_loss: 0.045359  g_loss: -0.004784\n",
      "Elapsed: [4:05:14.133536]  batch: 50  d_loss: -0.062424  g_loss: 0.275101\n",
      "Elapsed: [4:05:48.746394]  batch: 100  d_loss: -0.001532  g_loss: 0.059939\n",
      "Elapsed: [4:06:20.053998]  batch: 150  d_loss: 0.071500  g_loss: 0.022468\n",
      "Elapsed: [4:06:57.243172]  batch: 200  d_loss: -0.044648  g_loss: 0.169235\n",
      "Elapsed: [4:07:37.189615]  batch: 250  d_loss: 0.317122  g_loss: 0.107042\n",
      "Elapsed: [4:08:13.463964]  batch: 300  d_loss: -0.000515  g_loss: 0.065174\n",
      "Elapsed: [4:08:48.449604]  batch: 350  d_loss: 0.016537  g_loss: 0.107341\n",
      "Elapsed: [4:09:26.768427]  batch: 400  d_loss: 0.002200  g_loss: 0.171045\n",
      "Elapsed: [4:09:57.395158]  batch: 450  d_loss: -0.055738  g_loss: 0.239734\n",
      "Elapsed: [4:10:26.582335]  batch: 500  d_loss: 0.021423  g_loss: 0.070478\n",
      "Time taken for epoch: 396.328 secs\n",
      "ticker =  28001\n",
      "\n",
      "Epoch: 57\n",
      "Elapsed: [4:11:10.212805]  batch: 1  d_loss: -0.059277  g_loss: 0.150198\n",
      "Elapsed: [4:11:42.671432]  batch: 50  d_loss: -0.070483  g_loss: 0.176614\n",
      "Elapsed: [4:12:17.742867]  batch: 100  d_loss: -0.085464  g_loss: 0.206996\n",
      "Elapsed: [4:12:54.044229]  batch: 150  d_loss: -0.054684  g_loss: 0.204613\n",
      "Elapsed: [4:13:28.813426]  batch: 200  d_loss: -0.057135  g_loss: 0.090810\n",
      "Elapsed: [4:14:01.913931]  batch: 250  d_loss: 0.033722  g_loss: 0.111834\n",
      "Elapsed: [4:14:38.817403]  batch: 300  d_loss: -0.021340  g_loss: 0.063456\n",
      "Elapsed: [4:15:10.763564]  batch: 350  d_loss: -0.051889  g_loss: 0.102675\n",
      "Elapsed: [4:15:41.057806]  batch: 400  d_loss: -0.050639  g_loss: 0.130428\n",
      "Elapsed: [4:16:15.737246]  batch: 450  d_loss: -0.063859  g_loss: 0.092828\n",
      "Elapsed: [4:16:51.066487]  batch: 500  d_loss: 0.002075  g_loss: 0.119439\n",
      "Time taken for epoch: 384.259 secs\n",
      "ticker =  28501\n",
      "\n",
      "Epoch: 58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: [4:17:35.802039]  batch: 1  d_loss: -0.070774  g_loss: 0.206004\n",
      "Elapsed: [4:18:12.598577]  batch: 50  d_loss: -0.038757  g_loss: 0.146457\n",
      "Elapsed: [4:18:41.269844]  batch: 100  d_loss: 0.009416  g_loss: 0.080396\n",
      "Elapsed: [4:19:18.811627]  batch: 150  d_loss: -0.109678  g_loss: 0.236846\n",
      "Elapsed: [4:19:57.655012]  batch: 200  d_loss: 0.000672  g_loss: 0.102820\n",
      "Elapsed: [4:20:31.761803]  batch: 250  d_loss: -0.070043  g_loss: 0.288541\n",
      "Elapsed: [4:21:07.879879]  batch: 300  d_loss: -0.032340  g_loss: 0.057097\n",
      "Elapsed: [4:21:41.816516]  batch: 350  d_loss: -0.048632  g_loss: 0.239187\n",
      "Elapsed: [4:22:24.436070]  batch: 400  d_loss: 0.044922  g_loss: 0.084479\n",
      "Elapsed: [4:22:54.158009]  batch: 450  d_loss: -0.001999  g_loss: 0.076212\n",
      "Elapsed: [4:23:30.582270]  batch: 500  d_loss: -0.026412  g_loss: 0.179533\n",
      "Time taken for epoch: 399.139 secs\n",
      "ticker =  29001\n",
      "\n",
      "Epoch: 59\n",
      "Elapsed: [4:24:16.392916]  batch: 1  d_loss: -0.050510  g_loss: 0.191955\n",
      "Elapsed: [4:24:45.107398]  batch: 50  d_loss: 0.016620  g_loss: 0.165294\n",
      "Elapsed: [4:25:09.738357]  batch: 100  d_loss: -0.030460  g_loss: 0.176530\n",
      "Elapsed: [4:25:42.544469]  batch: 150  d_loss: -0.036974  g_loss: 0.107203\n",
      "Elapsed: [4:26:16.872062]  batch: 200  d_loss: -0.042145  g_loss: 0.203478\n",
      "Elapsed: [4:26:53.675632]  batch: 250  d_loss: -0.016937  g_loss: 0.141937\n",
      "Elapsed: [4:27:21.885266]  batch: 300  d_loss: -0.021989  g_loss: 0.090292\n",
      "Elapsed: [4:27:57.738919]  batch: 350  d_loss: 0.021521  g_loss: 0.173178\n",
      "Elapsed: [4:28:29.142651]  batch: 400  d_loss: -0.019763  g_loss: 0.077135\n",
      "Elapsed: [4:29:02.037122]  batch: 450  d_loss: -0.037977  g_loss: 0.130763\n",
      "Elapsed: [4:29:28.649848]  batch: 500  d_loss: 0.003544  g_loss: 0.153567\n",
      "Time taken for epoch: 357.500 secs\n",
      "ticker =  29501\n",
      "\n",
      "Epoch: 60\n",
      "Elapsed: [4:30:07.852602]  batch: 1  d_loss: -0.037514  g_loss: 0.159876\n",
      "Elapsed: [4:30:24.643918]  batch: 50  d_loss: -0.017495  g_loss: 0.120865\n",
      "Elapsed: [4:30:43.820841]  batch: 100  d_loss: -0.064148  g_loss: 0.218194\n",
      "Elapsed: [4:31:01.794642]  batch: 150  d_loss: -0.039104  g_loss: 0.217234\n",
      "Elapsed: [4:31:19.225391]  batch: 200  d_loss: -0.044585  g_loss: 0.128229\n",
      "Elapsed: [4:31:28.693993]  batch: 250  d_loss: -0.023643  g_loss: 0.036966\n",
      "Elapsed: [4:31:38.224158]  batch: 300  d_loss: -0.017290  g_loss: 0.128319\n",
      "Elapsed: [4:31:47.960079]  batch: 350  d_loss: -0.091598  g_loss: 0.163216\n",
      "Elapsed: [4:31:58.405690]  batch: 400  d_loss: -0.039570  g_loss: 0.146596\n",
      "Elapsed: [4:32:08.676827]  batch: 450  d_loss: 0.022284  g_loss: 0.207219\n",
      "Elapsed: [4:32:18.703017]  batch: 500  d_loss: -0.033296  g_loss: 0.129002\n",
      "Time taken for epoch: 169.128 secs\n",
      "ticker =  30001\n",
      "\n",
      "Epoch: 61\n",
      "Elapsed: [4:32:36.067130]  batch: 1  d_loss: -0.022984  g_loss: 0.151607\n",
      "Elapsed: [4:32:45.359318]  batch: 50  d_loss: -0.014554  g_loss: 0.177404\n",
      "Elapsed: [4:32:56.428762]  batch: 100  d_loss: 0.009961  g_loss: 0.104573\n",
      "Elapsed: [4:33:06.417719]  batch: 150  d_loss: -0.062058  g_loss: 0.160302\n",
      "Elapsed: [4:33:15.796844]  batch: 200  d_loss: -0.056855  g_loss: 0.120241\n",
      "Elapsed: [4:33:27.967725]  batch: 250  d_loss: 0.061561  g_loss: -0.013163\n",
      "Elapsed: [4:33:38.555243]  batch: 300  d_loss: -0.012405  g_loss: 0.111347\n",
      "Elapsed: [4:33:48.624840]  batch: 350  d_loss: -0.036352  g_loss: 0.134057\n",
      "Elapsed: [4:33:58.100727]  batch: 400  d_loss: -0.017199  g_loss: 0.067637\n",
      "Elapsed: [4:34:08.208169]  batch: 450  d_loss: 0.003000  g_loss: 0.141694\n",
      "Elapsed: [4:34:22.904725]  batch: 500  d_loss: -0.003249  g_loss: 0.076767\n",
      "Time taken for epoch: 124.458 secs\n",
      "ticker =  30501\n",
      "\n",
      "Epoch: 62\n",
      "Elapsed: [4:34:46.874313]  batch: 1  d_loss: -0.045067  g_loss: 0.090142\n",
      "Elapsed: [4:34:52.955163]  batch: 50  d_loss: -0.030442  g_loss: 0.174669\n",
      "Elapsed: [4:34:58.769720]  batch: 100  d_loss: -0.016438  g_loss: 0.136745\n",
      "Elapsed: [4:35:04.894169]  batch: 150  d_loss: 0.008786  g_loss: 0.180092\n",
      "Elapsed: [4:35:11.773782]  batch: 200  d_loss: 0.019712  g_loss: -0.049012\n",
      "Elapsed: [4:35:24.384421]  batch: 250  d_loss: -0.031856  g_loss: 0.182353\n",
      "Elapsed: [4:35:34.484376]  batch: 300  d_loss: -0.024840  g_loss: 0.109430\n",
      "Elapsed: [4:35:44.245943]  batch: 350  d_loss: 0.002249  g_loss: 0.144203\n",
      "Elapsed: [4:35:54.365796]  batch: 400  d_loss: -0.018426  g_loss: 0.105859\n",
      "Elapsed: [4:36:04.833989]  batch: 450  d_loss: -0.011548  g_loss: -0.005652\n",
      "Elapsed: [4:36:14.712646]  batch: 500  d_loss: 0.039426  g_loss: 0.247745\n",
      "Time taken for epoch: 110.810 secs\n",
      "ticker =  31001\n",
      "\n",
      "Epoch: 63\n",
      "Elapsed: [4:36:32.044382]  batch: 1  d_loss: 0.003549  g_loss: 0.274086\n",
      "Elapsed: [4:36:39.211621]  batch: 50  d_loss: -0.075942  g_loss: 0.118572\n",
      "Elapsed: [4:36:45.191319]  batch: 100  d_loss: -0.030396  g_loss: 0.106000\n",
      "Elapsed: [4:36:52.073125]  batch: 150  d_loss: 0.129888  g_loss: 0.031094\n",
      "Elapsed: [4:37:03.997346]  batch: 200  d_loss: 0.020442  g_loss: 0.023729\n",
      "Elapsed: [4:37:13.249341]  batch: 250  d_loss: 0.004687  g_loss: 0.184814\n",
      "Elapsed: [4:37:23.213272]  batch: 300  d_loss: 0.023331  g_loss: 0.097644\n",
      "Elapsed: [4:37:32.902783]  batch: 350  d_loss: -0.019849  g_loss: 0.165836\n",
      "Elapsed: [4:37:42.532505]  batch: 400  d_loss: -0.028033  g_loss: 0.125350\n",
      "Elapsed: [4:37:52.420055]  batch: 450  d_loss: 0.023459  g_loss: 0.089520\n",
      "Elapsed: [4:38:02.048856]  batch: 500  d_loss: 0.024389  g_loss: 0.127410\n",
      "Time taken for epoch: 107.220 secs\n",
      "ticker =  31501\n",
      "\n",
      "Epoch: 64\n",
      "Elapsed: [4:38:21.350620]  batch: 1  d_loss: -0.000638  g_loss: 0.092879\n",
      "Elapsed: [4:38:27.907493]  batch: 50  d_loss: 0.041076  g_loss: 0.082493\n",
      "Elapsed: [4:38:34.034305]  batch: 100  d_loss: 0.001330  g_loss: 0.115780\n",
      "Elapsed: [4:38:40.343185]  batch: 150  d_loss: 0.037482  g_loss: -0.003475\n",
      "Elapsed: [4:38:46.618623]  batch: 200  d_loss: 0.016239  g_loss: 0.087055\n",
      "Elapsed: [4:38:54.382621]  batch: 250  d_loss: 0.086833  g_loss: 0.026604\n",
      "Elapsed: [4:39:04.540774]  batch: 300  d_loss: 0.042024  g_loss: 0.135088\n",
      "Elapsed: [4:39:16.032660]  batch: 350  d_loss: -0.021108  g_loss: 0.090332\n",
      "Elapsed: [4:39:25.201782]  batch: 400  d_loss: -0.049704  g_loss: 0.177199\n",
      "Elapsed: [4:39:35.161421]  batch: 450  d_loss: -0.056909  g_loss: 0.149602\n",
      "Elapsed: [4:39:44.719278]  batch: 500  d_loss: -0.031834  g_loss: 0.099047\n",
      "Time taken for epoch: 102.492 secs\n",
      "ticker =  32001\n",
      "\n",
      "Epoch: 65\n",
      "Elapsed: [4:40:01.405771]  batch: 1  d_loss: -0.054569  g_loss: 0.157209\n",
      "Elapsed: [4:40:08.000164]  batch: 50  d_loss: 0.000336  g_loss: 0.199240\n",
      "Elapsed: [4:40:13.960885]  batch: 100  d_loss: 0.028589  g_loss: 0.088328\n",
      "Elapsed: [4:40:20.162235]  batch: 150  d_loss: -0.033856  g_loss: 0.108304\n",
      "Elapsed: [4:40:26.492310]  batch: 200  d_loss: -0.056425  g_loss: 0.187896\n",
      "Elapsed: [4:40:36.274175]  batch: 250  d_loss: -0.020571  g_loss: 0.158634\n",
      "Elapsed: [4:40:46.005905]  batch: 300  d_loss: 0.010778  g_loss: 0.086663\n",
      "Elapsed: [4:40:55.045408]  batch: 350  d_loss: 0.002093  g_loss: 0.121562\n",
      "Elapsed: [4:41:04.627364]  batch: 400  d_loss: 0.014303  g_loss: 0.063894\n",
      "Elapsed: [4:41:14.473782]  batch: 450  d_loss: -0.039790  g_loss: 0.178779\n",
      "Elapsed: [4:41:23.618008]  batch: 500  d_loss: -0.030736  g_loss: 0.150735\n",
      "Time taken for epoch: 98.713 secs\n",
      "ticker =  32501\n",
      "\n",
      "Epoch: 66\n",
      "Elapsed: [4:41:40.693322]  batch: 1  d_loss: -0.034867  g_loss: 0.164413\n",
      "Elapsed: [4:41:46.996509]  batch: 50  d_loss: -0.008257  g_loss: 0.162176\n",
      "Elapsed: [4:41:52.984270]  batch: 100  d_loss: -0.023545  g_loss: 0.076251\n",
      "Elapsed: [4:41:59.188586]  batch: 150  d_loss: 0.008118  g_loss: 0.041747\n",
      "Elapsed: [4:42:05.457563]  batch: 200  d_loss: -0.017121  g_loss: 0.183984\n",
      "Elapsed: [4:42:14.785557]  batch: 250  d_loss: -0.020264  g_loss: 0.147670\n",
      "Elapsed: [4:42:25.484118]  batch: 300  d_loss: -0.035574  g_loss: 0.222972\n",
      "Elapsed: [4:42:34.685502]  batch: 350  d_loss: -0.009406  g_loss: 0.202742\n",
      "Elapsed: [4:42:45.790212]  batch: 400  d_loss: -0.028423  g_loss: 0.188262\n",
      "Elapsed: [4:42:55.534966]  batch: 450  d_loss: -0.079128  g_loss: 0.203959\n",
      "Elapsed: [4:43:05.025595]  batch: 500  d_loss: -0.019720  g_loss: 0.095091\n",
      "Time taken for epoch: 101.125 secs\n",
      "ticker =  33001\n",
      "\n",
      "Epoch: 67\n",
      "Elapsed: [4:43:21.805348]  batch: 1  d_loss: -0.050484  g_loss: 0.195529\n",
      "Elapsed: [4:43:27.867737]  batch: 50  d_loss: -0.025699  g_loss: 0.148916\n",
      "Elapsed: [4:43:33.888526]  batch: 100  d_loss: -0.066725  g_loss: 0.130226\n",
      "Elapsed: [4:43:40.064538]  batch: 150  d_loss: 0.081888  g_loss: 0.119883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: [4:43:46.373251]  batch: 200  d_loss: 0.021796  g_loss: 0.046020\n",
      "Elapsed: [4:43:53.183696]  batch: 250  d_loss: 0.009003  g_loss: 0.107139\n",
      "Elapsed: [4:44:03.828246]  batch: 300  d_loss: -0.007605  g_loss: 0.152230\n",
      "Elapsed: [4:44:15.129880]  batch: 350  d_loss: -0.010149  g_loss: 0.121872\n",
      "Elapsed: [4:44:25.352614]  batch: 400  d_loss: 0.015359  g_loss: 0.128862\n",
      "Elapsed: [4:44:35.279142]  batch: 450  d_loss: 0.231925  g_loss: 0.032255\n",
      "Elapsed: [4:44:44.502727]  batch: 500  d_loss: 0.004499  g_loss: 0.119672\n",
      "Time taken for epoch: 99.303 secs\n",
      "ticker =  33501\n",
      "\n",
      "Epoch: 68\n",
      "Elapsed: [4:45:01.247417]  batch: 1  d_loss: 0.002710  g_loss: 0.076602\n",
      "Elapsed: [4:45:07.241828]  batch: 50  d_loss: 0.007031  g_loss: 0.107121\n",
      "Elapsed: [4:45:13.248056]  batch: 100  d_loss: -0.037220  g_loss: 0.142516\n",
      "Elapsed: [4:45:19.465906]  batch: 150  d_loss: 0.069601  g_loss: 0.073394\n",
      "Elapsed: [4:45:25.835392]  batch: 200  d_loss: -0.030150  g_loss: 0.149442\n",
      "Elapsed: [4:45:32.669229]  batch: 250  d_loss: -0.052622  g_loss: 0.172594\n",
      "Elapsed: [4:45:43.177360]  batch: 300  d_loss: -0.002577  g_loss: 0.054865\n",
      "Elapsed: [4:45:52.558775]  batch: 350  d_loss: -0.009474  g_loss: 0.136995\n",
      "Elapsed: [4:46:02.641220]  batch: 400  d_loss: 0.045351  g_loss: 0.096236\n",
      "Elapsed: [4:46:12.816470]  batch: 450  d_loss: 0.011056  g_loss: 0.087033\n",
      "Elapsed: [4:46:23.045277]  batch: 500  d_loss: -0.047459  g_loss: 0.122707\n",
      "Time taken for epoch: 98.207 secs\n",
      "ticker =  34001\n",
      "\n",
      "Epoch: 69\n",
      "Elapsed: [4:46:39.490199]  batch: 1  d_loss: -0.003661  g_loss: 0.226556\n",
      "Elapsed: [4:46:45.378521]  batch: 50  d_loss: -0.016575  g_loss: 0.115588\n",
      "Elapsed: [4:46:51.303572]  batch: 100  d_loss: 0.016184  g_loss: 0.084367\n",
      "Elapsed: [4:46:57.524663]  batch: 150  d_loss: -0.035497  g_loss: 0.149433\n",
      "Elapsed: [4:47:03.729464]  batch: 200  d_loss: -0.089871  g_loss: 0.130328\n",
      "Elapsed: [4:47:10.348571]  batch: 250  d_loss: -0.008530  g_loss: 0.045511\n",
      "Elapsed: [4:47:20.329945]  batch: 300  d_loss: -0.019416  g_loss: 0.153252\n",
      "Elapsed: [4:47:31.668282]  batch: 350  d_loss: -0.045945  g_loss: 0.164810\n",
      "Elapsed: [4:47:41.775206]  batch: 400  d_loss: -0.042061  g_loss: 0.167723\n",
      "Elapsed: [4:47:51.600470]  batch: 450  d_loss: 0.000117  g_loss: 0.103853\n",
      "Elapsed: [4:48:01.728193]  batch: 500  d_loss: -0.045525  g_loss: 0.223613\n",
      "Time taken for epoch: 98.506 secs\n",
      "ticker =  34501\n",
      "\n",
      "Epoch: 70\n",
      "Elapsed: [4:48:17.924952]  batch: 1  d_loss: 0.004854  g_loss: 0.128088\n",
      "Elapsed: [4:48:24.236163]  batch: 50  d_loss: -0.006588  g_loss: 0.072784\n",
      "Elapsed: [4:48:30.695623]  batch: 100  d_loss: 0.012572  g_loss: -0.002145\n",
      "Elapsed: [4:48:37.090617]  batch: 150  d_loss: -0.109256  g_loss: 0.201774\n",
      "Elapsed: [4:48:43.319654]  batch: 200  d_loss: -0.055042  g_loss: 0.164538\n",
      "Elapsed: [4:48:52.064470]  batch: 250  d_loss: 0.052990  g_loss: 0.023838\n",
      "Elapsed: [4:49:02.454895]  batch: 300  d_loss: -0.049000  g_loss: 0.127023\n",
      "Elapsed: [4:49:12.194729]  batch: 350  d_loss: -0.056764  g_loss: 0.109012\n",
      "Elapsed: [4:49:21.955792]  batch: 400  d_loss: 0.018013  g_loss: 0.108742\n",
      "Elapsed: [4:49:31.719902]  batch: 450  d_loss: -0.019325  g_loss: 0.146937\n",
      "Elapsed: [4:49:40.900084]  batch: 500  d_loss: -0.036438  g_loss: 0.171361\n",
      "Time taken for epoch: 99.060 secs\n",
      "ticker =  35001\n",
      "\n",
      "Epoch: 71\n",
      "Elapsed: [4:49:57.805812]  batch: 1  d_loss: -0.058963  g_loss: 0.210103\n",
      "Elapsed: [4:50:03.875948]  batch: 50  d_loss: 0.016231  g_loss: 0.101547\n",
      "Elapsed: [4:50:09.827399]  batch: 100  d_loss: -0.016594  g_loss: 0.235086\n",
      "Elapsed: [4:50:16.053854]  batch: 150  d_loss: 0.043339  g_loss: 0.088752\n",
      "Elapsed: [4:50:22.329599]  batch: 200  d_loss: 0.004198  g_loss: 0.065102\n",
      "Elapsed: [4:50:29.570775]  batch: 250  d_loss: -0.017171  g_loss: 0.134596\n",
      "Elapsed: [4:50:39.719817]  batch: 300  d_loss: -0.028349  g_loss: 0.130195\n",
      "Elapsed: [4:50:49.864806]  batch: 350  d_loss: 0.008085  g_loss: 0.087808\n",
      "Elapsed: [4:50:59.230885]  batch: 400  d_loss: -0.004582  g_loss: 0.125846\n",
      "Elapsed: [4:51:09.169226]  batch: 450  d_loss: 0.041829  g_loss: 0.075542\n",
      "Elapsed: [4:51:18.828304]  batch: 500  d_loss: -0.042041  g_loss: 0.093605\n",
      "Time taken for epoch: 97.561 secs\n",
      "ticker =  35501\n",
      "\n",
      "Epoch: 72\n",
      "Elapsed: [4:51:35.661811]  batch: 1  d_loss: 0.013988  g_loss: 0.047137\n",
      "Elapsed: [4:51:41.857793]  batch: 50  d_loss: -0.026901  g_loss: 0.207762\n",
      "Elapsed: [4:51:47.823468]  batch: 100  d_loss: -0.035414  g_loss: 0.128688\n",
      "Elapsed: [4:51:54.033679]  batch: 150  d_loss: -0.023993  g_loss: 0.052640\n",
      "Elapsed: [4:52:00.260540]  batch: 200  d_loss: -0.031720  g_loss: 0.134426\n",
      "Elapsed: [4:52:06.727751]  batch: 250  d_loss: -0.023044  g_loss: 0.065744\n",
      "Elapsed: [4:52:16.385966]  batch: 300  d_loss: -0.032901  g_loss: 0.078750\n",
      "Elapsed: [4:52:26.472982]  batch: 350  d_loss: 0.016044  g_loss: 0.137406\n",
      "Elapsed: [4:52:35.758676]  batch: 400  d_loss: 0.001769  g_loss: 0.055575\n",
      "Elapsed: [4:52:45.261769]  batch: 450  d_loss: 0.007573  g_loss: 0.074377\n",
      "Elapsed: [4:52:56.292845]  batch: 500  d_loss: -0.083405  g_loss: 0.091710\n",
      "Time taken for epoch: 97.291 secs\n",
      "ticker =  36001\n",
      "\n",
      "Epoch: 73\n",
      "Elapsed: [4:53:12.640845]  batch: 1  d_loss: 0.010968  g_loss: 0.069913\n",
      "Elapsed: [4:53:18.789617]  batch: 50  d_loss: 0.048277  g_loss: 0.049417\n",
      "Elapsed: [4:53:24.748869]  batch: 100  d_loss: -0.009752  g_loss: 0.080928\n",
      "Elapsed: [4:53:30.912400]  batch: 150  d_loss: -0.052440  g_loss: 0.144976\n",
      "Elapsed: [4:53:37.183802]  batch: 200  d_loss: -0.070314  g_loss: 0.147310\n",
      "Elapsed: [4:53:47.205244]  batch: 250  d_loss: -0.067721  g_loss: 0.160397\n",
      "Elapsed: [4:53:57.142266]  batch: 300  d_loss: -0.040407  g_loss: 0.159611\n",
      "Elapsed: [4:54:06.996658]  batch: 350  d_loss: -0.019938  g_loss: 0.110940\n",
      "Elapsed: [4:54:16.898947]  batch: 400  d_loss: -0.003204  g_loss: 0.092020\n",
      "Elapsed: [4:54:26.055052]  batch: 450  d_loss: 0.038943  g_loss: 0.110853\n",
      "Elapsed: [4:54:35.805808]  batch: 500  d_loss: -0.002879  g_loss: 0.075010\n",
      "Time taken for epoch: 99.320 secs\n",
      "ticker =  36501\n",
      "\n",
      "Epoch: 74\n",
      "Elapsed: [4:54:52.380963]  batch: 1  d_loss: -0.028756  g_loss: 0.113835\n",
      "Elapsed: [4:54:58.456509]  batch: 50  d_loss: 0.009824  g_loss: 0.144972\n",
      "Elapsed: [4:55:04.408721]  batch: 100  d_loss: -0.025614  g_loss: 0.176292\n",
      "Elapsed: [4:55:10.581083]  batch: 150  d_loss: -0.070951  g_loss: 0.167320\n",
      "Elapsed: [4:55:16.792275]  batch: 200  d_loss: -0.020285  g_loss: 0.093737\n",
      "Elapsed: [4:55:23.875467]  batch: 250  d_loss: -0.061307  g_loss: 0.130626\n",
      "Elapsed: [4:55:36.960329]  batch: 300  d_loss: -0.057202  g_loss: 0.083664\n",
      "Elapsed: [4:55:47.455838]  batch: 350  d_loss: -0.029058  g_loss: 0.114496\n",
      "Elapsed: [4:55:57.008248]  batch: 400  d_loss: -0.016770  g_loss: 0.066949\n",
      "Elapsed: [4:56:06.672408]  batch: 450  d_loss: -0.062248  g_loss: 0.147191\n",
      "Elapsed: [4:56:16.482085]  batch: 500  d_loss: -0.073994  g_loss: 0.231529\n",
      "Time taken for epoch: 100.504 secs\n",
      "ticker =  37001\n",
      "\n",
      "Epoch: 75\n",
      "Elapsed: [4:56:32.840811]  batch: 1  d_loss: 0.003075  g_loss: 0.120015\n",
      "Elapsed: [4:56:38.845169]  batch: 50  d_loss: -0.037828  g_loss: 0.205326\n",
      "Elapsed: [4:56:44.895842]  batch: 100  d_loss: -0.085872  g_loss: 0.239090\n",
      "Elapsed: [4:56:51.055156]  batch: 150  d_loss: -0.066140  g_loss: 0.192188\n",
      "Elapsed: [4:56:57.276356]  batch: 200  d_loss: -0.052201  g_loss: 0.108577\n",
      "Elapsed: [4:57:03.760611]  batch: 250  d_loss: 0.021725  g_loss: 0.031288\n",
      "Elapsed: [4:57:14.253063]  batch: 300  d_loss: -0.063037  g_loss: 0.183133\n",
      "Elapsed: [4:57:23.516511]  batch: 350  d_loss: -0.043600  g_loss: 0.161407\n",
      "Elapsed: [4:57:33.465500]  batch: 400  d_loss: 0.029027  g_loss: 0.097690\n",
      "Elapsed: [4:57:43.129485]  batch: 450  d_loss: 0.026213  g_loss: 0.056055\n",
      "Elapsed: [4:57:53.424047]  batch: 500  d_loss: 0.011974  g_loss: 0.029893\n",
      "Time taken for epoch: 97.063 secs\n",
      "ticker =  37501\n",
      "\n",
      "\n",
      "Currently working on Depth:  5\n",
      "Current resolution: 64 x 64\n",
      "Ticker 1\n",
      "\n",
      "Epoch: 1\n",
      "Elapsed: [4:58:11.606271]  batch: 1  d_loss: 2.465490  g_loss: 0.363928\n",
      "Elapsed: [4:58:22.334782]  batch: 50  d_loss: -0.075646  g_loss: 0.552753\n",
      "Elapsed: [4:58:34.113282]  batch: 100  d_loss: -0.032200  g_loss: 0.260128\n",
      "Elapsed: [4:58:45.915303]  batch: 150  d_loss: -0.138541  g_loss: 0.369920\n",
      "Elapsed: [4:58:57.682178]  batch: 200  d_loss: 0.034605  g_loss: 0.123728\n",
      "Elapsed: [4:59:09.462700]  batch: 250  d_loss: -0.116635  g_loss: 0.252120\n",
      "Elapsed: [4:59:21.246950]  batch: 300  d_loss: 0.001521  g_loss: 0.191958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: [4:59:33.045413]  batch: 350  d_loss: -0.154911  g_loss: 0.358027\n",
      "Elapsed: [4:59:44.837852]  batch: 400  d_loss: -0.108661  g_loss: 0.300672\n",
      "Elapsed: [4:59:56.630872]  batch: 450  d_loss: 0.046734  g_loss: 0.241582\n",
      "Elapsed: [5:00:08.434199]  batch: 500  d_loss: 0.050620  g_loss: 0.098022\n",
      "Time taken for epoch: 134.255 secs\n",
      "ticker =  501\n",
      "\n",
      "Epoch: 2\n",
      "Elapsed: [5:00:18.703843]  batch: 1  d_loss: -0.084435  g_loss: 0.271175\n",
      "Elapsed: [5:00:30.255295]  batch: 50  d_loss: 0.015245  g_loss: 0.117113\n",
      "Elapsed: [5:00:42.045734]  batch: 100  d_loss: -0.168004  g_loss: 0.462083\n",
      "Elapsed: [5:00:53.831945]  batch: 150  d_loss: -0.005975  g_loss: 0.291597\n",
      "Elapsed: [5:01:05.632765]  batch: 200  d_loss: -0.053822  g_loss: 0.276295\n",
      "Elapsed: [5:01:17.426597]  batch: 250  d_loss: -0.106145  g_loss: 0.473181\n",
      "Elapsed: [5:01:29.215720]  batch: 300  d_loss: 0.027543  g_loss: 0.148373\n",
      "Elapsed: [5:01:41.013279]  batch: 350  d_loss: 0.044683  g_loss: 0.197069\n",
      "Elapsed: [5:01:54.128720]  batch: 400  d_loss: 0.190934  g_loss: 0.021854\n",
      "Elapsed: [5:02:06.052356]  batch: 450  d_loss: 0.000332  g_loss: 0.370333\n",
      "Elapsed: [5:02:19.279554]  batch: 500  d_loss: -0.093431  g_loss: 0.334596\n",
      "Time taken for epoch: 130.766 secs\n",
      "ticker =  1001\n",
      "\n",
      "Epoch: 3\n",
      "Elapsed: [5:02:29.805726]  batch: 1  d_loss: -0.069873  g_loss: 0.375598\n",
      "Elapsed: [5:02:41.366650]  batch: 50  d_loss: 0.009418  g_loss: 0.316567\n",
      "Elapsed: [5:02:53.159320]  batch: 100  d_loss: 0.078359  g_loss: 0.061434\n",
      "Elapsed: [5:03:04.966536]  batch: 150  d_loss: -0.290967  g_loss: 0.475200\n",
      "Elapsed: [5:03:16.774123]  batch: 200  d_loss: -0.012833  g_loss: 0.178795\n",
      "Elapsed: [5:03:29.880406]  batch: 250  d_loss: -0.010793  g_loss: 0.282468\n",
      "Elapsed: [5:03:42.997938]  batch: 300  d_loss: 0.183043  g_loss: -0.155258\n",
      "Elapsed: [5:03:56.106118]  batch: 350  d_loss: 0.069781  g_loss: 0.209583\n",
      "Elapsed: [5:04:08.495490]  batch: 400  d_loss: -0.162121  g_loss: 0.405087\n",
      "Elapsed: [5:04:21.225526]  batch: 450  d_loss: 0.041562  g_loss: 0.048722\n",
      "Elapsed: [5:04:34.169148]  batch: 500  d_loss: -0.056985  g_loss: 0.392493\n",
      "Time taken for epoch: 134.752 secs\n",
      "ticker =  1501\n",
      "\n",
      "Epoch: 4\n",
      "Elapsed: [5:04:44.623861]  batch: 1  d_loss: -0.247803  g_loss: 0.674716\n",
      "Elapsed: [5:04:56.203101]  batch: 50  d_loss: -0.235366  g_loss: 0.387669\n",
      "Elapsed: [5:05:08.015727]  batch: 100  d_loss: 0.079559  g_loss: 0.147155\n",
      "Elapsed: [5:05:20.014284]  batch: 150  d_loss: -0.333512  g_loss: 0.684578\n",
      "Elapsed: [5:05:32.971829]  batch: 200  d_loss: -0.019291  g_loss: 0.283987\n",
      "Elapsed: [5:05:46.304928]  batch: 250  d_loss: -0.222480  g_loss: 0.415146\n",
      "Elapsed: [5:05:59.235393]  batch: 300  d_loss: -0.148774  g_loss: 0.446329\n",
      "Elapsed: [5:06:13.298089]  batch: 350  d_loss: -0.138267  g_loss: 0.434383\n",
      "Elapsed: [5:06:25.976580]  batch: 400  d_loss: 0.004432  g_loss: 0.330650\n",
      "Elapsed: [5:06:39.495278]  batch: 450  d_loss: 0.065290  g_loss: 0.056447\n",
      "Elapsed: [5:06:52.988035]  batch: 500  d_loss: -0.000749  g_loss: 0.150858\n",
      "Time taken for epoch: 138.698 secs\n",
      "ticker =  2001\n",
      "\n",
      "Epoch: 5\n",
      "Elapsed: [5:07:03.384706]  batch: 1  d_loss: -0.133319  g_loss: 0.259984\n",
      "Elapsed: [5:07:14.945439]  batch: 50  d_loss: -0.058339  g_loss: 0.425947\n",
      "Elapsed: [5:07:26.741879]  batch: 100  d_loss: -0.043721  g_loss: 0.373993\n",
      "Elapsed: [5:07:39.846854]  batch: 150  d_loss: -0.289407  g_loss: 0.591467\n",
      "Elapsed: [5:07:52.529266]  batch: 200  d_loss: -0.293012  g_loss: 0.731798\n",
      "Elapsed: [5:08:05.206285]  batch: 250  d_loss: -0.238924  g_loss: 0.416525\n",
      "Elapsed: [5:08:19.036422]  batch: 300  d_loss: -0.282078  g_loss: 0.649770\n",
      "Elapsed: [5:08:34.573343]  batch: 350  d_loss: -0.084757  g_loss: 0.598806\n",
      "Elapsed: [5:08:48.061397]  batch: 400  d_loss: 0.042593  g_loss: 0.255120\n",
      "Elapsed: [5:09:01.736915]  batch: 450  d_loss: -0.342859  g_loss: 0.562684\n",
      "Elapsed: [5:09:15.142411]  batch: 500  d_loss: 0.070720  g_loss: 0.193286\n",
      "Time taken for epoch: 142.273 secs\n",
      "ticker =  2501\n",
      "\n",
      "Epoch: 6\n",
      "Elapsed: [5:09:32.638948]  batch: 1  d_loss: -0.194480  g_loss: 0.412584\n",
      "Elapsed: [5:09:45.855952]  batch: 50  d_loss: -0.003691  g_loss: 0.278535\n",
      "Elapsed: [5:09:58.746008]  batch: 100  d_loss: -0.024661  g_loss: 0.247130\n",
      "Elapsed: [5:10:11.732879]  batch: 150  d_loss: -0.150682  g_loss: 0.501025\n",
      "Elapsed: [5:10:25.546231]  batch: 200  d_loss: -0.092886  g_loss: 0.307988\n",
      "Elapsed: [5:10:39.108024]  batch: 250  d_loss: -0.108518  g_loss: 0.373164\n",
      "Elapsed: [5:10:59.729175]  batch: 300  d_loss: 0.072506  g_loss: -0.015226\n",
      "Elapsed: [5:11:15.652614]  batch: 350  d_loss: 0.063282  g_loss: 0.137868\n",
      "Elapsed: [5:11:31.575437]  batch: 400  d_loss: 0.000477  g_loss: 0.217285\n",
      "Elapsed: [5:11:47.756002]  batch: 450  d_loss: -0.215548  g_loss: 0.486921\n",
      "Elapsed: [5:12:04.888351]  batch: 500  d_loss: 0.031252  g_loss: 0.123308\n",
      "Time taken for epoch: 169.585 secs\n",
      "ticker =  3001\n",
      "\n",
      "Epoch: 7\n",
      "Elapsed: [5:12:26.298500]  batch: 1  d_loss: -0.502596  g_loss: 0.821318\n",
      "Elapsed: [5:12:38.130415]  batch: 50  d_loss: -0.055411  g_loss: 0.176402\n",
      "Elapsed: [5:12:50.743124]  batch: 100  d_loss: -0.268978  g_loss: 0.575996\n",
      "Elapsed: [5:13:03.492375]  batch: 150  d_loss: -0.111108  g_loss: 0.211830\n",
      "Elapsed: [5:13:16.417063]  batch: 200  d_loss: -0.096216  g_loss: 0.460969\n",
      "Elapsed: [5:13:29.324659]  batch: 250  d_loss: -0.062645  g_loss: 0.453296\n",
      "Elapsed: [5:13:42.239592]  batch: 300  d_loss: -0.222298  g_loss: 0.596835\n",
      "Elapsed: [5:13:55.275753]  batch: 350  d_loss: -0.428360  g_loss: 0.767834\n",
      "Elapsed: [5:14:08.117270]  batch: 400  d_loss: -0.473130  g_loss: 0.708398\n",
      "Elapsed: [5:14:21.012787]  batch: 450  d_loss: 0.054658  g_loss: 0.331297\n",
      "Elapsed: [5:14:33.925540]  batch: 500  d_loss: 0.002092  g_loss: 0.141371\n",
      "Time taken for epoch: 148.593 secs\n",
      "ticker =  3501\n",
      "\n",
      "Epoch: 8\n",
      "Elapsed: [5:14:50.863586]  batch: 1  d_loss: -0.610888  g_loss: 0.779844\n",
      "Elapsed: [5:15:02.880138]  batch: 50  d_loss: 0.016887  g_loss: 0.284577\n",
      "Elapsed: [5:15:15.681159]  batch: 100  d_loss: -0.141261  g_loss: 0.377393\n",
      "Elapsed: [5:15:28.516126]  batch: 150  d_loss: -0.175270  g_loss: 0.509877\n",
      "Elapsed: [5:15:41.456684]  batch: 200  d_loss: 0.127588  g_loss: 0.179106\n",
      "Elapsed: [5:15:54.349823]  batch: 250  d_loss: -0.320055  g_loss: 0.688112\n",
      "Elapsed: [5:16:07.127489]  batch: 300  d_loss: -0.338073  g_loss: 0.714723\n",
      "Elapsed: [5:16:20.309997]  batch: 350  d_loss: -0.177511  g_loss: 0.416154\n",
      "Elapsed: [5:16:35.458575]  batch: 400  d_loss: -0.008531  g_loss: 0.324588\n",
      "Elapsed: [5:16:52.173975]  batch: 450  d_loss: -0.084568  g_loss: -0.070762\n",
      "Elapsed: [5:17:05.975400]  batch: 500  d_loss: -0.121248  g_loss: 0.291961\n",
      "Time taken for epoch: 151.906 secs\n",
      "ticker =  4001\n",
      "\n",
      "Epoch: 9\n",
      "Elapsed: [5:17:23.129197]  batch: 1  d_loss: -0.236090  g_loss: 0.658966\n",
      "Elapsed: [5:17:35.115114]  batch: 50  d_loss: 0.021375  g_loss: 0.189680\n",
      "Elapsed: [5:17:48.676713]  batch: 100  d_loss: -0.050839  g_loss: 0.424002\n",
      "Elapsed: [5:18:01.502671]  batch: 150  d_loss: -0.272410  g_loss: 0.676507\n",
      "Elapsed: [5:18:14.576657]  batch: 200  d_loss: -0.327734  g_loss: 0.531477\n",
      "Elapsed: [5:18:27.480253]  batch: 250  d_loss: 0.100828  g_loss: 0.063967\n",
      "Elapsed: [5:18:40.196693]  batch: 300  d_loss: 0.080193  g_loss: -0.192529\n",
      "Elapsed: [5:18:53.041674]  batch: 350  d_loss: -0.144321  g_loss: 0.532630\n",
      "Elapsed: [5:19:05.944915]  batch: 400  d_loss: -0.162057  g_loss: 0.609596\n",
      "Elapsed: [5:19:18.887114]  batch: 450  d_loss: -0.298842  g_loss: 0.624162\n",
      "Elapsed: [5:19:31.776359]  batch: 500  d_loss: -0.405113  g_loss: 0.790453\n",
      "Time taken for epoch: 145.682 secs\n",
      "ticker =  4501\n",
      "\n",
      "Epoch: 10\n",
      "Elapsed: [5:19:48.832707]  batch: 1  d_loss: 0.014895  g_loss: 0.364616\n",
      "Elapsed: [5:20:00.800058]  batch: 50  d_loss: -0.097636  g_loss: 0.557172\n",
      "Elapsed: [5:20:13.458336]  batch: 100  d_loss: -0.196470  g_loss: 0.326308\n",
      "Elapsed: [5:20:26.298618]  batch: 150  d_loss: -0.688598  g_loss: 0.752727\n",
      "Elapsed: [5:20:39.110724]  batch: 200  d_loss: -0.194081  g_loss: 0.541874\n",
      "Elapsed: [5:20:51.976918]  batch: 250  d_loss: -0.430064  g_loss: 0.653131\n",
      "Elapsed: [5:21:04.757659]  batch: 300  d_loss: -0.420568  g_loss: 0.985196\n",
      "Elapsed: [5:21:17.496911]  batch: 350  d_loss: -0.021496  g_loss: 0.356841\n",
      "Elapsed: [5:21:30.455914]  batch: 400  d_loss: -0.462306  g_loss: 0.755771\n",
      "Elapsed: [5:21:43.418576]  batch: 450  d_loss: -0.362980  g_loss: 0.738069\n",
      "Elapsed: [5:21:56.253237]  batch: 500  d_loss: -0.081157  g_loss: 0.429796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for epoch: 144.329 secs\n",
      "ticker =  5001\n",
      "\n",
      "Epoch: 11\n",
      "Elapsed: [5:22:14.060919]  batch: 1  d_loss: -0.366838  g_loss: 0.740974\n",
      "Elapsed: [5:22:26.110209]  batch: 50  d_loss: -0.530829  g_loss: 0.994204\n",
      "Elapsed: [5:22:38.748623]  batch: 100  d_loss: -0.101806  g_loss: 0.397645\n",
      "Elapsed: [5:22:51.481761]  batch: 150  d_loss: -0.384716  g_loss: 0.773385\n",
      "Elapsed: [5:23:04.342601]  batch: 200  d_loss: -0.166794  g_loss: 0.431569\n",
      "Elapsed: [5:23:17.078675]  batch: 250  d_loss: -0.103416  g_loss: 0.389526\n",
      "Elapsed: [5:23:29.788483]  batch: 300  d_loss: -0.522305  g_loss: 0.962807\n",
      "Elapsed: [5:23:42.527191]  batch: 350  d_loss: -0.401831  g_loss: 0.654923\n",
      "Elapsed: [5:23:55.476085]  batch: 400  d_loss: -0.136250  g_loss: 0.443864\n",
      "Elapsed: [5:24:08.351017]  batch: 450  d_loss: -0.421199  g_loss: 0.784932\n",
      "Elapsed: [5:24:21.055903]  batch: 500  d_loss: -0.613756  g_loss: 0.775488\n",
      "Time taken for epoch: 144.460 secs\n",
      "ticker =  5501\n",
      "\n",
      "Epoch: 12\n",
      "Elapsed: [5:24:37.813749]  batch: 1  d_loss: 0.085852  g_loss: 0.065012\n",
      "Elapsed: [5:24:49.658938]  batch: 50  d_loss: -0.023449  g_loss: 0.251206\n",
      "Elapsed: [5:25:02.393215]  batch: 100  d_loss: -0.292174  g_loss: 0.523006\n",
      "Elapsed: [5:25:15.141743]  batch: 150  d_loss: -0.121646  g_loss: 0.591019\n",
      "Elapsed: [5:25:28.974415]  batch: 200  d_loss: -0.177994  g_loss: 0.578785\n",
      "Elapsed: [5:25:41.623005]  batch: 250  d_loss: 0.042357  g_loss: 0.119550\n",
      "Elapsed: [5:25:54.600408]  batch: 300  d_loss: 0.001334  g_loss: 0.334033\n",
      "Elapsed: [5:26:07.456088]  batch: 350  d_loss: -0.323615  g_loss: 0.620483\n",
      "Elapsed: [5:26:20.337857]  batch: 400  d_loss: 0.060511  g_loss: 0.217113\n",
      "Elapsed: [5:26:33.181227]  batch: 450  d_loss: 0.027831  g_loss: 0.230680\n",
      "Elapsed: [5:26:45.945178]  batch: 500  d_loss: 0.025857  g_loss: 0.150203\n",
      "Time taken for epoch: 144.725 secs\n",
      "ticker =  6001\n",
      "\n",
      "Epoch: 13\n",
      "Elapsed: [5:27:02.855088]  batch: 1  d_loss: -0.492287  g_loss: 0.790479\n",
      "Elapsed: [5:27:14.682947]  batch: 50  d_loss: -0.211966  g_loss: 0.568692\n",
      "Elapsed: [5:27:27.547034]  batch: 100  d_loss: -0.402699  g_loss: 0.697309\n",
      "Elapsed: [5:27:40.289133]  batch: 150  d_loss: 0.079899  g_loss: -0.019899\n",
      "Elapsed: [5:27:53.021158]  batch: 200  d_loss: -0.648510  g_loss: 0.946703\n",
      "Elapsed: [5:28:05.687065]  batch: 250  d_loss: -0.705998  g_loss: 0.925088\n",
      "Elapsed: [5:28:18.400948]  batch: 300  d_loss: -0.655039  g_loss: 0.989673\n",
      "Elapsed: [5:28:31.154218]  batch: 350  d_loss: -0.035619  g_loss: 0.149834\n",
      "Elapsed: [5:28:43.840342]  batch: 400  d_loss: 0.116445  g_loss: 0.070526\n",
      "Elapsed: [5:28:56.639619]  batch: 450  d_loss: -0.005890  g_loss: 0.215514\n",
      "Elapsed: [5:29:11.818871]  batch: 500  d_loss: 0.004556  g_loss: 0.360554\n",
      "Time taken for epoch: 145.708 secs\n",
      "ticker =  6501\n",
      "\n",
      "Epoch: 14\n",
      "Elapsed: [5:29:34.114878]  batch: 1  d_loss: -0.581832  g_loss: 0.963610\n",
      "Elapsed: [5:29:44.796056]  batch: 50  d_loss: -0.633255  g_loss: 0.806886\n",
      "Elapsed: [5:29:56.573163]  batch: 100  d_loss: 0.032034  g_loss: 0.302320\n",
      "Elapsed: [5:30:09.202101]  batch: 150  d_loss: -0.046308  g_loss: 0.027204\n",
      "Elapsed: [5:30:21.987628]  batch: 200  d_loss: 0.109937  g_loss: 0.002829\n",
      "Elapsed: [5:30:34.717343]  batch: 250  d_loss: -0.331060  g_loss: 0.754955\n",
      "Elapsed: [5:30:47.460386]  batch: 300  d_loss: -0.674555  g_loss: 0.893841\n",
      "Elapsed: [5:31:00.165941]  batch: 350  d_loss: 0.013754  g_loss: 0.300769\n",
      "Elapsed: [5:31:12.872286]  batch: 400  d_loss: -0.431774  g_loss: 0.464360\n",
      "Elapsed: [5:31:25.550468]  batch: 450  d_loss: -0.352575  g_loss: 0.624140\n",
      "Elapsed: [5:31:38.278064]  batch: 500  d_loss: -0.426249  g_loss: 0.834850\n",
      "Time taken for epoch: 146.187 secs\n",
      "ticker =  7001\n",
      "\n",
      "Epoch: 15\n",
      "Elapsed: [5:31:54.836766]  batch: 1  d_loss: 0.017816  g_loss: 0.187386\n",
      "Elapsed: [5:32:06.368861]  batch: 50  d_loss: -0.799203  g_loss: 0.983201\n",
      "Elapsed: [5:32:18.946814]  batch: 100  d_loss: 0.086894  g_loss: 0.055647\n",
      "Elapsed: [5:32:31.508859]  batch: 150  d_loss: -0.455325  g_loss: 0.718828\n",
      "Elapsed: [5:32:44.081685]  batch: 200  d_loss: -0.268246  g_loss: 0.623480\n",
      "Elapsed: [5:32:56.785638]  batch: 250  d_loss: -0.090457  g_loss: 0.338204\n",
      "Elapsed: [5:33:09.487532]  batch: 300  d_loss: -0.061218  g_loss: 0.152746\n",
      "Elapsed: [5:33:22.185818]  batch: 350  d_loss: -0.247013  g_loss: 0.564298\n",
      "Elapsed: [5:33:35.055703]  batch: 400  d_loss: 0.041985  g_loss: 0.011903\n",
      "Elapsed: [5:33:47.766396]  batch: 450  d_loss: 0.082290  g_loss: 0.007802\n",
      "Elapsed: [5:34:00.499558]  batch: 500  d_loss: 0.062423  g_loss: 0.064621\n",
      "Time taken for epoch: 142.034 secs\n",
      "ticker =  7501\n",
      "\n",
      "Epoch: 16\n",
      "Elapsed: [5:34:17.393372]  batch: 1  d_loss: -0.243893  g_loss: 0.530954\n",
      "Elapsed: [5:34:29.087412]  batch: 50  d_loss: -0.215783  g_loss: 0.445426\n",
      "Elapsed: [5:34:41.736555]  batch: 100  d_loss: -0.530070  g_loss: 0.905607\n",
      "Elapsed: [5:34:54.771643]  batch: 150  d_loss: -0.420464  g_loss: 0.783655\n",
      "Elapsed: [5:35:07.580821]  batch: 200  d_loss: -0.245948  g_loss: 0.569114\n",
      "Elapsed: [5:35:20.374571]  batch: 250  d_loss: 0.010787  g_loss: 0.037579\n",
      "Elapsed: [5:35:33.140461]  batch: 300  d_loss: -0.629998  g_loss: 0.789660\n",
      "Elapsed: [5:35:45.983089]  batch: 350  d_loss: -0.148717  g_loss: 0.552087\n",
      "Elapsed: [5:35:58.697895]  batch: 400  d_loss: -0.478888  g_loss: 0.883421\n",
      "Elapsed: [5:36:11.341573]  batch: 450  d_loss: -0.228230  g_loss: 0.563110\n",
      "Elapsed: [5:36:24.215562]  batch: 500  d_loss: -0.472392  g_loss: 0.835475\n",
      "Time taken for epoch: 143.541 secs\n",
      "ticker =  8001\n",
      "\n",
      "Epoch: 17\n",
      "Elapsed: [5:36:41.022439]  batch: 1  d_loss: 0.041199  g_loss: 0.089558\n",
      "Elapsed: [5:36:52.557425]  batch: 50  d_loss: -0.560995  g_loss: 0.827932\n",
      "Elapsed: [5:37:05.099874]  batch: 100  d_loss: -0.141529  g_loss: 0.333011\n",
      "Elapsed: [5:37:17.881853]  batch: 150  d_loss: -0.363022  g_loss: 0.379826\n",
      "Elapsed: [5:37:30.580115]  batch: 200  d_loss: 0.004244  g_loss: 0.155976\n",
      "Elapsed: [5:37:43.312738]  batch: 250  d_loss: -0.090373  g_loss: 0.273840\n",
      "Elapsed: [5:37:56.056986]  batch: 300  d_loss: -0.402520  g_loss: 0.744225\n",
      "Elapsed: [5:38:08.889789]  batch: 350  d_loss: -0.096202  g_loss: 0.513618\n",
      "Elapsed: [5:38:21.729922]  batch: 400  d_loss: -0.623657  g_loss: 0.937636\n",
      "Elapsed: [5:38:34.465254]  batch: 450  d_loss: 0.072523  g_loss: -0.079825\n",
      "Elapsed: [5:38:47.178492]  batch: 500  d_loss: -0.293897  g_loss: 0.593715\n",
      "Time taken for epoch: 142.864 secs\n",
      "ticker =  8501\n",
      "\n",
      "Epoch: 18\n",
      "Elapsed: [5:39:04.435011]  batch: 1  d_loss: -0.380437  g_loss: 0.753471\n",
      "Elapsed: [5:39:16.058401]  batch: 50  d_loss: -0.057636  g_loss: 0.120904\n",
      "Elapsed: [5:39:28.707587]  batch: 100  d_loss: -0.442459  g_loss: 0.506822\n",
      "Elapsed: [5:39:41.510027]  batch: 150  d_loss: -0.418029  g_loss: 1.000819\n",
      "Elapsed: [5:39:54.359481]  batch: 200  d_loss: 0.065486  g_loss: -0.023337\n",
      "Elapsed: [5:40:07.093723]  batch: 250  d_loss: -0.133897  g_loss: 0.358691\n",
      "Elapsed: [5:40:19.796530]  batch: 300  d_loss: -0.156982  g_loss: 0.434389\n",
      "Elapsed: [5:40:33.216835]  batch: 350  d_loss: -0.054335  g_loss: 0.414982\n",
      "Elapsed: [5:40:46.038042]  batch: 400  d_loss: -0.059888  g_loss: 0.257137\n",
      "Elapsed: [5:40:58.759404]  batch: 450  d_loss: -0.194329  g_loss: 0.464496\n",
      "Elapsed: [5:41:11.578566]  batch: 500  d_loss: -0.002874  g_loss: 0.368743\n",
      "Time taken for epoch: 144.150 secs\n",
      "ticker =  9001\n",
      "\n",
      "Epoch: 19\n",
      "Elapsed: [5:41:28.502459]  batch: 1  d_loss: -0.462187  g_loss: 0.650945\n",
      "Elapsed: [5:41:40.129712]  batch: 50  d_loss: -0.391107  g_loss: 0.476458\n",
      "Elapsed: [5:41:52.773255]  batch: 100  d_loss: -0.485370  g_loss: 0.646145\n",
      "Elapsed: [5:42:05.628059]  batch: 150  d_loss: -0.340210  g_loss: 0.529790\n",
      "Elapsed: [5:42:18.491127]  batch: 200  d_loss: -0.304762  g_loss: 0.620186\n",
      "Elapsed: [5:42:31.450841]  batch: 250  d_loss: -0.019143  g_loss: 0.430721\n",
      "Elapsed: [5:42:44.243059]  batch: 300  d_loss: -0.762906  g_loss: 0.759641\n",
      "Elapsed: [5:42:57.146101]  batch: 350  d_loss: -0.449048  g_loss: 0.698274\n",
      "Elapsed: [5:43:09.871164]  batch: 400  d_loss: -0.213441  g_loss: 0.590861\n",
      "Elapsed: [5:43:22.606763]  batch: 450  d_loss: -0.091092  g_loss: 0.453339\n",
      "Elapsed: [5:43:35.575246]  batch: 500  d_loss: -0.531637  g_loss: 0.748653\n",
      "Time taken for epoch: 143.821 secs\n",
      "ticker =  9501\n",
      "\n",
      "Epoch: 20\n",
      "Elapsed: [5:43:52.235653]  batch: 1  d_loss: -0.186663  g_loss: 0.337544\n",
      "Elapsed: [5:44:04.162482]  batch: 50  d_loss: -0.407745  g_loss: 0.717217\n",
      "Elapsed: [5:44:16.771561]  batch: 100  d_loss: -0.315771  g_loss: 0.609706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: [5:44:29.454876]  batch: 150  d_loss: -0.431914  g_loss: 0.693339\n",
      "Elapsed: [5:44:42.061933]  batch: 200  d_loss: -0.385265  g_loss: 0.460359\n",
      "Elapsed: [5:44:54.826470]  batch: 250  d_loss: -0.258533  g_loss: 0.353410\n",
      "Elapsed: [5:45:07.578296]  batch: 300  d_loss: -0.287069  g_loss: 0.480823\n",
      "Elapsed: [5:45:20.313624]  batch: 350  d_loss: -0.107412  g_loss: 0.338129\n",
      "Elapsed: [5:45:33.187419]  batch: 400  d_loss: -0.348738  g_loss: 0.504640\n",
      "Elapsed: [5:45:45.924161]  batch: 450  d_loss: -0.043871  g_loss: 0.036244\n",
      "Elapsed: [5:45:58.785731]  batch: 500  d_loss: -0.348469  g_loss: 0.617232\n",
      "Time taken for epoch: 143.078 secs\n",
      "ticker =  10001\n",
      "\n",
      "Epoch: 21\n",
      "Elapsed: [5:46:15.829704]  batch: 1  d_loss: -0.231307  g_loss: 0.546772\n",
      "Elapsed: [5:46:27.372870]  batch: 50  d_loss: 0.219519  g_loss: 0.151132\n",
      "Elapsed: [5:46:39.931201]  batch: 100  d_loss: -0.257097  g_loss: 0.723190\n",
      "Elapsed: [5:46:52.741154]  batch: 150  d_loss: -0.354782  g_loss: 0.715596\n",
      "Elapsed: [5:47:05.391526]  batch: 200  d_loss: -0.161005  g_loss: 0.549536\n",
      "Elapsed: [5:47:18.149437]  batch: 250  d_loss: -0.655406  g_loss: 0.941418\n",
      "Elapsed: [5:47:30.849990]  batch: 300  d_loss: -0.571598  g_loss: 0.804590\n",
      "Elapsed: [5:47:43.511317]  batch: 350  d_loss: -0.239157  g_loss: 0.479772\n",
      "Elapsed: [5:47:56.358610]  batch: 400  d_loss: 0.034192  g_loss: -0.066576\n",
      "Elapsed: [5:48:10.018202]  batch: 450  d_loss: -0.155597  g_loss: 0.395167\n",
      "Elapsed: [5:48:22.928282]  batch: 500  d_loss: -0.522995  g_loss: 0.912184\n",
      "Time taken for epoch: 143.950 secs\n",
      "ticker =  10501\n",
      "\n",
      "Epoch: 22\n",
      "Elapsed: [5:48:39.467792]  batch: 1  d_loss: -0.027602  g_loss: 0.284565\n",
      "Elapsed: [5:48:51.542528]  batch: 50  d_loss: -0.454804  g_loss: 0.783511\n",
      "Elapsed: [5:49:04.222035]  batch: 100  d_loss: -0.520911  g_loss: 0.783312\n",
      "Elapsed: [5:49:16.909732]  batch: 150  d_loss: -0.560665  g_loss: 0.950749\n",
      "Elapsed: [5:49:29.678014]  batch: 200  d_loss: -0.377814  g_loss: 0.615592\n",
      "Elapsed: [5:49:42.454287]  batch: 250  d_loss: -0.269959  g_loss: 0.684293\n",
      "Elapsed: [5:49:55.186585]  batch: 300  d_loss: -0.309427  g_loss: 0.667060\n",
      "Elapsed: [5:50:07.910196]  batch: 350  d_loss: -0.344699  g_loss: 0.660255\n",
      "Elapsed: [5:50:20.694831]  batch: 400  d_loss: -0.274986  g_loss: 0.577908\n",
      "Elapsed: [5:50:33.395015]  batch: 450  d_loss: -0.647366  g_loss: 0.922776\n",
      "Elapsed: [5:50:46.075536]  batch: 500  d_loss: -0.422800  g_loss: 0.459251\n",
      "Time taken for epoch: 143.066 secs\n",
      "ticker =  11001\n",
      "\n",
      "Epoch: 23\n",
      "Elapsed: [5:51:03.298203]  batch: 1  d_loss: -0.288774  g_loss: 0.525643\n",
      "Elapsed: [5:51:14.849960]  batch: 50  d_loss: -0.340034  g_loss: 0.701850\n",
      "Elapsed: [5:51:27.355801]  batch: 100  d_loss: -0.587761  g_loss: 0.865773\n",
      "Elapsed: [5:51:40.158613]  batch: 150  d_loss: -0.177853  g_loss: 0.452298\n",
      "Elapsed: [5:51:53.063402]  batch: 200  d_loss: -0.364145  g_loss: 0.585665\n",
      "Elapsed: [5:52:05.793545]  batch: 250  d_loss: -0.074747  g_loss: 0.532748\n",
      "Elapsed: [5:52:18.498207]  batch: 300  d_loss: -0.179327  g_loss: 0.327740\n",
      "Elapsed: [5:52:31.382355]  batch: 350  d_loss: -0.515035  g_loss: 0.885064\n",
      "Elapsed: [5:52:44.036513]  batch: 400  d_loss: -0.154444  g_loss: 0.440863\n",
      "Elapsed: [5:52:56.751533]  batch: 450  d_loss: -0.253094  g_loss: 0.905035\n",
      "Elapsed: [5:53:09.422420]  batch: 500  d_loss: -0.074362  g_loss: 0.256830\n",
      "Time taken for epoch: 143.058 secs\n",
      "ticker =  11501\n",
      "\n",
      "Epoch: 24\n",
      "Elapsed: [5:53:26.384595]  batch: 1  d_loss: -0.354771  g_loss: 0.442761\n",
      "Elapsed: [5:53:38.163381]  batch: 50  d_loss: -0.170919  g_loss: 0.387401\n",
      "Elapsed: [5:53:50.718613]  batch: 100  d_loss: -0.085733  g_loss: 0.866569\n",
      "Elapsed: [5:54:03.453414]  batch: 150  d_loss: -0.041704  g_loss: 0.639973\n",
      "Elapsed: [5:54:16.237434]  batch: 200  d_loss: -0.028401  g_loss: 0.061628\n",
      "Elapsed: [5:54:29.095216]  batch: 250  d_loss: -0.435144  g_loss: 0.552692\n",
      "Elapsed: [5:54:41.778968]  batch: 300  d_loss: -0.361005  g_loss: 0.690782\n",
      "Elapsed: [5:54:54.478177]  batch: 350  d_loss: -0.098626  g_loss: 0.227810\n",
      "Elapsed: [5:55:07.234935]  batch: 400  d_loss: -0.334698  g_loss: 0.451623\n",
      "Elapsed: [5:55:19.976174]  batch: 450  d_loss: -0.396752  g_loss: 0.760462\n",
      "Elapsed: [5:55:33.170030]  batch: 500  d_loss: -0.341995  g_loss: 0.469284\n",
      "Time taken for epoch: 143.651 secs\n",
      "ticker =  12001\n",
      "\n",
      "Epoch: 25\n",
      "Elapsed: [5:55:50.023543]  batch: 1  d_loss: -0.025852  g_loss: -0.027087\n",
      "Elapsed: [5:56:01.816537]  batch: 50  d_loss: -0.131246  g_loss: 0.476329\n",
      "Elapsed: [5:56:14.503040]  batch: 100  d_loss: -0.473942  g_loss: 0.347913\n",
      "Elapsed: [5:56:27.105669]  batch: 150  d_loss: -0.347971  g_loss: 0.757530\n",
      "Elapsed: [5:56:39.864224]  batch: 200  d_loss: -0.293786  g_loss: 0.705424\n",
      "Elapsed: [5:56:52.590028]  batch: 250  d_loss: 0.011158  g_loss: -0.068809\n",
      "Elapsed: [5:57:05.339020]  batch: 300  d_loss: -0.058178  g_loss: 0.245264\n",
      "Elapsed: [5:57:18.088466]  batch: 350  d_loss: -0.234949  g_loss: 0.800262\n",
      "Elapsed: [5:57:30.931041]  batch: 400  d_loss: -0.229034  g_loss: 1.398725\n",
      "Elapsed: [5:57:43.561746]  batch: 450  d_loss: -0.634816  g_loss: 1.088289\n",
      "Elapsed: [5:57:56.378455]  batch: 500  d_loss: 0.264798  g_loss: -0.172323\n",
      "Time taken for epoch: 142.914 secs\n",
      "ticker =  12501\n",
      "\n",
      "Epoch: 26\n",
      "Elapsed: [5:58:12.821743]  batch: 1  d_loss: -0.438252  g_loss: 0.606141\n",
      "Elapsed: [5:58:24.650379]  batch: 50  d_loss: -0.035175  g_loss: 0.066754\n",
      "Elapsed: [5:58:37.188309]  batch: 100  d_loss: -0.105015  g_loss: 0.435043\n",
      "Elapsed: [5:58:49.783866]  batch: 150  d_loss: -0.078658  g_loss: 0.346088\n",
      "Elapsed: [5:59:02.501712]  batch: 200  d_loss: -0.325623  g_loss: 0.952117\n",
      "Elapsed: [5:59:15.262132]  batch: 250  d_loss: -0.057929  g_loss: 0.302475\n",
      "Elapsed: [5:59:27.813209]  batch: 300  d_loss: -0.383916  g_loss: 1.038095\n",
      "Elapsed: [5:59:40.555259]  batch: 350  d_loss: -0.430411  g_loss: 0.611530\n",
      "Elapsed: [5:59:53.330523]  batch: 400  d_loss: -0.084037  g_loss: 0.280813\n",
      "Elapsed: [6:00:06.050358]  batch: 450  d_loss: -0.580927  g_loss: 0.689515\n",
      "Elapsed: [6:00:18.911210]  batch: 500  d_loss: -0.338376  g_loss: 0.969285\n",
      "Time taken for epoch: 142.404 secs\n",
      "ticker =  13001\n",
      "\n",
      "Epoch: 27\n",
      "Elapsed: [6:00:35.638546]  batch: 1  d_loss: -0.216942  g_loss: 0.756606\n",
      "Elapsed: [6:00:47.118812]  batch: 50  d_loss: -0.320910  g_loss: 0.504502\n",
      "Elapsed: [6:00:59.510110]  batch: 100  d_loss: -0.364175  g_loss: 0.686161\n",
      "Elapsed: [6:01:12.273042]  batch: 150  d_loss: -0.018248  g_loss: 0.561703\n",
      "Elapsed: [6:01:24.773726]  batch: 200  d_loss: -0.425980  g_loss: 0.754428\n",
      "Elapsed: [6:01:37.417415]  batch: 250  d_loss: -0.004991  g_loss: 0.392787\n",
      "Elapsed: [6:01:50.001521]  batch: 300  d_loss: -0.512318  g_loss: 0.938320\n",
      "Elapsed: [6:02:02.671500]  batch: 350  d_loss: -0.383042  g_loss: 0.959413\n",
      "Elapsed: [6:02:15.421325]  batch: 400  d_loss: -0.265198  g_loss: 0.511193\n",
      "Elapsed: [6:02:28.221683]  batch: 450  d_loss: -0.420120  g_loss: 0.938207\n",
      "Elapsed: [6:02:40.880498]  batch: 500  d_loss: -0.092687  g_loss: 0.684951\n",
      "Time taken for epoch: 141.828 secs\n",
      "ticker =  13501\n",
      "\n",
      "Epoch: 28\n",
      "Elapsed: [6:02:57.857858]  batch: 1  d_loss: -0.168559  g_loss: 0.747631\n",
      "Elapsed: [6:03:09.400971]  batch: 50  d_loss: -0.327616  g_loss: 0.510911\n",
      "Elapsed: [6:03:21.795688]  batch: 100  d_loss: -0.074099  g_loss: 0.648870\n",
      "Elapsed: [6:03:34.533677]  batch: 150  d_loss: -0.479471  g_loss: 0.614520\n",
      "Elapsed: [6:03:47.205621]  batch: 200  d_loss: -0.067333  g_loss: 0.192262\n",
      "Elapsed: [6:04:00.028891]  batch: 250  d_loss: -0.128600  g_loss: 0.337241\n",
      "Elapsed: [6:04:12.922518]  batch: 300  d_loss: -0.250828  g_loss: 0.520910\n",
      "Elapsed: [6:04:26.356205]  batch: 350  d_loss: -0.449566  g_loss: 0.844827\n",
      "Elapsed: [6:04:39.198314]  batch: 400  d_loss: -0.553747  g_loss: 0.853302\n",
      "Elapsed: [6:04:52.111438]  batch: 450  d_loss: -0.003312  g_loss: 0.286903\n",
      "Elapsed: [6:05:05.080156]  batch: 500  d_loss: -0.107851  g_loss: 0.588380\n",
      "Time taken for epoch: 143.971 secs\n",
      "ticker =  14001\n",
      "\n",
      "Epoch: 29\n",
      "Elapsed: [6:05:21.933656]  batch: 1  d_loss: 0.211568  g_loss: 0.052403\n",
      "Elapsed: [6:05:33.372994]  batch: 50  d_loss: -0.443937  g_loss: 0.564062\n",
      "Elapsed: [6:05:45.855270]  batch: 100  d_loss: 0.030221  g_loss: 0.109818\n",
      "Elapsed: [6:05:58.441894]  batch: 150  d_loss: 0.012030  g_loss: 0.325997\n",
      "Elapsed: [6:06:11.103119]  batch: 200  d_loss: -0.266429  g_loss: 0.677279\n",
      "Elapsed: [6:06:24.093451]  batch: 250  d_loss: -0.002955  g_loss: 0.294702\n",
      "Elapsed: [6:06:36.696810]  batch: 300  d_loss: -0.041909  g_loss: 0.304818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: [6:06:49.546155]  batch: 350  d_loss: -0.238031  g_loss: 0.748288\n",
      "Elapsed: [6:07:02.188526]  batch: 400  d_loss: -0.042082  g_loss: 0.398411\n",
      "Elapsed: [6:07:15.003726]  batch: 450  d_loss: 0.076049  g_loss: 0.129472\n",
      "Elapsed: [6:07:27.707273]  batch: 500  d_loss: -0.175315  g_loss: 0.589691\n",
      "Time taken for epoch: 142.517 secs\n",
      "ticker =  14501\n",
      "\n",
      "Epoch: 30\n",
      "Elapsed: [6:07:44.768502]  batch: 1  d_loss: -0.147321  g_loss: 0.458575\n",
      "Elapsed: [6:07:56.362330]  batch: 50  d_loss: -0.423624  g_loss: 0.567875\n",
      "Elapsed: [6:08:08.858032]  batch: 100  d_loss: -0.156252  g_loss: 0.517627\n",
      "Elapsed: [6:08:21.549299]  batch: 150  d_loss: -0.099882  g_loss: 0.475799\n",
      "Elapsed: [6:08:35.330286]  batch: 200  d_loss: -0.418112  g_loss: 0.651347\n",
      "Elapsed: [6:08:48.378050]  batch: 250  d_loss: 0.041613  g_loss: 0.010311\n",
      "Elapsed: [6:09:01.085640]  batch: 300  d_loss: -0.048922  g_loss: 0.388881\n",
      "Elapsed: [6:09:14.198472]  batch: 350  d_loss: 0.062585  g_loss: 0.093108\n",
      "Elapsed: [6:09:26.948677]  batch: 400  d_loss: -0.162076  g_loss: 0.562622\n",
      "Elapsed: [6:09:39.606895]  batch: 450  d_loss: 0.105780  g_loss: -0.041116\n",
      "Elapsed: [6:09:52.298796]  batch: 500  d_loss: -0.070079  g_loss: 0.522677\n",
      "Time taken for epoch: 144.365 secs\n",
      "ticker =  15001\n",
      "\n",
      "Epoch: 31\n",
      "Elapsed: [6:10:09.017610]  batch: 1  d_loss: -0.268971  g_loss: 0.615530\n",
      "Elapsed: [6:10:20.503876]  batch: 50  d_loss: -0.028604  g_loss: 0.003168\n",
      "Elapsed: [6:10:33.147595]  batch: 100  d_loss: -0.311522  g_loss: 0.641852\n",
      "Elapsed: [6:10:46.347740]  batch: 150  d_loss: -0.126743  g_loss: 0.393652\n",
      "Elapsed: [6:10:58.922918]  batch: 200  d_loss: -0.330166  g_loss: 0.597664\n",
      "Elapsed: [6:11:11.608508]  batch: 250  d_loss: 0.029934  g_loss: 0.183669\n",
      "Elapsed: [6:11:24.165601]  batch: 300  d_loss: 0.050994  g_loss: 0.216892\n",
      "Elapsed: [6:11:36.810120]  batch: 350  d_loss: -0.014843  g_loss: 0.555553\n",
      "Elapsed: [6:11:49.484396]  batch: 400  d_loss: -0.532305  g_loss: 0.795140\n",
      "Elapsed: [6:12:02.417738]  batch: 450  d_loss: -0.279056  g_loss: 0.651573\n",
      "Elapsed: [6:12:15.029042]  batch: 500  d_loss: -0.057522  g_loss: 0.409662\n",
      "Time taken for epoch: 142.593 secs\n",
      "ticker =  15501\n",
      "\n",
      "Epoch: 32\n",
      "Elapsed: [6:12:31.997263]  batch: 1  d_loss: -0.007196  g_loss: 0.429402\n",
      "Elapsed: [6:12:43.373914]  batch: 50  d_loss: -0.331210  g_loss: 0.611955\n",
      "Elapsed: [6:12:56.004450]  batch: 100  d_loss: -0.330945  g_loss: 0.757361\n",
      "Elapsed: [6:13:08.612174]  batch: 150  d_loss: -0.088776  g_loss: 0.388544\n",
      "Elapsed: [6:13:21.277797]  batch: 200  d_loss: -0.510315  g_loss: 0.642814\n",
      "Elapsed: [6:13:34.013985]  batch: 250  d_loss: 0.054656  g_loss: 0.010624\n",
      "Elapsed: [6:13:46.623594]  batch: 300  d_loss: -0.343326  g_loss: 0.761087\n",
      "Elapsed: [6:13:59.323315]  batch: 350  d_loss: 0.051520  g_loss: 0.107909\n",
      "Elapsed: [6:14:11.997110]  batch: 400  d_loss: -0.082026  g_loss: 0.435795\n",
      "Elapsed: [6:14:24.952775]  batch: 450  d_loss: -0.174752  g_loss: 0.451542\n",
      "Elapsed: [6:14:37.616484]  batch: 500  d_loss: -0.194333  g_loss: 0.682650\n",
      "Time taken for epoch: 142.350 secs\n",
      "ticker =  16001\n",
      "\n",
      "Epoch: 33\n",
      "Elapsed: [6:14:54.215534]  batch: 1  d_loss: -0.120038  g_loss: 0.513195\n",
      "Elapsed: [6:15:05.795974]  batch: 50  d_loss: 0.013827  g_loss: 0.369141\n",
      "Elapsed: [6:15:18.384618]  batch: 100  d_loss: -0.076872  g_loss: 0.355191\n",
      "Elapsed: [6:15:31.056301]  batch: 150  d_loss: -0.311839  g_loss: 0.478507\n",
      "Elapsed: [6:15:43.802242]  batch: 200  d_loss: 0.014586  g_loss: 0.046122\n",
      "Elapsed: [6:15:56.684569]  batch: 250  d_loss: -0.237551  g_loss: 0.476014\n",
      "Elapsed: [6:16:09.448870]  batch: 300  d_loss: -0.439220  g_loss: 0.759097\n",
      "Elapsed: [6:16:22.130771]  batch: 350  d_loss: -0.263038  g_loss: 0.614393\n",
      "Elapsed: [6:16:34.976532]  batch: 400  d_loss: -0.072904  g_loss: 0.396142\n",
      "Elapsed: [6:16:47.692961]  batch: 450  d_loss: -0.140006  g_loss: 0.456459\n",
      "Elapsed: [6:17:00.286310]  batch: 500  d_loss: -0.094292  g_loss: 0.326462\n",
      "Time taken for epoch: 142.423 secs\n",
      "ticker =  16501\n",
      "\n",
      "Epoch: 34\n",
      "Elapsed: [6:17:17.451424]  batch: 1  d_loss: -0.268643  g_loss: 0.538692\n",
      "Elapsed: [6:17:28.793928]  batch: 50  d_loss: -0.197825  g_loss: 0.414667\n",
      "Elapsed: [6:17:42.193127]  batch: 100  d_loss: 0.124839  g_loss: 0.056497\n",
      "Elapsed: [6:17:54.797134]  batch: 150  d_loss: -0.133393  g_loss: 0.547390\n",
      "Elapsed: [6:18:07.445376]  batch: 200  d_loss: 0.017170  g_loss: 0.369876\n",
      "Elapsed: [6:18:20.235351]  batch: 250  d_loss: -0.220652  g_loss: 0.614474\n",
      "Elapsed: [6:18:34.213103]  batch: 300  d_loss: -0.181769  g_loss: 0.435526\n",
      "Elapsed: [6:18:46.934699]  batch: 350  d_loss: -0.353850  g_loss: 0.457572\n",
      "Elapsed: [6:18:59.772440]  batch: 400  d_loss: -0.071022  g_loss: 0.250959\n",
      "Elapsed: [6:19:12.502237]  batch: 450  d_loss: -0.193685  g_loss: 0.503906\n",
      "Elapsed: [6:19:25.249782]  batch: 500  d_loss: -0.264741  g_loss: 0.665979\n",
      "Time taken for epoch: 144.844 secs\n",
      "ticker =  17001\n",
      "\n",
      "Epoch: 35\n",
      "Elapsed: [6:19:42.514675]  batch: 1  d_loss: 0.007791  g_loss: 0.303779\n",
      "Elapsed: [6:19:53.967615]  batch: 50  d_loss: 0.000872  g_loss: 0.236917\n",
      "Elapsed: [6:20:06.385914]  batch: 100  d_loss: -0.228374  g_loss: 0.662467\n",
      "Elapsed: [6:20:19.093174]  batch: 150  d_loss: -0.105086  g_loss: 0.299571\n",
      "Elapsed: [6:20:31.716839]  batch: 200  d_loss: -0.136767  g_loss: 0.502937\n",
      "Elapsed: [6:20:44.400421]  batch: 250  d_loss: 0.082387  g_loss: 0.227391\n",
      "Elapsed: [6:20:57.044720]  batch: 300  d_loss: -0.398384  g_loss: 0.650490\n",
      "Elapsed: [6:21:09.799915]  batch: 350  d_loss: -0.348335  g_loss: 0.585259\n",
      "Elapsed: [6:21:22.453800]  batch: 400  d_loss: 0.078438  g_loss: 0.284031\n",
      "Elapsed: [6:21:35.264760]  batch: 450  d_loss: 0.074223  g_loss: -0.092884\n",
      "Elapsed: [6:21:48.259158]  batch: 500  d_loss: -0.206327  g_loss: 0.257162\n",
      "Time taken for epoch: 142.731 secs\n",
      "ticker =  17501\n",
      "\n",
      "Epoch: 36\n",
      "Elapsed: [6:22:05.307346]  batch: 1  d_loss: -0.225920  g_loss: 0.472328\n",
      "Elapsed: [6:22:16.774590]  batch: 50  d_loss: 0.059774  g_loss: 0.178249\n",
      "Elapsed: [6:22:29.153214]  batch: 100  d_loss: -0.033749  g_loss: 0.369826\n",
      "Elapsed: [6:22:42.068924]  batch: 150  d_loss: -0.116699  g_loss: 0.334623\n",
      "Elapsed: [6:22:54.666291]  batch: 200  d_loss: -0.027145  g_loss: 0.303943\n",
      "Elapsed: [6:23:07.519701]  batch: 250  d_loss: 0.051151  g_loss: 0.301214\n",
      "Elapsed: [6:23:20.196259]  batch: 300  d_loss: -0.207209  g_loss: 0.505261\n",
      "Elapsed: [6:23:32.921634]  batch: 350  d_loss: 0.014535  g_loss: 0.244145\n",
      "Elapsed: [6:23:45.813317]  batch: 400  d_loss: 0.108646  g_loss: 0.011852\n",
      "Elapsed: [6:23:58.588064]  batch: 450  d_loss: -0.051591  g_loss: 0.385532\n",
      "Elapsed: [6:24:11.413060]  batch: 500  d_loss: -0.188143  g_loss: 0.439346\n",
      "Time taken for epoch: 143.039 secs\n",
      "ticker =  18001\n",
      "\n",
      "Epoch: 37\n",
      "Elapsed: [6:24:29.127697]  batch: 1  d_loss: -0.100357  g_loss: 0.285239\n",
      "Elapsed: [6:24:40.369423]  batch: 50  d_loss: -0.126938  g_loss: 0.623579\n",
      "Elapsed: [6:24:52.703729]  batch: 100  d_loss: 0.043469  g_loss: 0.268183\n",
      "Elapsed: [6:25:05.576890]  batch: 150  d_loss: -0.128524  g_loss: 0.418742\n",
      "Elapsed: [6:25:18.316236]  batch: 200  d_loss: -0.211241  g_loss: 0.242777\n",
      "Elapsed: [6:25:31.066672]  batch: 250  d_loss: -0.227175  g_loss: 0.596681\n",
      "Elapsed: [6:25:44.382504]  batch: 300  d_loss: -0.059122  g_loss: 0.360359\n",
      "Elapsed: [6:25:57.151013]  batch: 350  d_loss: 0.052578  g_loss: 0.104001\n",
      "Elapsed: [6:26:10.011074]  batch: 400  d_loss: -0.161896  g_loss: 0.441997\n",
      "Elapsed: [6:26:22.745992]  batch: 450  d_loss: -0.386268  g_loss: 0.655661\n",
      "Elapsed: [6:26:35.451711]  batch: 500  d_loss: -0.045069  g_loss: 0.470674\n",
      "Time taken for epoch: 143.802 secs\n",
      "ticker =  18501\n",
      "\n",
      "Epoch: 38\n",
      "Elapsed: [6:26:52.583664]  batch: 1  d_loss: -0.126774  g_loss: 0.472268\n",
      "Elapsed: [6:27:03.867065]  batch: 50  d_loss: -0.064362  g_loss: 0.249940\n",
      "Elapsed: [6:27:16.253411]  batch: 100  d_loss: 0.102328  g_loss: -0.010411\n",
      "Elapsed: [6:27:29.121971]  batch: 150  d_loss: 0.058735  g_loss: 0.029640\n",
      "Elapsed: [6:27:41.899596]  batch: 200  d_loss: -0.282964  g_loss: 0.666761\n",
      "Elapsed: [6:27:54.443604]  batch: 250  d_loss: -0.173603  g_loss: 0.467110\n",
      "Elapsed: [6:28:07.163994]  batch: 300  d_loss: -0.181714  g_loss: 0.503784\n",
      "Elapsed: [6:28:19.842108]  batch: 350  d_loss: -0.100389  g_loss: 0.456125\n",
      "Elapsed: [6:28:32.520076]  batch: 400  d_loss: 0.023229  g_loss: 0.206342\n",
      "Elapsed: [6:28:45.257151]  batch: 450  d_loss: -0.191140  g_loss: 0.486921\n",
      "Elapsed: [6:28:57.907468]  batch: 500  d_loss: -0.102145  g_loss: 0.387556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for epoch: 142.252 secs\n",
      "ticker =  19001\n",
      "\n",
      "Epoch: 39\n",
      "Elapsed: [6:29:19.791024]  batch: 1  d_loss: -0.080553  g_loss: 0.400126\n",
      "Elapsed: [6:29:30.747249]  batch: 50  d_loss: -0.267824  g_loss: 0.662455\n",
      "Elapsed: [6:29:42.585615]  batch: 100  d_loss: -0.210170  g_loss: 0.362602\n",
      "Elapsed: [6:29:54.824900]  batch: 150  d_loss: -0.044232  g_loss: 0.331736\n",
      "Elapsed: [6:30:07.797970]  batch: 200  d_loss: 0.121361  g_loss: -0.068997\n",
      "Elapsed: [6:30:20.627598]  batch: 250  d_loss: 0.028897  g_loss: 0.179678\n",
      "Elapsed: [6:30:33.494170]  batch: 300  d_loss: 0.007000  g_loss: 0.279419\n",
      "Elapsed: [6:30:46.256006]  batch: 350  d_loss: 0.118768  g_loss: 0.090685\n",
      "Elapsed: [6:30:59.014637]  batch: 400  d_loss: -0.195365  g_loss: 0.484943\n",
      "Elapsed: [6:31:11.886007]  batch: 450  d_loss: -0.142285  g_loss: 0.328850\n",
      "Elapsed: [6:31:24.653733]  batch: 500  d_loss: -0.210750  g_loss: 0.353764\n",
      "Time taken for epoch: 146.515 secs\n",
      "ticker =  19501\n",
      "\n",
      "Epoch: 40\n",
      "Elapsed: [6:31:41.918509]  batch: 1  d_loss: -0.163395  g_loss: 0.492385\n",
      "Elapsed: [6:31:53.561249]  batch: 50  d_loss: -0.237321  g_loss: 0.459420\n",
      "Elapsed: [6:32:06.052832]  batch: 100  d_loss: -0.020145  g_loss: 0.184146\n",
      "Elapsed: [6:32:18.660301]  batch: 150  d_loss: -0.010660  g_loss: 0.331173\n",
      "Elapsed: [6:32:31.365424]  batch: 200  d_loss: -0.196318  g_loss: 0.504041\n",
      "Elapsed: [6:32:44.009702]  batch: 250  d_loss: 0.086356  g_loss: 0.076447\n",
      "Elapsed: [6:32:56.568939]  batch: 300  d_loss: 0.021131  g_loss: 0.159686\n",
      "Elapsed: [6:33:09.246092]  batch: 350  d_loss: 0.014656  g_loss: 0.169384\n",
      "Elapsed: [6:33:22.012754]  batch: 400  d_loss: -0.351557  g_loss: 0.699224\n",
      "Elapsed: [6:33:34.729692]  batch: 450  d_loss: -0.070942  g_loss: 0.342961\n",
      "Elapsed: [6:33:47.461007]  batch: 500  d_loss: 0.069125  g_loss: -0.069568\n",
      "Time taken for epoch: 142.649 secs\n",
      "ticker =  20001\n",
      "\n",
      "Epoch: 41\n",
      "Elapsed: [6:34:04.528810]  batch: 1  d_loss: -0.089439  g_loss: 0.269621\n",
      "Elapsed: [6:34:16.233576]  batch: 50  d_loss: -0.011071  g_loss: 0.122083\n",
      "Elapsed: [6:34:28.821104]  batch: 100  d_loss: 0.000870  g_loss: 0.262339\n",
      "Elapsed: [6:34:41.556240]  batch: 150  d_loss: -0.357707  g_loss: 0.539460\n",
      "Elapsed: [6:34:54.475911]  batch: 200  d_loss: -0.289563  g_loss: 0.625170\n",
      "Elapsed: [6:35:07.069309]  batch: 250  d_loss: -0.187585  g_loss: 0.503076\n",
      "Elapsed: [6:35:19.912654]  batch: 300  d_loss: -0.007471  g_loss: 0.306973\n",
      "Elapsed: [6:35:32.708225]  batch: 350  d_loss: -0.349162  g_loss: 0.567970\n",
      "Elapsed: [6:35:45.522062]  batch: 400  d_loss: 0.009924  g_loss: 0.207605\n",
      "Elapsed: [6:35:58.323709]  batch: 450  d_loss: -0.005960  g_loss: 0.423218\n",
      "Elapsed: [6:36:11.148393]  batch: 500  d_loss: -0.148728  g_loss: 0.458784\n",
      "Time taken for epoch: 143.472 secs\n",
      "ticker =  20501\n",
      "\n",
      "Epoch: 42\n",
      "Elapsed: [6:36:28.145432]  batch: 1  d_loss: -0.080425  g_loss: 0.365706\n",
      "Elapsed: [6:36:39.636326]  batch: 50  d_loss: 0.029308  g_loss: 0.109223\n",
      "Elapsed: [6:36:52.271406]  batch: 100  d_loss: 0.081383  g_loss: 0.232411\n",
      "Elapsed: [6:37:04.910269]  batch: 150  d_loss: 0.047887  g_loss: 0.151370\n",
      "Elapsed: [6:37:17.545436]  batch: 200  d_loss: -0.366599  g_loss: 0.627227\n",
      "Elapsed: [6:37:30.258274]  batch: 250  d_loss: -0.166426  g_loss: 0.440221\n",
      "Elapsed: [6:37:42.986699]  batch: 300  d_loss: -0.008191  g_loss: 0.249567\n",
      "Elapsed: [6:37:55.617003]  batch: 350  d_loss: -0.006726  g_loss: 0.393213\n",
      "Elapsed: [6:38:08.414430]  batch: 400  d_loss: -0.196550  g_loss: 0.481329\n",
      "Elapsed: [6:38:21.310241]  batch: 450  d_loss: -0.154732  g_loss: 0.419395\n",
      "Elapsed: [6:38:33.860177]  batch: 500  d_loss: -0.055821  g_loss: 0.318261\n",
      "Time taken for epoch: 142.678 secs\n",
      "ticker =  21001\n",
      "\n",
      "Epoch: 43\n",
      "Elapsed: [6:38:50.778243]  batch: 1  d_loss: -0.264970  g_loss: 0.649340\n",
      "Elapsed: [6:39:02.134095]  batch: 50  d_loss: -0.108152  g_loss: 0.375922\n",
      "Elapsed: [6:39:14.717893]  batch: 100  d_loss: -0.246704  g_loss: 0.661709\n",
      "Elapsed: [6:39:27.348616]  batch: 150  d_loss: -0.116107  g_loss: 0.223202\n",
      "Elapsed: [6:39:40.007177]  batch: 200  d_loss: -0.001989  g_loss: 0.290967\n",
      "Elapsed: [6:39:52.717665]  batch: 250  d_loss: 0.072617  g_loss: 0.197194\n",
      "Elapsed: [6:40:05.413702]  batch: 300  d_loss: -0.007172  g_loss: 0.248099\n",
      "Elapsed: [6:40:18.028131]  batch: 350  d_loss: -0.033996  g_loss: 0.159746\n",
      "Elapsed: [6:40:30.685153]  batch: 400  d_loss: -0.296868  g_loss: 0.667059\n",
      "Elapsed: [6:40:43.911148]  batch: 450  d_loss: -0.107221  g_loss: 0.324448\n",
      "Elapsed: [6:40:56.624725]  batch: 500  d_loss: 0.128960  g_loss: 0.126451\n",
      "Time taken for epoch: 142.469 secs\n",
      "ticker =  21501\n",
      "\n",
      "Epoch: 44\n",
      "Elapsed: [6:41:13.592112]  batch: 1  d_loss: -0.136923  g_loss: 0.317317\n",
      "Elapsed: [6:41:24.918833]  batch: 50  d_loss: 0.014081  g_loss: 0.043713\n",
      "Elapsed: [6:41:37.468810]  batch: 100  d_loss: 0.014759  g_loss: -0.000199\n",
      "Elapsed: [6:41:49.990451]  batch: 150  d_loss: -0.286927  g_loss: 0.762666\n",
      "Elapsed: [6:42:02.749516]  batch: 200  d_loss: -0.026020  g_loss: 0.112431\n",
      "Elapsed: [6:42:15.505826]  batch: 250  d_loss: -0.185887  g_loss: 0.487033\n",
      "Elapsed: [6:42:28.346227]  batch: 300  d_loss: -0.422183  g_loss: 0.520435\n",
      "Elapsed: [6:42:41.129735]  batch: 350  d_loss: -0.194526  g_loss: 0.528868\n",
      "Elapsed: [6:42:54.011429]  batch: 400  d_loss: -0.159869  g_loss: 0.343953\n",
      "Elapsed: [6:43:06.629067]  batch: 450  d_loss: -0.160414  g_loss: 0.478208\n",
      "Elapsed: [6:43:19.327889]  batch: 500  d_loss: -0.267630  g_loss: 0.615974\n",
      "Time taken for epoch: 142.476 secs\n",
      "ticker =  22001\n",
      "\n",
      "Epoch: 45\n",
      "Elapsed: [6:43:36.448405]  batch: 1  d_loss: 0.109595  g_loss: -0.056033\n",
      "Elapsed: [6:43:48.035617]  batch: 50  d_loss: -0.072619  g_loss: 0.474507\n",
      "Elapsed: [6:44:00.561915]  batch: 100  d_loss: -0.056077  g_loss: 0.191734\n",
      "Elapsed: [6:44:13.240321]  batch: 150  d_loss: -0.249632  g_loss: 0.379133\n",
      "Elapsed: [6:44:26.223939]  batch: 200  d_loss: 0.004268  g_loss: 0.030594\n",
      "Elapsed: [6:44:39.007075]  batch: 250  d_loss: 0.107717  g_loss: 0.198071\n",
      "Elapsed: [6:44:51.637467]  batch: 300  d_loss: -0.069796  g_loss: 0.185875\n",
      "Elapsed: [6:45:04.336651]  batch: 350  d_loss: 0.113734  g_loss: 0.061013\n",
      "Elapsed: [6:45:17.174224]  batch: 400  d_loss: -0.191702  g_loss: 0.517393\n",
      "Elapsed: [6:45:29.836718]  batch: 450  d_loss: -0.122728  g_loss: 0.512399\n",
      "Elapsed: [6:45:42.536320]  batch: 500  d_loss: -0.219610  g_loss: 0.482854\n",
      "Time taken for epoch: 143.068 secs\n",
      "ticker =  22501\n",
      "\n",
      "Epoch: 46\n",
      "Elapsed: [6:45:59.492080]  batch: 1  d_loss: -0.031643  g_loss: 0.252112\n",
      "Elapsed: [6:46:10.978324]  batch: 50  d_loss: -0.093832  g_loss: 0.373614\n",
      "Elapsed: [6:46:23.539912]  batch: 100  d_loss: 0.076998  g_loss: -0.005424\n",
      "Elapsed: [6:46:36.244239]  batch: 150  d_loss: -0.046917  g_loss: 0.398570\n",
      "Elapsed: [6:46:49.203577]  batch: 200  d_loss: 0.024639  g_loss: 0.090407\n",
      "Elapsed: [6:47:01.847077]  batch: 250  d_loss: -0.277501  g_loss: 0.441407\n",
      "Elapsed: [6:47:14.465811]  batch: 300  d_loss: -0.098411  g_loss: 0.326572\n",
      "Elapsed: [6:47:27.044517]  batch: 350  d_loss: -0.368226  g_loss: 0.458703\n",
      "Elapsed: [6:47:39.767817]  batch: 400  d_loss: -0.249543  g_loss: 0.623839\n",
      "Elapsed: [6:47:52.320753]  batch: 450  d_loss: 0.007975  g_loss: 0.086827\n",
      "Elapsed: [6:48:05.891598]  batch: 500  d_loss: -0.166774  g_loss: 0.430883\n",
      "Time taken for epoch: 143.147 secs\n",
      "ticker =  23001\n",
      "\n",
      "Epoch: 47\n",
      "Elapsed: [6:48:22.553890]  batch: 1  d_loss: -0.145184  g_loss: 0.477334\n",
      "Elapsed: [6:48:34.043158]  batch: 50  d_loss: 0.145246  g_loss: 0.194391\n",
      "Elapsed: [6:48:46.495538]  batch: 100  d_loss: -0.112518  g_loss: 0.208646\n",
      "Elapsed: [6:48:59.238425]  batch: 150  d_loss: 0.068303  g_loss: -0.040017\n",
      "Elapsed: [6:49:11.948313]  batch: 200  d_loss: -0.006811  g_loss: 0.419972\n",
      "Elapsed: [6:49:24.800075]  batch: 250  d_loss: -0.283125  g_loss: 0.664365\n",
      "Elapsed: [6:49:37.565948]  batch: 300  d_loss: -0.166316  g_loss: 0.286356\n",
      "Elapsed: [6:49:50.341548]  batch: 350  d_loss: -0.194567  g_loss: 0.313765\n",
      "Elapsed: [6:50:03.163988]  batch: 400  d_loss: -0.305716  g_loss: 0.462963\n",
      "Elapsed: [6:50:15.949739]  batch: 450  d_loss: -0.308801  g_loss: 0.665194\n",
      "Elapsed: [6:50:29.029514]  batch: 500  d_loss: 0.203157  g_loss: -0.034845\n",
      "Time taken for epoch: 142.933 secs\n",
      "ticker =  23501\n",
      "\n",
      "Epoch: 48\n",
      "Elapsed: [6:50:45.937867]  batch: 1  d_loss: 0.020585  g_loss: 0.345416\n",
      "Elapsed: [6:50:57.244868]  batch: 50  d_loss: -0.021166  g_loss: 0.193937\n",
      "Elapsed: [6:51:09.447934]  batch: 100  d_loss: -0.140759  g_loss: 0.197873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: [6:51:22.230620]  batch: 150  d_loss: 0.048770  g_loss: 0.017078\n",
      "Elapsed: [6:51:34.926478]  batch: 200  d_loss: -0.094637  g_loss: 0.424470\n",
      "Elapsed: [6:51:47.706738]  batch: 250  d_loss: 0.026818  g_loss: 0.181694\n",
      "Elapsed: [6:52:00.530822]  batch: 300  d_loss: -0.317301  g_loss: 0.509398\n",
      "Elapsed: [6:52:13.252432]  batch: 350  d_loss: -0.367969  g_loss: 0.578145\n",
      "Elapsed: [6:52:25.911941]  batch: 400  d_loss: 0.019173  g_loss: 0.074201\n",
      "Elapsed: [6:52:38.835311]  batch: 450  d_loss: -0.404040  g_loss: 0.573694\n",
      "Elapsed: [6:52:51.556816]  batch: 500  d_loss: -0.138194  g_loss: 0.365441\n",
      "Time taken for epoch: 142.359 secs\n",
      "ticker =  24001\n",
      "\n",
      "Epoch: 49\n",
      "Elapsed: [6:53:08.489753]  batch: 1  d_loss: -0.350594  g_loss: 0.565234\n",
      "Elapsed: [6:53:19.683629]  batch: 50  d_loss: -0.183778  g_loss: 0.434727\n",
      "Elapsed: [6:53:32.133182]  batch: 100  d_loss: -0.306417  g_loss: 0.469871\n",
      "Elapsed: [6:53:44.888947]  batch: 150  d_loss: -0.244993  g_loss: 0.524274\n",
      "Elapsed: [6:53:57.555747]  batch: 200  d_loss: -0.012645  g_loss: 0.199246\n",
      "Elapsed: [6:54:10.397662]  batch: 250  d_loss: -0.266164  g_loss: 0.384634\n",
      "Elapsed: [6:54:23.343899]  batch: 300  d_loss: -0.012114  g_loss: 0.340201\n",
      "Elapsed: [6:54:35.972967]  batch: 350  d_loss: -0.286926  g_loss: 0.448627\n",
      "Elapsed: [6:54:48.572123]  batch: 400  d_loss: -0.264243  g_loss: 0.435942\n",
      "Elapsed: [6:55:01.320996]  batch: 450  d_loss: 0.083160  g_loss: 0.220708\n",
      "Elapsed: [6:55:13.989255]  batch: 500  d_loss: -0.338520  g_loss: 0.476820\n",
      "Time taken for epoch: 142.274 secs\n",
      "ticker =  24501\n",
      "\n",
      "Epoch: 50\n",
      "Elapsed: [6:55:31.289535]  batch: 1  d_loss: -0.114139  g_loss: 0.369714\n",
      "Elapsed: [6:55:42.434723]  batch: 50  d_loss: -0.154576  g_loss: 0.317083\n",
      "Elapsed: [6:55:54.767399]  batch: 100  d_loss: -0.019462  g_loss: 0.391996\n",
      "Elapsed: [6:56:07.675429]  batch: 150  d_loss: -0.205754  g_loss: 0.428673\n",
      "Elapsed: [6:56:20.368216]  batch: 200  d_loss: 0.038273  g_loss: -0.051954\n",
      "Elapsed: [6:56:32.895267]  batch: 250  d_loss: -0.367101  g_loss: 0.565520\n",
      "Elapsed: [6:56:45.629615]  batch: 300  d_loss: -0.232375  g_loss: 0.372876\n",
      "Elapsed: [6:56:58.285534]  batch: 350  d_loss: -0.250728  g_loss: 0.636558\n",
      "Elapsed: [6:57:11.046607]  batch: 400  d_loss: -0.279057  g_loss: 0.533492\n",
      "Elapsed: [6:57:23.683286]  batch: 450  d_loss: 0.041213  g_loss: 0.057315\n",
      "Elapsed: [6:57:36.494160]  batch: 500  d_loss: -0.066811  g_loss: 0.342020\n",
      "Time taken for epoch: 142.353 secs\n",
      "ticker =  25001\n",
      "\n",
      "Epoch: 51\n",
      "Elapsed: [6:57:53.445388]  batch: 1  d_loss: -0.125823  g_loss: 0.333186\n",
      "Elapsed: [6:58:04.867260]  batch: 50  d_loss: -0.173994  g_loss: 0.487562\n",
      "Elapsed: [6:58:17.406550]  batch: 100  d_loss: 0.025343  g_loss: 0.112634\n",
      "Elapsed: [6:58:29.889689]  batch: 150  d_loss: -0.083845  g_loss: 0.159681\n",
      "Elapsed: [6:58:42.559974]  batch: 200  d_loss: -0.288060  g_loss: 0.547733\n",
      "Elapsed: [6:58:55.174235]  batch: 250  d_loss: -0.091310  g_loss: 0.346877\n",
      "Elapsed: [6:59:07.912364]  batch: 300  d_loss: -0.113919  g_loss: 0.325511\n",
      "Elapsed: [6:59:20.597467]  batch: 350  d_loss: -0.166415  g_loss: 0.421994\n",
      "Elapsed: [6:59:33.375516]  batch: 400  d_loss: -0.096374  g_loss: 0.358453\n",
      "Elapsed: [6:59:46.074346]  batch: 450  d_loss: -0.347795  g_loss: 0.623469\n",
      "Elapsed: [6:59:58.567980]  batch: 500  d_loss: -0.104831  g_loss: 0.280429\n",
      "Time taken for epoch: 142.019 secs\n",
      "ticker =  25501\n",
      "\n",
      "Epoch: 52\n",
      "Elapsed: [7:00:15.473770]  batch: 1  d_loss: -0.227279  g_loss: 0.444003\n",
      "Elapsed: [7:00:26.834028]  batch: 50  d_loss: -0.091511  g_loss: 0.323326\n",
      "Elapsed: [7:00:39.166584]  batch: 100  d_loss: -0.281384  g_loss: 0.584961\n",
      "Elapsed: [7:00:51.736269]  batch: 150  d_loss: 0.027721  g_loss: 0.074377\n",
      "Elapsed: [7:01:04.464000]  batch: 200  d_loss: -0.098942  g_loss: 0.492249\n",
      "Elapsed: [7:01:17.082219]  batch: 250  d_loss: -0.273812  g_loss: 0.482143\n",
      "Elapsed: [7:01:29.763503]  batch: 300  d_loss: -0.260307  g_loss: 0.553167\n",
      "Elapsed: [7:01:42.498768]  batch: 350  d_loss: -0.195038  g_loss: 0.461327\n",
      "Elapsed: [7:01:55.208537]  batch: 400  d_loss: -0.135909  g_loss: 0.171758\n",
      "Elapsed: [7:02:07.936212]  batch: 450  d_loss: -0.323997  g_loss: 0.491098\n",
      "Elapsed: [7:02:20.751905]  batch: 500  d_loss: -0.214699  g_loss: 0.589889\n",
      "Time taken for epoch: 141.895 secs\n",
      "ticker =  26001\n",
      "\n",
      "Epoch: 53\n",
      "Elapsed: [7:02:37.863147]  batch: 1  d_loss: 0.027609  g_loss: 0.262431\n",
      "Elapsed: [7:02:48.991200]  batch: 50  d_loss: -0.260021  g_loss: 0.609611\n",
      "Elapsed: [7:03:01.470356]  batch: 100  d_loss: 0.089421  g_loss: 0.020932\n",
      "Elapsed: [7:03:14.135863]  batch: 150  d_loss: 0.082818  g_loss: 0.047509\n",
      "Elapsed: [7:03:26.877008]  batch: 200  d_loss: -0.180702  g_loss: 0.336121\n",
      "Elapsed: [7:03:39.596617]  batch: 250  d_loss: -0.363886  g_loss: 0.556598\n",
      "Elapsed: [7:03:52.445969]  batch: 300  d_loss: -0.241584  g_loss: 0.429952\n",
      "Elapsed: [7:04:05.148690]  batch: 350  d_loss: -0.261548  g_loss: 0.736359\n",
      "Elapsed: [7:04:18.568598]  batch: 400  d_loss: -0.173147  g_loss: 0.398029\n",
      "Elapsed: [7:04:31.404247]  batch: 450  d_loss: -0.104439  g_loss: 0.395666\n",
      "Elapsed: [7:04:44.232306]  batch: 500  d_loss: 0.033407  g_loss: 0.179677\n",
      "Time taken for epoch: 143.330 secs\n",
      "ticker =  26501\n",
      "\n",
      "Epoch: 54\n",
      "Elapsed: [7:05:01.290714]  batch: 1  d_loss: -0.108092  g_loss: 0.297857\n",
      "Elapsed: [7:05:12.467320]  batch: 50  d_loss: -0.181265  g_loss: 0.488152\n",
      "Elapsed: [7:05:24.945870]  batch: 100  d_loss: -0.219851  g_loss: 0.430917\n",
      "Elapsed: [7:05:37.689684]  batch: 150  d_loss: -0.062093  g_loss: 0.273488\n",
      "Elapsed: [7:05:50.334265]  batch: 200  d_loss: -0.321490  g_loss: 0.504098\n",
      "Elapsed: [7:06:03.082052]  batch: 250  d_loss: -0.069206  g_loss: 0.194113\n",
      "Elapsed: [7:06:15.936651]  batch: 300  d_loss: -0.088150  g_loss: 0.330462\n",
      "Elapsed: [7:06:28.755919]  batch: 350  d_loss: -0.104234  g_loss: 0.244123\n",
      "Elapsed: [7:06:41.353100]  batch: 400  d_loss: -0.199606  g_loss: 0.480008\n",
      "Elapsed: [7:06:54.052333]  batch: 450  d_loss: -0.098910  g_loss: 0.318611\n",
      "Elapsed: [7:07:06.730712]  batch: 500  d_loss: -0.193698  g_loss: 0.329158\n",
      "Time taken for epoch: 142.378 secs\n",
      "ticker =  27001\n",
      "\n",
      "Epoch: 55\n",
      "Elapsed: [7:07:23.712342]  batch: 1  d_loss: -0.219323  g_loss: 0.424355\n",
      "Elapsed: [7:07:35.210390]  batch: 50  d_loss: -0.114099  g_loss: 0.305085\n",
      "Elapsed: [7:07:47.669067]  batch: 100  d_loss: -0.236419  g_loss: 0.298135\n",
      "Elapsed: [7:08:00.283040]  batch: 150  d_loss: 0.121434  g_loss: 0.196209\n",
      "Elapsed: [7:08:12.984831]  batch: 200  d_loss: -0.045210  g_loss: 0.176148\n",
      "Elapsed: [7:08:25.704973]  batch: 250  d_loss: -0.145643  g_loss: 0.512279\n",
      "Elapsed: [7:08:38.455248]  batch: 300  d_loss: 0.078838  g_loss: 0.093568\n",
      "Elapsed: [7:08:52.139628]  batch: 350  d_loss: -0.171499  g_loss: 0.397971\n",
      "Elapsed: [7:09:05.091279]  batch: 400  d_loss: -0.371729  g_loss: 0.483591\n",
      "Elapsed: [7:09:17.877275]  batch: 450  d_loss: -0.185053  g_loss: 0.395842\n",
      "Elapsed: [7:09:30.811428]  batch: 500  d_loss: 0.055193  g_loss: 0.165054\n",
      "Time taken for epoch: 143.801 secs\n",
      "ticker =  27501\n",
      "\n",
      "Epoch: 56\n",
      "Elapsed: [7:09:47.740428]  batch: 1  d_loss: -0.287758  g_loss: 0.452850\n",
      "Elapsed: [7:09:58.895445]  batch: 50  d_loss: 0.063352  g_loss: 0.115458\n",
      "Elapsed: [7:10:11.059753]  batch: 100  d_loss: -0.132442  g_loss: 0.313903\n",
      "Elapsed: [7:10:23.908408]  batch: 150  d_loss: -0.074348  g_loss: 0.280832\n",
      "Elapsed: [7:10:36.653689]  batch: 200  d_loss: -0.308174  g_loss: 0.508494\n",
      "Elapsed: [7:10:49.689387]  batch: 250  d_loss: -0.138342  g_loss: 0.189179\n",
      "Elapsed: [7:11:02.376814]  batch: 300  d_loss: -0.198722  g_loss: 0.576164\n",
      "Elapsed: [7:11:14.974971]  batch: 350  d_loss: -0.305233  g_loss: 0.356595\n",
      "Elapsed: [7:11:27.579849]  batch: 400  d_loss: -0.037679  g_loss: 0.182821\n",
      "Elapsed: [7:11:40.094557]  batch: 450  d_loss: 0.044482  g_loss: 0.072467\n",
      "Elapsed: [7:11:52.753498]  batch: 500  d_loss: -0.014014  g_loss: 0.107168\n",
      "Time taken for epoch: 141.723 secs\n",
      "ticker =  28001\n",
      "\n",
      "Epoch: 57\n",
      "Elapsed: [7:12:09.521527]  batch: 1  d_loss: -0.260753  g_loss: 0.420051\n",
      "Elapsed: [7:12:21.005791]  batch: 50  d_loss: -0.025481  g_loss: 0.192304\n",
      "Elapsed: [7:12:33.425371]  batch: 100  d_loss: 0.023941  g_loss: 0.128958\n",
      "Elapsed: [7:12:46.193777]  batch: 150  d_loss: -0.211281  g_loss: 0.445330\n",
      "Elapsed: [7:12:58.748677]  batch: 200  d_loss: -0.211175  g_loss: 0.264298\n",
      "Elapsed: [7:13:11.408882]  batch: 250  d_loss: -0.065783  g_loss: 0.333929\n",
      "Elapsed: [7:13:24.293841]  batch: 300  d_loss: -0.206501  g_loss: 0.540070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: [7:13:37.091498]  batch: 350  d_loss: 0.024665  g_loss: 0.146986\n",
      "Elapsed: [7:13:49.789651]  batch: 400  d_loss: -0.129888  g_loss: 0.444701\n",
      "Elapsed: [7:14:02.599669]  batch: 450  d_loss: -0.095129  g_loss: 0.197451\n",
      "Elapsed: [7:14:15.207366]  batch: 500  d_loss: -0.065459  g_loss: 0.360327\n",
      "Time taken for epoch: 142.540 secs\n",
      "ticker =  28501\n",
      "\n",
      "Epoch: 58\n",
      "Elapsed: [7:14:32.720661]  batch: 1  d_loss: -0.292913  g_loss: 0.421530\n",
      "Elapsed: [7:14:43.830002]  batch: 50  d_loss: -0.036429  g_loss: 0.189951\n",
      "Elapsed: [7:14:55.959792]  batch: 100  d_loss: -0.022617  g_loss: 0.275686\n",
      "Elapsed: [7:15:08.645268]  batch: 150  d_loss: 0.040851  g_loss: 0.093661\n",
      "Elapsed: [7:15:21.302357]  batch: 200  d_loss: -0.134570  g_loss: 0.408915\n",
      "Elapsed: [7:15:33.873049]  batch: 250  d_loss: -0.219462  g_loss: 0.407833\n",
      "Elapsed: [7:15:46.556450]  batch: 300  d_loss: -0.097854  g_loss: 0.392712\n",
      "Elapsed: [7:15:59.152599]  batch: 350  d_loss: -0.265004  g_loss: 0.316474\n",
      "Elapsed: [7:16:11.842512]  batch: 400  d_loss: -0.073118  g_loss: 0.230848\n",
      "Elapsed: [7:16:24.388090]  batch: 450  d_loss: -0.317180  g_loss: 0.434994\n",
      "Elapsed: [7:16:37.068439]  batch: 500  d_loss: -0.106703  g_loss: 0.190539\n",
      "Time taken for epoch: 141.336 secs\n",
      "ticker =  29001\n",
      "\n",
      "Epoch: 59\n",
      "Elapsed: [7:16:53.692398]  batch: 1  d_loss: -0.267096  g_loss: 0.478683\n",
      "Elapsed: [7:17:05.244020]  batch: 50  d_loss: -0.029224  g_loss: 0.148952\n",
      "Elapsed: [7:17:17.870957]  batch: 100  d_loss: -0.061751  g_loss: 0.148676\n",
      "Elapsed: [7:17:30.510513]  batch: 150  d_loss: 0.013772  g_loss: 0.075411\n",
      "Elapsed: [7:17:43.172416]  batch: 200  d_loss: 0.036755  g_loss: 0.114965\n",
      "Elapsed: [7:17:55.944738]  batch: 250  d_loss: -0.241990  g_loss: 0.424439\n",
      "Elapsed: [7:18:08.757904]  batch: 300  d_loss: -0.158916  g_loss: 0.316281\n",
      "Elapsed: [7:18:22.393733]  batch: 350  d_loss: 0.017620  g_loss: 0.249989\n",
      "Elapsed: [7:18:34.999141]  batch: 400  d_loss: 0.040266  g_loss: 0.117431\n",
      "Elapsed: [7:18:47.769461]  batch: 450  d_loss: -0.222747  g_loss: 0.405205\n",
      "Elapsed: [7:19:00.456331]  batch: 500  d_loss: -0.134318  g_loss: 0.367308\n",
      "Time taken for epoch: 143.227 secs\n",
      "ticker =  29501\n",
      "\n",
      "Epoch: 60\n",
      "Elapsed: [7:19:17.339595]  batch: 1  d_loss: -0.013092  g_loss: 0.205572\n",
      "Elapsed: [7:19:28.500354]  batch: 50  d_loss: 0.017769  g_loss: 0.239747\n",
      "Elapsed: [7:19:40.815999]  batch: 100  d_loss: -0.217411  g_loss: 0.419468\n",
      "Elapsed: [7:19:53.494477]  batch: 150  d_loss: -0.063903  g_loss: 0.125487\n",
      "Elapsed: [7:20:06.208812]  batch: 200  d_loss: -0.018501  g_loss: 0.298675\n",
      "Elapsed: [7:20:18.927589]  batch: 250  d_loss: -0.201839  g_loss: 0.298450\n",
      "Elapsed: [7:20:31.593443]  batch: 300  d_loss: -0.176166  g_loss: 0.399327\n",
      "Elapsed: [7:20:44.138015]  batch: 350  d_loss: -0.240254  g_loss: 0.442325\n",
      "Elapsed: [7:20:56.808789]  batch: 400  d_loss: -0.188364  g_loss: 0.372930\n",
      "Elapsed: [7:21:09.548722]  batch: 450  d_loss: -0.265097  g_loss: 0.475562\n",
      "Elapsed: [7:21:22.241621]  batch: 500  d_loss: -0.093375  g_loss: 0.228323\n",
      "Time taken for epoch: 141.579 secs\n",
      "ticker =  30001\n",
      "\n",
      "Epoch: 61\n",
      "Elapsed: [7:21:39.301816]  batch: 1  d_loss: -0.149590  g_loss: 0.270460\n",
      "Elapsed: [7:21:50.548375]  batch: 50  d_loss: -0.253594  g_loss: 0.467044\n",
      "Elapsed: [7:22:03.052743]  batch: 100  d_loss: 0.004029  g_loss: 0.094791\n",
      "Elapsed: [7:22:15.851750]  batch: 150  d_loss: -0.139672  g_loss: 0.240101\n",
      "Elapsed: [7:22:28.759709]  batch: 200  d_loss: -0.141125  g_loss: 0.208692\n",
      "Elapsed: [7:22:41.512533]  batch: 250  d_loss: -0.137427  g_loss: 0.239248\n",
      "Elapsed: [7:22:54.258368]  batch: 300  d_loss: 0.000491  g_loss: 0.193910\n",
      "Elapsed: [7:23:06.861157]  batch: 350  d_loss: -0.078457  g_loss: 0.196650\n",
      "Elapsed: [7:23:19.564713]  batch: 400  d_loss: -0.207201  g_loss: 0.386473\n",
      "Elapsed: [7:23:32.102757]  batch: 450  d_loss: -0.049384  g_loss: 0.088518\n",
      "Elapsed: [7:23:44.789966]  batch: 500  d_loss: -0.018146  g_loss: 0.265223\n",
      "Time taken for epoch: 142.523 secs\n",
      "ticker =  30501\n",
      "\n",
      "Epoch: 62\n",
      "Elapsed: [7:24:01.685778]  batch: 1  d_loss: -0.086001  g_loss: 0.277802\n",
      "Elapsed: [7:24:12.999012]  batch: 50  d_loss: -0.023467  g_loss: 0.182373\n",
      "Elapsed: [7:24:25.328493]  batch: 100  d_loss: -0.235225  g_loss: 0.377618\n",
      "Elapsed: [7:24:37.826659]  batch: 150  d_loss: 0.000810  g_loss: 0.262469\n",
      "Elapsed: [7:24:50.472847]  batch: 200  d_loss: -0.132060  g_loss: 0.280300\n",
      "Elapsed: [7:25:03.206601]  batch: 250  d_loss: -0.125484  g_loss: 0.343178\n",
      "Elapsed: [7:25:15.829498]  batch: 300  d_loss: -0.260059  g_loss: 0.371167\n",
      "Elapsed: [7:25:28.560304]  batch: 350  d_loss: 0.050197  g_loss: -0.013159\n",
      "Elapsed: [7:25:41.206993]  batch: 400  d_loss: -0.150156  g_loss: 0.335125\n",
      "Elapsed: [7:25:54.286302]  batch: 450  d_loss: -0.075402  g_loss: 0.081810\n",
      "Elapsed: [7:26:06.979211]  batch: 500  d_loss: -0.155604  g_loss: 0.326518\n",
      "Time taken for epoch: 141.967 secs\n",
      "ticker =  31001\n",
      "\n",
      "Epoch: 63\n",
      "Elapsed: [7:26:24.473818]  batch: 1  d_loss: -0.065692  g_loss: 0.258274\n",
      "Elapsed: [7:26:35.540847]  batch: 50  d_loss: -0.186570  g_loss: 0.351918\n",
      "Elapsed: [7:26:47.617236]  batch: 100  d_loss: -0.119752  g_loss: 0.213037\n",
      "Elapsed: [7:27:00.379964]  batch: 150  d_loss: -0.102606  g_loss: 0.159224\n",
      "Elapsed: [7:27:13.111089]  batch: 200  d_loss: -0.144136  g_loss: 0.316208\n",
      "Elapsed: [7:27:25.809478]  batch: 250  d_loss: -0.213600  g_loss: 0.425963\n",
      "Elapsed: [7:27:38.447953]  batch: 300  d_loss: -0.109488  g_loss: 0.327838\n",
      "Elapsed: [7:27:51.208755]  batch: 350  d_loss: -0.341752  g_loss: 0.496605\n",
      "Elapsed: [7:28:03.840539]  batch: 400  d_loss: -0.101028  g_loss: 0.217141\n",
      "Elapsed: [7:28:17.068057]  batch: 450  d_loss: 0.029455  g_loss: 0.073630\n",
      "Elapsed: [7:28:29.847901]  batch: 500  d_loss: -0.102595  g_loss: 0.268233\n",
      "Time taken for epoch: 142.632 secs\n",
      "ticker =  31501\n",
      "\n",
      "Epoch: 64\n",
      "Elapsed: [7:28:47.015130]  batch: 1  d_loss: -0.057477  g_loss: 0.229286\n",
      "Elapsed: [7:28:58.200215]  batch: 50  d_loss: 0.038434  g_loss: 0.081537\n",
      "Elapsed: [7:29:13.148321]  batch: 100  d_loss: -0.171583  g_loss: 0.509534\n",
      "Elapsed: [7:29:27.863121]  batch: 150  d_loss: -0.156474  g_loss: 0.361168\n",
      "Elapsed: [7:29:41.234270]  batch: 200  d_loss: -0.024827  g_loss: 0.373979\n",
      "Elapsed: [7:29:53.876289]  batch: 250  d_loss: -0.124027  g_loss: 0.277313\n",
      "Elapsed: [7:30:06.516157]  batch: 300  d_loss: -0.091836  g_loss: 0.297292\n",
      "Elapsed: [7:30:19.381314]  batch: 350  d_loss: -0.106449  g_loss: 0.301786\n",
      "Elapsed: [7:30:32.050046]  batch: 400  d_loss: 0.037402  g_loss: 0.088911\n",
      "Elapsed: [7:30:44.701467]  batch: 450  d_loss: -0.111131  g_loss: 0.276076\n",
      "Elapsed: [7:30:57.400193]  batch: 500  d_loss: -0.116409  g_loss: 0.226350\n",
      "Time taken for epoch: 147.350 secs\n",
      "ticker =  32001\n",
      "\n",
      "Epoch: 65\n",
      "Elapsed: [7:31:14.381636]  batch: 1  d_loss: -0.114647  g_loss: 0.222000\n",
      "Elapsed: [7:31:25.681466]  batch: 50  d_loss: -0.035985  g_loss: 0.163438\n",
      "Elapsed: [7:31:37.947041]  batch: 100  d_loss: -0.049035  g_loss: 0.182297\n",
      "Elapsed: [7:31:50.690154]  batch: 150  d_loss: -0.214430  g_loss: 0.374866\n",
      "Elapsed: [7:32:03.280838]  batch: 200  d_loss: -0.116492  g_loss: 0.325603\n",
      "Elapsed: [7:32:15.891619]  batch: 250  d_loss: -0.034479  g_loss: 0.035617\n",
      "Elapsed: [7:32:28.473058]  batch: 300  d_loss: -0.199727  g_loss: 0.403801\n",
      "Elapsed: [7:32:41.168923]  batch: 350  d_loss: 0.019789  g_loss: 0.121653\n",
      "Elapsed: [7:32:53.833822]  batch: 400  d_loss: -0.104600  g_loss: 0.190019\n",
      "Elapsed: [7:33:06.619703]  batch: 450  d_loss: -0.056659  g_loss: 0.273143\n",
      "Elapsed: [7:33:19.301182]  batch: 500  d_loss: -0.021338  g_loss: 0.239161\n",
      "Time taken for epoch: 141.613 secs\n",
      "ticker =  32501\n",
      "\n",
      "Epoch: 66\n",
      "Elapsed: [7:33:36.330619]  batch: 1  d_loss: -0.202419  g_loss: 0.398707\n",
      "Elapsed: [7:33:47.818516]  batch: 50  d_loss: -0.145301  g_loss: 0.437100\n",
      "Elapsed: [7:34:00.364675]  batch: 100  d_loss: -0.072412  g_loss: 0.224000\n",
      "Elapsed: [7:34:13.143406]  batch: 150  d_loss: -0.167606  g_loss: 0.322901\n",
      "Elapsed: [7:34:25.951259]  batch: 200  d_loss: 0.043596  g_loss: 0.010568\n",
      "Elapsed: [7:34:38.576938]  batch: 250  d_loss: -0.083422  g_loss: 0.380418\n",
      "Elapsed: [7:34:51.197328]  batch: 300  d_loss: -0.080365  g_loss: 0.225805\n",
      "Elapsed: [7:35:03.953096]  batch: 350  d_loss: -0.127034  g_loss: 0.245194\n",
      "Elapsed: [7:35:16.545595]  batch: 400  d_loss: -0.095480  g_loss: 0.292099\n",
      "Elapsed: [7:35:29.369684]  batch: 450  d_loss: -0.120491  g_loss: 0.294864\n",
      "Elapsed: [7:35:42.034752]  batch: 500  d_loss: -0.201730  g_loss: 0.332471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for epoch: 142.571 secs\n",
      "ticker =  33001\n",
      "\n",
      "Epoch: 67\n",
      "Elapsed: [7:35:58.974824]  batch: 1  d_loss: -0.080835  g_loss: 0.305438\n",
      "Elapsed: [7:36:10.205568]  batch: 50  d_loss: 0.070750  g_loss: 0.028266\n",
      "Elapsed: [7:36:22.437345]  batch: 100  d_loss: -0.216420  g_loss: 0.390110\n",
      "Elapsed: [7:36:35.180236]  batch: 150  d_loss: -0.024789  g_loss: 0.185009\n",
      "Elapsed: [7:36:47.876432]  batch: 200  d_loss: -0.096176  g_loss: 0.216639\n",
      "Elapsed: [7:37:00.618474]  batch: 250  d_loss: -0.123482  g_loss: 0.295250\n",
      "Elapsed: [7:37:13.206842]  batch: 300  d_loss: -0.091541  g_loss: 0.254847\n",
      "Elapsed: [7:37:25.873633]  batch: 350  d_loss: -0.007837  g_loss: 0.059104\n",
      "Elapsed: [7:37:38.454824]  batch: 400  d_loss: -0.249927  g_loss: 0.372101\n",
      "Elapsed: [7:37:51.120561]  batch: 450  d_loss: -0.066037  g_loss: 0.204495\n",
      "Elapsed: [7:38:03.809439]  batch: 500  d_loss: -0.045463  g_loss: 0.091734\n",
      "Time taken for epoch: 141.638 secs\n",
      "ticker =  33501\n",
      "\n",
      "Epoch: 68\n",
      "Elapsed: [7:38:21.131959]  batch: 1  d_loss: -0.158709  g_loss: 0.185953\n",
      "Elapsed: [7:38:32.466173]  batch: 50  d_loss: 0.074196  g_loss: 0.161590\n",
      "Elapsed: [7:38:44.664796]  batch: 100  d_loss: -0.037971  g_loss: 0.136542\n",
      "Elapsed: [7:38:57.411646]  batch: 150  d_loss: -0.096283  g_loss: 0.354564\n",
      "Elapsed: [7:39:10.042077]  batch: 200  d_loss: -0.254085  g_loss: 0.545910\n",
      "Elapsed: [7:39:22.752977]  batch: 250  d_loss: -0.123752  g_loss: 0.219766\n",
      "Elapsed: [7:39:35.256671]  batch: 300  d_loss: -0.303559  g_loss: 0.454346\n",
      "Elapsed: [7:39:47.916883]  batch: 350  d_loss: -0.142828  g_loss: 0.378608\n",
      "Elapsed: [7:40:00.548177]  batch: 400  d_loss: -0.140889  g_loss: 0.350765\n",
      "Elapsed: [7:40:13.280087]  batch: 450  d_loss: -0.105480  g_loss: 0.330202\n",
      "Elapsed: [7:40:25.938151]  batch: 500  d_loss: -0.157915  g_loss: 0.344494\n",
      "Time taken for epoch: 141.843 secs\n",
      "ticker =  34001\n",
      "\n",
      "Epoch: 69\n",
      "Elapsed: [7:40:42.517488]  batch: 1  d_loss: 0.007667  g_loss: 0.089895\n",
      "Elapsed: [7:40:54.152339]  batch: 50  d_loss: -0.151380  g_loss: 0.197442\n",
      "Elapsed: [7:41:06.645842]  batch: 100  d_loss: -0.001835  g_loss: 0.192516\n",
      "Elapsed: [7:41:19.175782]  batch: 150  d_loss: -0.166873  g_loss: 0.365816\n",
      "Elapsed: [7:41:31.657072]  batch: 200  d_loss: -0.051909  g_loss: 0.270424\n",
      "Elapsed: [7:41:44.176845]  batch: 250  d_loss: -0.087516  g_loss: 0.316780\n",
      "Elapsed: [7:41:56.743155]  batch: 300  d_loss: -0.118958  g_loss: 0.274583\n",
      "Elapsed: [7:42:09.531974]  batch: 350  d_loss: 0.034269  g_loss: 0.123369\n",
      "Elapsed: [7:42:22.596356]  batch: 400  d_loss: -0.165026  g_loss: 0.384562\n",
      "Elapsed: [7:42:35.209746]  batch: 450  d_loss: -0.181441  g_loss: 0.328129\n",
      "Elapsed: [7:42:47.849210]  batch: 500  d_loss: 0.011688  g_loss: 0.216831\n",
      "Time taken for epoch: 141.839 secs\n",
      "ticker =  34501\n",
      "\n",
      "Epoch: 70\n",
      "Elapsed: [7:43:04.518529]  batch: 1  d_loss: -0.216514  g_loss: 0.363044\n",
      "Elapsed: [7:43:15.833275]  batch: 50  d_loss: -0.242533  g_loss: 0.357152\n",
      "Elapsed: [7:43:28.166934]  batch: 100  d_loss: -0.256156  g_loss: 0.440454\n",
      "Elapsed: [7:43:40.861220]  batch: 150  d_loss: 0.065756  g_loss: -0.004174\n",
      "Elapsed: [7:43:53.527045]  batch: 200  d_loss: -0.276809  g_loss: 0.490006\n",
      "Elapsed: [7:44:06.169873]  batch: 250  d_loss: -0.309507  g_loss: 0.462986\n",
      "Elapsed: [7:44:19.116613]  batch: 300  d_loss: -0.318991  g_loss: 0.361518\n",
      "Elapsed: [7:44:31.811777]  batch: 350  d_loss: -0.087678  g_loss: 0.202696\n",
      "Elapsed: [7:44:44.336561]  batch: 400  d_loss: -0.287418  g_loss: 0.449047\n",
      "Elapsed: [7:44:57.102179]  batch: 450  d_loss: -0.085461  g_loss: 0.251465\n",
      "Elapsed: [7:45:09.918847]  batch: 500  d_loss: -0.219599  g_loss: 0.421736\n",
      "Time taken for epoch: 141.766 secs\n",
      "ticker =  35001\n",
      "\n",
      "Epoch: 71\n",
      "Elapsed: [7:45:26.678215]  batch: 1  d_loss: -0.063086  g_loss: 0.269378\n",
      "Elapsed: [7:45:37.859520]  batch: 50  d_loss: -0.226409  g_loss: 0.381583\n",
      "Elapsed: [7:45:50.136548]  batch: 100  d_loss: -0.291011  g_loss: 0.454295\n",
      "Elapsed: [7:46:02.982578]  batch: 150  d_loss: -0.019919  g_loss: 0.220616\n",
      "Elapsed: [7:46:15.725723]  batch: 200  d_loss: -0.088845  g_loss: 0.275353\n",
      "Elapsed: [7:46:28.423317]  batch: 250  d_loss: -0.102788  g_loss: 0.257276\n",
      "Elapsed: [7:46:41.034796]  batch: 300  d_loss: -0.033475  g_loss: 0.106816\n",
      "Elapsed: [7:46:53.621668]  batch: 350  d_loss: -0.030968  g_loss: 0.229325\n",
      "Elapsed: [7:47:06.357589]  batch: 400  d_loss: -0.343107  g_loss: 0.454382\n",
      "Elapsed: [7:47:18.919715]  batch: 450  d_loss: -0.132390  g_loss: 0.304269\n",
      "Elapsed: [7:47:31.495810]  batch: 500  d_loss: 0.013113  g_loss: 0.195647\n",
      "Time taken for epoch: 141.465 secs\n",
      "ticker =  35501\n",
      "\n",
      "Epoch: 72\n",
      "Elapsed: [7:47:48.064723]  batch: 1  d_loss: -0.210398  g_loss: 0.413239\n",
      "Elapsed: [7:47:59.559800]  batch: 50  d_loss: -0.125335  g_loss: 0.360530\n",
      "Elapsed: [7:48:12.153357]  batch: 100  d_loss: -0.193400  g_loss: 0.420111\n",
      "Elapsed: [7:48:25.486342]  batch: 150  d_loss: 0.012527  g_loss: 0.123707\n",
      "Elapsed: [7:48:37.978839]  batch: 200  d_loss: -0.138533  g_loss: 0.421847\n",
      "Elapsed: [7:48:50.641795]  batch: 250  d_loss: -0.269293  g_loss: 0.459361\n",
      "Elapsed: [7:49:03.232179]  batch: 300  d_loss: -0.162583  g_loss: 0.332908\n",
      "Elapsed: [7:49:15.892372]  batch: 350  d_loss: -0.210474  g_loss: 0.307051\n",
      "Elapsed: [7:49:28.565654]  batch: 400  d_loss: 0.031313  g_loss: 0.146703\n",
      "Elapsed: [7:49:41.150403]  batch: 450  d_loss: -0.389831  g_loss: 0.596128\n",
      "Elapsed: [7:49:53.861893]  batch: 500  d_loss: -0.074074  g_loss: 0.251005\n",
      "Time taken for epoch: 142.110 secs\n",
      "ticker =  36001\n",
      "\n",
      "Epoch: 73\n",
      "Elapsed: [7:50:10.458056]  batch: 1  d_loss: -0.175269  g_loss: 0.337210\n",
      "Elapsed: [7:50:21.950272]  batch: 50  d_loss: -0.202459  g_loss: 0.322737\n",
      "Elapsed: [7:50:34.281866]  batch: 100  d_loss: -0.237503  g_loss: 0.348051\n",
      "Elapsed: [7:50:47.073559]  batch: 150  d_loss: -0.023061  g_loss: 0.083071\n",
      "Elapsed: [7:50:59.755806]  batch: 200  d_loss: 0.011265  g_loss: 0.175671\n",
      "Elapsed: [7:51:12.510869]  batch: 250  d_loss: 0.008807  g_loss: 0.017217\n",
      "Elapsed: [7:51:25.230423]  batch: 300  d_loss: -0.177891  g_loss: 0.410079\n",
      "Elapsed: [7:51:37.826357]  batch: 350  d_loss: -0.231492  g_loss: 0.384389\n",
      "Elapsed: [7:51:50.620145]  batch: 400  d_loss: 0.047833  g_loss: 0.001822\n",
      "Elapsed: [7:52:03.285869]  batch: 450  d_loss: -0.059004  g_loss: 0.250667\n",
      "Elapsed: [7:52:16.143808]  batch: 500  d_loss: -0.011866  g_loss: 0.104500\n",
      "Time taken for epoch: 142.071 secs\n",
      "ticker =  36501\n",
      "\n",
      "Epoch: 74\n",
      "Elapsed: [7:52:33.246280]  batch: 1  d_loss: -0.070874  g_loss: 0.365627\n",
      "Elapsed: [7:52:44.465078]  batch: 50  d_loss: -0.169178  g_loss: 0.379234\n",
      "Elapsed: [7:52:56.542217]  batch: 100  d_loss: -0.153670  g_loss: 0.353907\n",
      "Elapsed: [7:53:09.181545]  batch: 150  d_loss: 0.022486  g_loss: 0.130695\n",
      "Elapsed: [7:53:21.820732]  batch: 200  d_loss: -0.098052  g_loss: 0.283879\n",
      "Elapsed: [7:53:34.485634]  batch: 250  d_loss: 0.026004  g_loss: 0.154882\n",
      "Elapsed: [7:53:47.142475]  batch: 300  d_loss: -0.159158  g_loss: 0.354863\n",
      "Elapsed: [7:53:59.823749]  batch: 350  d_loss: -0.088365  g_loss: 0.301280\n",
      "Elapsed: [7:54:12.535192]  batch: 400  d_loss: -0.057454  g_loss: 0.199069\n",
      "Elapsed: [7:54:25.322197]  batch: 450  d_loss: 0.056201  g_loss: -0.017118\n",
      "Elapsed: [7:54:37.944103]  batch: 500  d_loss: -0.092101  g_loss: 0.313601\n",
      "Time taken for epoch: 141.673 secs\n",
      "ticker =  37001\n",
      "\n",
      "Epoch: 75\n",
      "Elapsed: [7:54:54.571909]  batch: 1  d_loss: -0.049691  g_loss: 0.222181\n",
      "Elapsed: [7:55:06.020596]  batch: 50  d_loss: -0.226024  g_loss: 0.433847\n",
      "Elapsed: [7:55:18.633657]  batch: 100  d_loss: 0.002797  g_loss: 0.167721\n",
      "Elapsed: [7:55:31.489217]  batch: 150  d_loss: -0.020541  g_loss: 0.193629\n",
      "Elapsed: [7:55:44.204000]  batch: 200  d_loss: -0.215718  g_loss: 0.420949\n",
      "Elapsed: [7:55:57.407448]  batch: 250  d_loss: -0.052846  g_loss: 0.228080\n",
      "Elapsed: [7:56:10.085473]  batch: 300  d_loss: -0.068329  g_loss: 0.228884\n",
      "Elapsed: [7:56:22.856858]  batch: 350  d_loss: -0.028049  g_loss: 0.161155\n",
      "Elapsed: [7:56:35.566859]  batch: 400  d_loss: -0.258578  g_loss: 0.437106\n",
      "Elapsed: [7:56:48.270980]  batch: 450  d_loss: -0.082598  g_loss: 0.216630\n",
      "Elapsed: [7:57:01.167805]  batch: 500  d_loss: -0.031586  g_loss: 0.214376\n",
      "Time taken for epoch: 143.059 secs\n",
      "ticker =  37501\n",
      "\n",
      "Epoch: 76\n",
      "Elapsed: [7:57:18.222068]  batch: 1  d_loss: -0.217405  g_loss: 0.395247\n",
      "Elapsed: [7:57:29.248966]  batch: 50  d_loss: -0.221147  g_loss: 0.301867\n",
      "Elapsed: [7:57:41.226959]  batch: 100  d_loss: -0.305879  g_loss: 0.454311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: [7:57:54.218254]  batch: 150  d_loss: -0.156314  g_loss: 0.345359\n",
      "Elapsed: [7:58:06.923727]  batch: 200  d_loss: 0.017349  g_loss: 0.004674\n",
      "Elapsed: [7:58:19.765515]  batch: 250  d_loss: -0.016729  g_loss: 0.143549\n",
      "Elapsed: [7:58:32.472983]  batch: 300  d_loss: -0.030743  g_loss: 0.155083\n",
      "Elapsed: [7:58:45.199974]  batch: 350  d_loss: -0.031250  g_loss: 0.147216\n",
      "Elapsed: [7:58:57.917958]  batch: 400  d_loss: -0.211640  g_loss: 0.326558\n",
      "Elapsed: [7:59:10.777905]  batch: 450  d_loss: -0.163761  g_loss: 0.405346\n",
      "Elapsed: [7:59:23.353667]  batch: 500  d_loss: -0.094789  g_loss: 0.276153\n",
      "Time taken for epoch: 142.031 secs\n",
      "ticker =  38001\n",
      "\n",
      "Epoch: 77\n",
      "Elapsed: [7:59:39.952333]  batch: 1  d_loss: -0.105036  g_loss: 0.271391\n",
      "Elapsed: [7:59:51.381575]  batch: 50  d_loss: 0.066231  g_loss: 0.056844\n",
      "Elapsed: [8:00:03.757328]  batch: 100  d_loss: -0.155289  g_loss: 0.258250\n",
      "Elapsed: [8:00:16.346192]  batch: 150  d_loss: -0.356666  g_loss: 0.467285\n",
      "Elapsed: [8:00:29.018870]  batch: 200  d_loss: -0.156240  g_loss: 0.346741\n",
      "Elapsed: [8:00:41.703905]  batch: 250  d_loss: -0.083092  g_loss: 0.268863\n",
      "Elapsed: [8:00:54.459150]  batch: 300  d_loss: -0.199919  g_loss: 0.365885\n",
      "Elapsed: [8:01:07.186783]  batch: 350  d_loss: -0.215963  g_loss: 0.351775\n",
      "Elapsed: [8:01:19.907325]  batch: 400  d_loss: -0.191096  g_loss: 0.285863\n",
      "Elapsed: [8:01:32.601126]  batch: 450  d_loss: -0.198180  g_loss: 0.400035\n",
      "Elapsed: [8:01:45.334996]  batch: 500  d_loss: 0.050965  g_loss: 0.078450\n",
      "Time taken for epoch: 141.684 secs\n",
      "ticker =  38501\n",
      "\n",
      "Epoch: 78\n",
      "Elapsed: [8:02:01.968399]  batch: 1  d_loss: -0.352223  g_loss: 0.528771\n",
      "Elapsed: [8:02:13.159892]  batch: 50  d_loss: -0.220206  g_loss: 0.451438\n",
      "Elapsed: [8:02:25.559273]  batch: 100  d_loss: -0.121091  g_loss: 0.366351\n",
      "Elapsed: [8:02:38.318387]  batch: 150  d_loss: 0.000827  g_loss: 0.106819\n",
      "Elapsed: [8:02:50.945640]  batch: 200  d_loss: -0.068731  g_loss: 0.254329\n",
      "Elapsed: [8:03:03.557464]  batch: 250  d_loss: -0.054725  g_loss: 0.206177\n",
      "Elapsed: [8:03:16.179151]  batch: 300  d_loss: -0.246902  g_loss: 0.403850\n",
      "Elapsed: [8:03:29.018396]  batch: 350  d_loss: 0.023872  g_loss: 0.266933\n",
      "Elapsed: [8:03:41.719372]  batch: 400  d_loss: -0.152088  g_loss: 0.240673\n",
      "Elapsed: [8:03:54.356565]  batch: 450  d_loss: -0.040471  g_loss: 0.029764\n",
      "Elapsed: [8:04:07.095819]  batch: 500  d_loss: -0.060387  g_loss: 0.271635\n",
      "Time taken for epoch: 141.598 secs\n",
      "ticker =  39001\n",
      "\n",
      "Epoch: 79\n",
      "Elapsed: [8:04:25.564136]  batch: 1  d_loss: -0.206743  g_loss: 0.339484\n",
      "Elapsed: [8:04:36.568574]  batch: 50  d_loss: -0.207011  g_loss: 0.308666\n",
      "Elapsed: [8:04:48.337197]  batch: 100  d_loss: 0.031093  g_loss: 0.088290\n",
      "Elapsed: [8:05:01.406602]  batch: 150  d_loss: 0.467254  g_loss: -0.078620\n",
      "Elapsed: [8:05:14.113901]  batch: 200  d_loss: -0.273944  g_loss: 0.401494\n",
      "Elapsed: [8:05:27.061818]  batch: 250  d_loss: -0.242202  g_loss: 0.402770\n",
      "Elapsed: [8:05:39.591534]  batch: 300  d_loss: -0.216064  g_loss: 0.327140\n",
      "Elapsed: [8:05:52.197622]  batch: 350  d_loss: -0.135711  g_loss: 0.254873\n",
      "Elapsed: [8:06:04.970702]  batch: 400  d_loss: -0.219348  g_loss: 0.392418\n",
      "Elapsed: [8:06:18.028570]  batch: 450  d_loss: -0.142295  g_loss: 0.295446\n",
      "Elapsed: [8:06:30.781590]  batch: 500  d_loss: -0.122346  g_loss: 0.284519\n",
      "Time taken for epoch: 143.533 secs\n",
      "ticker =  39501\n",
      "\n",
      "Epoch: 80\n",
      "Elapsed: [8:06:47.882457]  batch: 1  d_loss: -0.135165  g_loss: 0.296572\n",
      "Elapsed: [8:06:59.397751]  batch: 50  d_loss: -0.072847  g_loss: 0.324403\n",
      "Elapsed: [8:07:11.880147]  batch: 100  d_loss: -0.164634  g_loss: 0.265314\n",
      "Elapsed: [8:07:24.585993]  batch: 150  d_loss: -0.079887  g_loss: 0.209613\n",
      "Elapsed: [8:07:37.314617]  batch: 200  d_loss: -0.097093  g_loss: 0.237252\n",
      "Elapsed: [8:07:50.079814]  batch: 250  d_loss: -0.035386  g_loss: 0.206252\n",
      "Elapsed: [8:08:02.715398]  batch: 300  d_loss: -0.097429  g_loss: 0.256644\n",
      "Elapsed: [8:08:15.366138]  batch: 350  d_loss: -0.228011  g_loss: 0.350674\n",
      "Elapsed: [8:08:28.230464]  batch: 400  d_loss: 0.025386  g_loss: 0.080174\n",
      "Elapsed: [8:08:40.964521]  batch: 450  d_loss: -0.150557  g_loss: 0.343623\n",
      "Elapsed: [8:08:54.386085]  batch: 500  d_loss: -0.031694  g_loss: 0.207550\n",
      "Time taken for epoch: 143.508 secs\n",
      "ticker =  40001\n",
      "\n",
      "Epoch: 81\n",
      "Elapsed: [8:09:11.727266]  batch: 1  d_loss: -0.183932  g_loss: 0.372980\n",
      "Elapsed: [8:09:22.745389]  batch: 50  d_loss: -0.300999  g_loss: 0.358397\n",
      "Elapsed: [8:09:34.573431]  batch: 100  d_loss: -0.199315  g_loss: 0.388314\n",
      "Elapsed: [8:09:47.434422]  batch: 150  d_loss: -0.170847  g_loss: 0.290284\n",
      "Elapsed: [8:10:00.229126]  batch: 200  d_loss: 0.077572  g_loss: 0.094056\n",
      "Elapsed: [8:10:12.923527]  batch: 250  d_loss: -0.043651  g_loss: 0.259205\n",
      "Elapsed: [8:10:26.084414]  batch: 300  d_loss: -0.096259  g_loss: 0.228565\n",
      "Elapsed: [8:10:38.835133]  batch: 350  d_loss: -0.181716  g_loss: 0.232543\n",
      "Elapsed: [8:10:51.561308]  batch: 400  d_loss: -0.042183  g_loss: 0.166960\n",
      "Elapsed: [8:11:04.872820]  batch: 450  d_loss: 0.079274  g_loss: 0.063232\n",
      "Elapsed: [8:11:17.666224]  batch: 500  d_loss: -0.003403  g_loss: 0.076549\n",
      "Time taken for epoch: 142.994 secs\n",
      "ticker =  40501\n",
      "\n",
      "Epoch: 82\n",
      "Elapsed: [8:11:34.465994]  batch: 1  d_loss: -0.172414  g_loss: 0.424437\n",
      "Elapsed: [8:11:45.715672]  batch: 50  d_loss: 0.018638  g_loss: 0.058568\n",
      "Elapsed: [8:11:58.123232]  batch: 100  d_loss: -0.185048  g_loss: 0.304387\n",
      "Elapsed: [8:12:10.845981]  batch: 150  d_loss: -0.107738  g_loss: 0.301947\n",
      "Elapsed: [8:12:23.599543]  batch: 200  d_loss: -0.123016  g_loss: 0.254180\n",
      "Elapsed: [8:12:36.451077]  batch: 250  d_loss: -0.086771  g_loss: 0.231824\n",
      "Elapsed: [8:12:49.233596]  batch: 300  d_loss: -0.116716  g_loss: 0.305364\n",
      "Elapsed: [8:13:02.020690]  batch: 350  d_loss: -0.181767  g_loss: 0.269025\n",
      "Elapsed: [8:13:14.670850]  batch: 400  d_loss: -0.123620  g_loss: 0.289327\n",
      "Elapsed: [8:13:27.515633]  batch: 450  d_loss: -0.210624  g_loss: 0.382122\n",
      "Elapsed: [8:13:40.176137]  batch: 500  d_loss: -0.044507  g_loss: 0.156553\n",
      "Time taken for epoch: 142.334 secs\n",
      "ticker =  41001\n",
      "\n",
      "Epoch: 83\n",
      "Elapsed: [8:13:57.076590]  batch: 1  d_loss: -0.282117  g_loss: 0.417656\n",
      "Elapsed: [8:14:08.403462]  batch: 50  d_loss: -0.076947  g_loss: 0.277877\n",
      "Elapsed: [8:14:20.582628]  batch: 100  d_loss: 0.051379  g_loss: 0.040656\n",
      "Elapsed: [8:14:33.337206]  batch: 150  d_loss: -0.267389  g_loss: 0.303147\n",
      "Elapsed: [8:14:46.152731]  batch: 200  d_loss: -0.149598  g_loss: 0.369349\n",
      "Elapsed: [8:14:58.888648]  batch: 250  d_loss: -0.161350  g_loss: 0.310301\n",
      "Elapsed: [8:15:11.641382]  batch: 300  d_loss: -0.117510  g_loss: 0.303773\n",
      "Elapsed: [8:15:24.254580]  batch: 350  d_loss: -0.128563  g_loss: 0.265617\n",
      "Elapsed: [8:15:36.946538]  batch: 400  d_loss: -0.016096  g_loss: 0.203452\n",
      "Elapsed: [8:15:49.634550]  batch: 450  d_loss: -0.120415  g_loss: 0.209322\n",
      "Elapsed: [8:16:02.466245]  batch: 500  d_loss: -0.160426  g_loss: 0.373075\n",
      "Time taken for epoch: 142.233 secs\n",
      "ticker =  41501\n",
      "\n",
      "Epoch: 84\n",
      "Elapsed: [8:16:19.649512]  batch: 1  d_loss: -0.132151  g_loss: 0.351069\n",
      "Elapsed: [8:16:30.913531]  batch: 50  d_loss: -0.008384  g_loss: 0.079360\n",
      "Elapsed: [8:16:43.154391]  batch: 100  d_loss: -0.138317  g_loss: 0.233080\n",
      "Elapsed: [8:16:56.054835]  batch: 150  d_loss: -0.106626  g_loss: 0.366765\n",
      "Elapsed: [8:17:08.717921]  batch: 200  d_loss: -0.266169  g_loss: 0.379046\n",
      "Elapsed: [8:17:21.511781]  batch: 250  d_loss: -0.170692  g_loss: 0.277267\n",
      "Elapsed: [8:17:34.265257]  batch: 300  d_loss: 0.018619  g_loss: 0.120991\n",
      "Elapsed: [8:17:46.840388]  batch: 350  d_loss: -0.020423  g_loss: 0.240304\n",
      "Elapsed: [8:17:59.646785]  batch: 400  d_loss: -0.000027  g_loss: 0.110762\n",
      "Elapsed: [8:18:12.290969]  batch: 450  d_loss: -0.107951  g_loss: 0.241925\n",
      "Elapsed: [8:18:25.422570]  batch: 500  d_loss: 0.067151  g_loss: 0.071824\n",
      "Time taken for epoch: 142.671 secs\n",
      "ticker =  42001\n",
      "\n",
      "Epoch: 85\n",
      "Elapsed: [8:18:42.706133]  batch: 1  d_loss: -0.256988  g_loss: 0.311171\n",
      "Elapsed: [8:18:54.305759]  batch: 50  d_loss: -0.028882  g_loss: 0.133038\n",
      "Elapsed: [8:19:06.502835]  batch: 100  d_loss: -0.065422  g_loss: 0.113220\n",
      "Elapsed: [8:19:19.294375]  batch: 150  d_loss: -0.193829  g_loss: 0.311019\n",
      "Elapsed: [8:19:32.215890]  batch: 200  d_loss: -0.157407  g_loss: 0.324812\n",
      "Elapsed: [8:19:44.993117]  batch: 250  d_loss: -0.035866  g_loss: 0.238483\n",
      "Elapsed: [8:19:57.838962]  batch: 300  d_loss: -0.007616  g_loss: 0.014187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: [8:20:10.693560]  batch: 350  d_loss: -0.078026  g_loss: 0.197907\n",
      "Elapsed: [8:20:23.743027]  batch: 400  d_loss: -0.250060  g_loss: 0.336432\n",
      "Elapsed: [8:20:36.668196]  batch: 450  d_loss: -0.052483  g_loss: 0.164825\n",
      "Elapsed: [8:20:49.420896]  batch: 500  d_loss: -0.267615  g_loss: 0.473224\n",
      "Time taken for epoch: 143.847 secs\n",
      "ticker =  42501\n",
      "\n",
      "Epoch: 86\n",
      "Elapsed: [8:21:06.628344]  batch: 1  d_loss: 0.057721  g_loss: 0.004211\n",
      "Elapsed: [8:21:17.800695]  batch: 50  d_loss: -0.163517  g_loss: 0.276602\n",
      "Elapsed: [8:21:29.810470]  batch: 100  d_loss: 0.036874  g_loss: 0.108539\n",
      "Elapsed: [8:21:42.749007]  batch: 150  d_loss: -0.136768  g_loss: 0.266610\n",
      "Elapsed: [8:21:55.440644]  batch: 200  d_loss: -0.125540  g_loss: 0.218038\n",
      "Elapsed: [8:22:08.285362]  batch: 250  d_loss: -0.282224  g_loss: 0.398012\n",
      "Elapsed: [8:22:21.205983]  batch: 300  d_loss: -0.015214  g_loss: 0.167900\n",
      "Elapsed: [8:22:33.989566]  batch: 350  d_loss: -0.185306  g_loss: 0.231661\n",
      "Elapsed: [8:22:46.735474]  batch: 400  d_loss: -0.124523  g_loss: 0.197175\n",
      "Elapsed: [8:22:59.730470]  batch: 450  d_loss: -0.142925  g_loss: 0.281634\n",
      "Elapsed: [8:23:12.445856]  batch: 500  d_loss: -0.230499  g_loss: 0.379550\n",
      "Time taken for epoch: 142.814 secs\n",
      "ticker =  43001\n",
      "\n",
      "Epoch: 87\n",
      "Elapsed: [8:23:29.795940]  batch: 1  d_loss: 0.058039  g_loss: 0.082622\n",
      "Elapsed: [8:23:41.100110]  batch: 50  d_loss: -0.132941  g_loss: 0.264362\n",
      "Elapsed: [8:23:53.294518]  batch: 100  d_loss: -0.080463  g_loss: 0.210785\n",
      "Elapsed: [8:24:06.238033]  batch: 150  d_loss: -0.114863  g_loss: 0.324587\n",
      "Elapsed: [8:24:18.977727]  batch: 200  d_loss: -0.005910  g_loss: 0.056696\n",
      "Elapsed: [8:24:31.738771]  batch: 250  d_loss: 0.042904  g_loss: 0.217071\n",
      "Elapsed: [8:24:44.492421]  batch: 300  d_loss: 0.039925  g_loss: 0.147386\n",
      "Elapsed: [8:24:57.299568]  batch: 350  d_loss: -0.122724  g_loss: 0.213255\n",
      "Elapsed: [8:25:10.022027]  batch: 400  d_loss: -0.151685  g_loss: 0.290110\n",
      "Elapsed: [8:25:22.832961]  batch: 450  d_loss: -0.207706  g_loss: 0.372585\n",
      "Elapsed: [8:25:35.669531]  batch: 500  d_loss: 0.064540  g_loss: -0.001686\n",
      "Time taken for epoch: 143.027 secs\n",
      "ticker =  43501\n",
      "\n",
      "Epoch: 88\n",
      "Elapsed: [8:25:52.561463]  batch: 1  d_loss: -0.263885  g_loss: 0.380231\n",
      "Elapsed: [8:26:04.335007]  batch: 50  d_loss: -0.226166  g_loss: 0.315974\n",
      "Elapsed: [8:26:16.668155]  batch: 100  d_loss: -0.140735  g_loss: 0.285590\n",
      "Elapsed: [8:26:29.549968]  batch: 150  d_loss: -0.263122  g_loss: 0.455140\n",
      "Elapsed: [8:26:42.196966]  batch: 200  d_loss: -0.275406  g_loss: 0.333099\n",
      "Elapsed: [8:26:54.933263]  batch: 250  d_loss: 0.026664  g_loss: -0.017438\n",
      "Elapsed: [8:27:07.793059]  batch: 300  d_loss: -0.056788  g_loss: 0.133569\n",
      "Elapsed: [8:27:20.365595]  batch: 350  d_loss: -0.181654  g_loss: 0.306118\n",
      "Elapsed: [8:27:32.970977]  batch: 400  d_loss: -0.111831  g_loss: 0.251060\n",
      "Elapsed: [8:27:45.812157]  batch: 450  d_loss: -0.086545  g_loss: 0.202311\n",
      "Elapsed: [8:27:58.640032]  batch: 500  d_loss: -0.244827  g_loss: 0.303208\n",
      "Time taken for epoch: 142.809 secs\n",
      "ticker =  44001\n",
      "\n",
      "Epoch: 89\n",
      "Elapsed: [8:28:15.569488]  batch: 1  d_loss: -0.012508  g_loss: 0.160378\n",
      "Elapsed: [8:28:26.593856]  batch: 50  d_loss: -0.199016  g_loss: 0.312044\n",
      "Elapsed: [8:28:38.770048]  batch: 100  d_loss: -0.090577  g_loss: 0.194412\n",
      "Elapsed: [8:28:51.549814]  batch: 150  d_loss: -0.257737  g_loss: 0.426032\n",
      "Elapsed: [8:29:05.665843]  batch: 200  d_loss: 0.028678  g_loss: 0.123226\n",
      "Elapsed: [8:29:20.772328]  batch: 250  d_loss: -0.093466  g_loss: 0.258379\n",
      "Elapsed: [8:29:35.572080]  batch: 300  d_loss: -0.053926  g_loss: 0.223076\n",
      "Elapsed: [8:29:48.270149]  batch: 350  d_loss: -0.091931  g_loss: 0.290946\n",
      "Elapsed: [8:30:01.026089]  batch: 400  d_loss: -0.003428  g_loss: 0.119196\n",
      "Elapsed: [8:30:13.729964]  batch: 450  d_loss: -0.014684  g_loss: 0.195559\n",
      "Elapsed: [8:30:26.635914]  batch: 500  d_loss: -0.069095  g_loss: 0.266616\n",
      "Time taken for epoch: 147.847 secs\n",
      "ticker =  44501\n",
      "\n",
      "Epoch: 90\n",
      "Elapsed: [8:30:43.519315]  batch: 1  d_loss: -0.115477  g_loss: 0.297238\n",
      "Elapsed: [8:30:54.502687]  batch: 50  d_loss: 0.060193  g_loss: 0.062984\n",
      "Elapsed: [8:31:06.392381]  batch: 100  d_loss: -0.017988  g_loss: 0.138873\n",
      "Elapsed: [8:31:19.133516]  batch: 150  d_loss: -0.106951  g_loss: 0.325182\n",
      "Elapsed: [8:31:31.928330]  batch: 200  d_loss: -0.146499  g_loss: 0.252457\n",
      "Elapsed: [8:31:44.625859]  batch: 250  d_loss: -0.078655  g_loss: 0.248189\n",
      "Elapsed: [8:31:57.256774]  batch: 300  d_loss: -0.020425  g_loss: 0.173269\n",
      "Elapsed: [8:32:09.952707]  batch: 350  d_loss: -0.101879  g_loss: 0.248355\n",
      "Elapsed: [8:32:22.707684]  batch: 400  d_loss: -0.012095  g_loss: 0.174519\n",
      "Elapsed: [8:32:35.538244]  batch: 450  d_loss: 0.006649  g_loss: 0.121469\n",
      "Elapsed: [8:32:48.259259]  batch: 500  d_loss: 0.076461  g_loss: -0.004737\n",
      "Time taken for epoch: 141.464 secs\n",
      "ticker =  45001\n",
      "\n",
      "Epoch: 91\n",
      "Elapsed: [8:33:05.223282]  batch: 1  d_loss: -0.094550  g_loss: 0.144701\n",
      "Elapsed: [8:33:16.575705]  batch: 50  d_loss: -0.225642  g_loss: 0.390600\n",
      "Elapsed: [8:33:28.838988]  batch: 100  d_loss: -0.003607  g_loss: 0.099622\n",
      "Elapsed: [8:33:41.593435]  batch: 150  d_loss: -0.107455  g_loss: 0.293093\n",
      "Elapsed: [8:33:54.351295]  batch: 200  d_loss: -0.126181  g_loss: 0.264383\n",
      "Elapsed: [8:34:07.135024]  batch: 250  d_loss: -0.090976  g_loss: 0.289844\n",
      "Elapsed: [8:34:20.061114]  batch: 300  d_loss: -0.048981  g_loss: 0.389998\n",
      "Elapsed: [8:34:32.804891]  batch: 350  d_loss: -0.242058  g_loss: 0.395411\n",
      "Elapsed: [8:34:45.523440]  batch: 400  d_loss: -0.191458  g_loss: 0.297386\n",
      "Elapsed: [8:34:58.391821]  batch: 450  d_loss: -0.081895  g_loss: 0.264495\n",
      "Elapsed: [8:35:11.257726]  batch: 500  d_loss: -0.052343  g_loss: 0.196650\n",
      "Time taken for epoch: 142.755 secs\n",
      "ticker =  45501\n",
      "\n",
      "Epoch: 92\n",
      "Elapsed: [8:35:28.377233]  batch: 1  d_loss: -0.231570  g_loss: 0.328135\n",
      "Elapsed: [8:35:39.497372]  batch: 50  d_loss: -0.163232  g_loss: 0.232136\n",
      "Elapsed: [8:35:51.453016]  batch: 100  d_loss: -0.001454  g_loss: 0.236687\n",
      "Elapsed: [8:36:05.220577]  batch: 150  d_loss: -0.143156  g_loss: 0.402965\n",
      "Elapsed: [8:36:17.837173]  batch: 200  d_loss: -0.258631  g_loss: 0.363400\n",
      "Elapsed: [8:36:30.381749]  batch: 250  d_loss: -0.150324  g_loss: 0.291749\n",
      "Elapsed: [8:36:42.898644]  batch: 300  d_loss: -0.106332  g_loss: 0.296063\n",
      "Elapsed: [8:36:55.379812]  batch: 350  d_loss: -0.107283  g_loss: 0.231728\n",
      "Elapsed: [8:37:07.840452]  batch: 400  d_loss: -0.050531  g_loss: 0.163083\n",
      "Elapsed: [8:37:20.352694]  batch: 450  d_loss: -0.125390  g_loss: 0.372088\n",
      "Elapsed: [8:37:32.889149]  batch: 500  d_loss: -0.100549  g_loss: 0.147552\n",
      "Time taken for epoch: 141.427 secs\n",
      "ticker =  46001\n",
      "\n",
      "Epoch: 93\n",
      "Elapsed: [8:37:49.333404]  batch: 1  d_loss: -0.082775  g_loss: 0.195510\n",
      "Elapsed: [8:38:00.806193]  batch: 50  d_loss: 0.021041  g_loss: 0.073977\n",
      "Elapsed: [8:38:13.212813]  batch: 100  d_loss: -0.214659  g_loss: 0.372242\n",
      "Elapsed: [8:38:25.865766]  batch: 150  d_loss: -0.117473  g_loss: 0.243287\n",
      "Elapsed: [8:38:38.436632]  batch: 200  d_loss: -0.286076  g_loss: 0.414888\n",
      "Elapsed: [8:38:50.936785]  batch: 250  d_loss: -0.041908  g_loss: 0.155318\n",
      "Elapsed: [8:39:03.447882]  batch: 300  d_loss: -0.138938  g_loss: 0.240402\n",
      "Elapsed: [8:39:16.025915]  batch: 350  d_loss: -0.065917  g_loss: 0.199100\n",
      "Elapsed: [8:39:28.577123]  batch: 400  d_loss: -0.149004  g_loss: 0.240303\n",
      "Elapsed: [8:39:41.093775]  batch: 450  d_loss: 0.028851  g_loss: 0.155569\n",
      "Elapsed: [8:39:53.622869]  batch: 500  d_loss: -0.249469  g_loss: 0.425785\n",
      "Time taken for epoch: 140.541 secs\n",
      "ticker =  46501\n",
      "\n",
      "Epoch: 94\n",
      "Elapsed: [8:40:10.098787]  batch: 1  d_loss: -0.047715  g_loss: 0.177057\n",
      "Elapsed: [8:40:21.633851]  batch: 50  d_loss: -0.091913  g_loss: 0.102972\n",
      "Elapsed: [8:40:34.144104]  batch: 100  d_loss: -0.200566  g_loss: 0.339436\n",
      "Elapsed: [8:40:46.626970]  batch: 150  d_loss: -0.011980  g_loss: 0.161869\n",
      "Elapsed: [8:40:59.128830]  batch: 200  d_loss: -0.152295  g_loss: 0.213181\n",
      "Elapsed: [8:41:11.989636]  batch: 250  d_loss: -0.215313  g_loss: 0.422468\n",
      "Elapsed: [8:41:24.566590]  batch: 300  d_loss: -0.052615  g_loss: 0.215622\n",
      "Elapsed: [8:41:37.104013]  batch: 350  d_loss: -0.206405  g_loss: 0.356809\n",
      "Elapsed: [8:41:49.578452]  batch: 400  d_loss: -0.022144  g_loss: 0.162793\n",
      "Elapsed: [8:42:02.018268]  batch: 450  d_loss: -0.144104  g_loss: 0.244421\n",
      "Elapsed: [8:42:14.549275]  batch: 500  d_loss: -0.226159  g_loss: 0.351318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for epoch: 140.796 secs\n",
      "ticker =  47001\n",
      "\n",
      "Epoch: 95\n",
      "Elapsed: [8:42:31.138436]  batch: 1  d_loss: -0.085841  g_loss: 0.212501\n",
      "Elapsed: [8:42:42.502918]  batch: 50  d_loss: -0.173652  g_loss: 0.272795\n",
      "Elapsed: [8:42:54.842687]  batch: 100  d_loss: -0.154790  g_loss: 0.324552\n",
      "Elapsed: [8:43:07.671738]  batch: 150  d_loss: -0.059816  g_loss: 0.218077\n",
      "Elapsed: [8:43:20.276943]  batch: 200  d_loss: -0.101997  g_loss: 0.189168\n",
      "Elapsed: [8:43:32.737391]  batch: 250  d_loss: -0.189171  g_loss: 0.322135\n",
      "Elapsed: [8:43:45.252756]  batch: 300  d_loss: -0.178642  g_loss: 0.350037\n",
      "Elapsed: [8:43:57.748942]  batch: 350  d_loss: -0.280930  g_loss: 0.409338\n",
      "Elapsed: [8:44:10.401807]  batch: 400  d_loss: -0.124479  g_loss: 0.270455\n",
      "Elapsed: [8:44:22.907043]  batch: 450  d_loss: -0.040648  g_loss: 0.077591\n",
      "Elapsed: [8:44:35.596651]  batch: 500  d_loss: -0.010626  g_loss: 0.230550\n",
      "Time taken for epoch: 140.877 secs\n",
      "ticker =  47501\n",
      "\n",
      "Epoch: 96\n",
      "Elapsed: [8:44:51.713420]  batch: 1  d_loss: -0.128365  g_loss: 0.304076\n",
      "Elapsed: [8:45:03.251849]  batch: 50  d_loss: 0.034210  g_loss: 0.009961\n",
      "Elapsed: [8:45:15.680254]  batch: 100  d_loss: -0.211138  g_loss: 0.353798\n",
      "Elapsed: [8:45:28.188106]  batch: 150  d_loss: 0.044531  g_loss: 0.069560\n",
      "Elapsed: [8:45:40.700401]  batch: 200  d_loss: 0.001127  g_loss: 0.053264\n",
      "Elapsed: [8:45:53.188813]  batch: 250  d_loss: -0.239547  g_loss: 0.359522\n",
      "Elapsed: [8:46:05.885288]  batch: 300  d_loss: -0.201094  g_loss: 0.418024\n",
      "Elapsed: [8:46:18.375072]  batch: 350  d_loss: -0.331064  g_loss: 0.457390\n",
      "Elapsed: [8:46:30.838123]  batch: 400  d_loss: -0.089995  g_loss: 0.286300\n",
      "Elapsed: [8:46:43.330373]  batch: 450  d_loss: -0.004756  g_loss: 0.145445\n",
      "Elapsed: [8:46:55.792913]  batch: 500  d_loss: -0.143751  g_loss: 0.317030\n",
      "Time taken for epoch: 140.044 secs\n",
      "ticker =  48001\n",
      "\n",
      "Epoch: 97\n",
      "Elapsed: [8:47:12.297097]  batch: 1  d_loss: -0.056472  g_loss: 0.216227\n",
      "Elapsed: [8:47:23.576208]  batch: 50  d_loss: -0.010806  g_loss: 0.121333\n",
      "Elapsed: [8:47:35.927872]  batch: 100  d_loss: 0.043309  g_loss: 0.172844\n",
      "Elapsed: [8:47:48.438129]  batch: 150  d_loss: 0.057273  g_loss: -0.043094\n",
      "Elapsed: [8:48:01.058877]  batch: 200  d_loss: -0.250998  g_loss: 0.372604\n",
      "Elapsed: [8:48:13.569541]  batch: 250  d_loss: -0.024228  g_loss: 0.188091\n",
      "Elapsed: [8:48:26.079183]  batch: 300  d_loss: -0.137880  g_loss: 0.288041\n",
      "Elapsed: [8:48:38.565704]  batch: 350  d_loss: -0.213657  g_loss: 0.334454\n",
      "Elapsed: [8:48:51.061754]  batch: 400  d_loss: -0.007173  g_loss: 0.111171\n",
      "Elapsed: [8:49:04.664807]  batch: 450  d_loss: -0.007546  g_loss: 0.180138\n",
      "Elapsed: [8:49:17.209967]  batch: 500  d_loss: -0.099376  g_loss: 0.141684\n",
      "Time taken for epoch: 141.275 secs\n",
      "ticker =  48501\n",
      "\n",
      "Epoch: 98\n",
      "Elapsed: [8:49:33.673147]  batch: 1  d_loss: -0.154555  g_loss: 0.217701\n",
      "Elapsed: [8:49:45.147840]  batch: 50  d_loss: -0.066882  g_loss: 0.169905\n",
      "Elapsed: [8:49:57.449454]  batch: 100  d_loss: -0.084438  g_loss: 0.136016\n",
      "Elapsed: [8:50:09.992818]  batch: 150  d_loss: -0.198203  g_loss: 0.325373\n",
      "Elapsed: [8:50:22.577082]  batch: 200  d_loss: 0.003297  g_loss: 0.118643\n",
      "Elapsed: [8:50:35.355465]  batch: 250  d_loss: -0.067427  g_loss: 0.252129\n",
      "Elapsed: [8:50:47.842261]  batch: 300  d_loss: -0.161945  g_loss: 0.307932\n",
      "Elapsed: [8:51:00.324159]  batch: 350  d_loss: 0.045106  g_loss: 0.073798\n",
      "Elapsed: [8:51:12.810279]  batch: 400  d_loss: -0.179884  g_loss: 0.369897\n",
      "Elapsed: [8:51:25.277036]  batch: 450  d_loss: -0.114656  g_loss: 0.278099\n",
      "Elapsed: [8:51:37.794348]  batch: 500  d_loss: -0.012906  g_loss: 0.193554\n",
      "Time taken for epoch: 140.410 secs\n",
      "ticker =  49001\n",
      "\n",
      "Epoch: 99\n",
      "Elapsed: [8:51:54.053489]  batch: 1  d_loss: -0.158213  g_loss: 0.347691\n",
      "Elapsed: [8:52:05.591750]  batch: 50  d_loss: -0.201554  g_loss: 0.391724\n",
      "Elapsed: [8:52:17.915471]  batch: 100  d_loss: -0.253025  g_loss: 0.321891\n",
      "Elapsed: [8:52:30.567099]  batch: 150  d_loss: -0.044802  g_loss: 0.200658\n",
      "Elapsed: [8:52:43.030393]  batch: 200  d_loss: -0.165637  g_loss: 0.219880\n",
      "Elapsed: [8:52:55.509722]  batch: 250  d_loss: 0.056353  g_loss: 0.046431\n",
      "Elapsed: [8:53:08.316311]  batch: 300  d_loss: -0.268364  g_loss: 0.363086\n",
      "Elapsed: [8:53:20.821048]  batch: 350  d_loss: 0.040169  g_loss: 0.057580\n",
      "Elapsed: [8:53:33.306656]  batch: 400  d_loss: -0.106442  g_loss: 0.272252\n",
      "Elapsed: [8:53:45.835965]  batch: 450  d_loss: 0.033120  g_loss: 0.095457\n",
      "Elapsed: [8:53:58.328599]  batch: 500  d_loss: -0.059805  g_loss: 0.256508\n",
      "Time taken for epoch: 140.374 secs\n",
      "ticker =  49501\n",
      "\n",
      "Epoch: 100\n",
      "Elapsed: [8:54:14.665908]  batch: 1  d_loss: -0.139558  g_loss: 0.333967\n",
      "Elapsed: [8:54:26.218385]  batch: 50  d_loss: -0.061378  g_loss: 0.158403\n",
      "Elapsed: [8:54:38.729638]  batch: 100  d_loss: -0.171282  g_loss: 0.303662\n",
      "Elapsed: [8:54:51.224564]  batch: 150  d_loss: -0.110157  g_loss: 0.293739\n",
      "Elapsed: [8:55:03.756774]  batch: 200  d_loss: -0.096002  g_loss: 0.241378\n",
      "Elapsed: [8:55:16.350888]  batch: 250  d_loss: -0.078617  g_loss: 0.216337\n",
      "Elapsed: [8:55:28.852774]  batch: 300  d_loss: -0.246332  g_loss: 0.366984\n",
      "Elapsed: [8:55:41.356664]  batch: 350  d_loss: -0.211956  g_loss: 0.343595\n",
      "Elapsed: [8:55:53.860129]  batch: 400  d_loss: -0.067161  g_loss: 0.262717\n",
      "Elapsed: [8:56:06.738405]  batch: 450  d_loss: -0.190950  g_loss: 0.273324\n",
      "Elapsed: [8:56:19.298364]  batch: 500  d_loss: 0.052311  g_loss: 0.060896\n",
      "Time taken for epoch: 140.787 secs\n",
      "ticker =  50001\n",
      "\n",
      "\n",
      "Currently working on Depth:  6\n",
      "Current resolution: 128 x 128\n",
      "Ticker 1\n",
      "\n",
      "Epoch: 1\n",
      "Elapsed: [8:56:35.867925]  batch: 1  d_loss: 2.056190  g_loss: 0.698685\n",
      "Elapsed: [8:56:57.266408]  batch: 50  d_loss: -0.157306  g_loss: 0.747718\n",
      "Elapsed: [8:57:20.033417]  batch: 100  d_loss: 0.015718  g_loss: 0.437808\n",
      "Elapsed: [8:57:42.791863]  batch: 150  d_loss: -0.318531  g_loss: 0.597326\n",
      "Elapsed: [8:58:05.593410]  batch: 200  d_loss: 0.087921  g_loss: 0.114250\n",
      "Elapsed: [8:58:28.423442]  batch: 250  d_loss: -0.086489  g_loss: 0.178212\n",
      "Elapsed: [8:58:51.251963]  batch: 300  d_loss: -0.236084  g_loss: 0.640000\n",
      "Elapsed: [8:59:14.078530]  batch: 350  d_loss: -0.411286  g_loss: 0.615972\n",
      "Elapsed: [8:59:36.986125]  batch: 400  d_loss: -0.079065  g_loss: 0.443256\n",
      "Elapsed: [8:59:59.821831]  batch: 450  d_loss: -0.401747  g_loss: 0.336480\n",
      "Elapsed: [9:00:22.664783]  batch: 500  d_loss: -0.367830  g_loss: 0.747444\n",
      "Time taken for epoch: 243.240 secs\n",
      "ticker =  501\n",
      "\n",
      "Epoch: 2\n",
      "Elapsed: [9:00:40.599201]  batch: 1  d_loss: -0.012936  g_loss: 0.317427\n",
      "Elapsed: [9:01:02.424437]  batch: 50  d_loss: -0.286467  g_loss: 0.517892\n",
      "Elapsed: [9:01:25.217673]  batch: 100  d_loss: -0.044654  g_loss: 0.346080\n",
      "Elapsed: [9:01:48.017992]  batch: 150  d_loss: -0.339101  g_loss: 0.646859\n",
      "Elapsed: [9:02:10.810554]  batch: 200  d_loss: -0.455321  g_loss: 0.816480\n",
      "Elapsed: [9:02:33.660886]  batch: 250  d_loss: -0.328365  g_loss: 0.656470\n",
      "Elapsed: [9:02:56.502217]  batch: 300  d_loss: -0.175420  g_loss: 0.428466\n",
      "Elapsed: [9:03:19.339613]  batch: 350  d_loss: 0.115654  g_loss: -0.047178\n",
      "Elapsed: [9:03:42.215836]  batch: 400  d_loss: -0.281848  g_loss: 0.653989\n",
      "Elapsed: [9:04:05.076178]  batch: 450  d_loss: -0.306807  g_loss: 0.543360\n",
      "Elapsed: [9:04:28.218584]  batch: 500  d_loss: -0.452034  g_loss: 0.571471\n",
      "Time taken for epoch: 245.419 secs\n",
      "ticker =  1001\n",
      "\n",
      "Epoch: 3\n",
      "Elapsed: [9:04:45.886400]  batch: 1  d_loss: -0.177436  g_loss: 0.588559\n",
      "Elapsed: [9:05:07.805869]  batch: 50  d_loss: -0.125668  g_loss: 0.452134\n",
      "Elapsed: [9:05:30.622705]  batch: 100  d_loss: -0.053959  g_loss: 0.315180\n",
      "Elapsed: [9:05:53.432720]  batch: 150  d_loss: -0.456006  g_loss: 0.782398\n",
      "Elapsed: [9:06:16.229706]  batch: 200  d_loss: 0.044937  g_loss: 0.206188\n",
      "Elapsed: [9:06:39.119347]  batch: 250  d_loss: -0.100866  g_loss: 0.414457\n",
      "Elapsed: [9:07:01.917539]  batch: 300  d_loss: -0.395966  g_loss: 0.814576\n",
      "Elapsed: [9:07:24.768410]  batch: 350  d_loss: -0.653408  g_loss: 0.884322\n",
      "Elapsed: [9:07:47.644641]  batch: 400  d_loss: -0.132954  g_loss: 0.218555\n",
      "Elapsed: [9:08:10.475304]  batch: 450  d_loss: -0.181682  g_loss: 0.474178\n",
      "Elapsed: [9:08:33.313734]  batch: 500  d_loss: -0.343578  g_loss: 0.588167\n",
      "Time taken for epoch: 244.930 secs\n",
      "ticker =  1501\n",
      "\n",
      "Epoch: 4\n",
      "Elapsed: [9:08:50.734863]  batch: 1  d_loss: -0.376271  g_loss: 0.631971\n",
      "Elapsed: [9:09:13.162921]  batch: 50  d_loss: -0.408252  g_loss: 0.701590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: [9:09:35.965155]  batch: 100  d_loss: -0.196517  g_loss: 0.559105\n",
      "Elapsed: [9:10:03.057146]  batch: 150  d_loss: -0.351792  g_loss: 0.486977\n",
      "Elapsed: [9:10:26.666871]  batch: 200  d_loss: -0.633950  g_loss: 0.909546\n",
      "Elapsed: [9:10:49.601200]  batch: 250  d_loss: -0.504869  g_loss: 0.847893\n",
      "Elapsed: [9:11:13.542165]  batch: 300  d_loss: -0.467293  g_loss: 0.757056\n",
      "Elapsed: [9:11:36.447207]  batch: 350  d_loss: -0.265649  g_loss: 0.533413\n",
      "Elapsed: [9:11:59.544508]  batch: 400  d_loss: 0.022037  g_loss: 0.333173\n",
      "Elapsed: [9:12:22.463741]  batch: 450  d_loss: -0.444723  g_loss: 0.763699\n",
      "Elapsed: [9:12:45.429152]  batch: 500  d_loss: -0.577942  g_loss: 0.963854\n",
      "Time taken for epoch: 251.972 secs\n",
      "ticker =  2001\n",
      "\n",
      "Epoch: 5\n",
      "Elapsed: [9:13:04.252306]  batch: 1  d_loss: -0.005593  g_loss: 0.328369\n",
      "Elapsed: [9:13:26.261415]  batch: 50  d_loss: 0.102792  g_loss: -0.023021\n",
      "Elapsed: [9:13:49.106997]  batch: 100  d_loss: -0.196466  g_loss: 0.470565\n",
      "Elapsed: [9:14:11.979041]  batch: 150  d_loss: -0.233061  g_loss: 0.605160\n",
      "Elapsed: [9:14:34.906341]  batch: 200  d_loss: -0.473456  g_loss: 0.639636\n",
      "Elapsed: [9:14:57.825594]  batch: 250  d_loss: -0.017828  g_loss: 0.240372\n",
      "Elapsed: [9:15:20.742501]  batch: 300  d_loss: -0.552857  g_loss: 0.841153\n",
      "Elapsed: [9:15:43.646863]  batch: 350  d_loss: -0.154721  g_loss: 0.555498\n",
      "Elapsed: [9:16:06.560119]  batch: 400  d_loss: -0.070639  g_loss: 0.250265\n",
      "Elapsed: [9:16:29.587414]  batch: 450  d_loss: -0.709978  g_loss: 1.004887\n",
      "Elapsed: [9:16:52.470592]  batch: 500  d_loss: -0.222883  g_loss: 0.623565\n",
      "Time taken for epoch: 246.784 secs\n",
      "ticker =  2501\n",
      "\n",
      "Epoch: 6\n",
      "Elapsed: [9:17:10.498650]  batch: 1  d_loss: -0.289093  g_loss: 0.665067\n",
      "Elapsed: [9:17:32.652349]  batch: 50  d_loss: -0.326532  g_loss: 0.630805\n",
      "Elapsed: [9:17:55.509839]  batch: 100  d_loss: -0.656159  g_loss: 1.019282\n",
      "Elapsed: [9:18:18.405685]  batch: 150  d_loss: -0.542103  g_loss: 1.011360\n",
      "Elapsed: [9:18:41.315006]  batch: 200  d_loss: -0.143861  g_loss: 0.529843\n",
      "Elapsed: [9:19:04.221363]  batch: 250  d_loss: 0.088070  g_loss: 0.192403\n",
      "Elapsed: [9:19:27.420659]  batch: 300  d_loss: -0.211342  g_loss: 0.499797\n",
      "Elapsed: [9:19:50.301313]  batch: 350  d_loss: 0.025988  g_loss: 0.174957\n",
      "Elapsed: [9:20:13.181485]  batch: 400  d_loss: -0.031917  g_loss: 0.356879\n",
      "Elapsed: [9:20:36.097825]  batch: 450  d_loss: -0.322334  g_loss: 0.700165\n",
      "Elapsed: [9:20:59.090498]  batch: 500  d_loss: -0.349731  g_loss: 0.748859\n",
      "Time taken for epoch: 246.520 secs\n",
      "ticker =  3001\n",
      "\n",
      "Epoch: 7\n",
      "Elapsed: [9:21:17.180686]  batch: 1  d_loss: -0.271337  g_loss: 0.695600\n",
      "Elapsed: [9:21:39.323603]  batch: 50  d_loss: -0.257569  g_loss: 0.437770\n",
      "Elapsed: [9:22:02.164059]  batch: 100  d_loss: -0.161880  g_loss: 0.547143\n",
      "Elapsed: [9:22:25.083685]  batch: 150  d_loss: -0.274616  g_loss: 0.571823\n",
      "Elapsed: [9:22:48.005429]  batch: 200  d_loss: 0.088012  g_loss: 0.058534\n",
      "Elapsed: [9:23:10.924339]  batch: 250  d_loss: -0.586527  g_loss: 0.864941\n",
      "Elapsed: [9:23:33.813441]  batch: 300  d_loss: -0.366089  g_loss: 0.636252\n",
      "Elapsed: [9:23:56.818383]  batch: 350  d_loss: -0.345678  g_loss: 0.760741\n",
      "Elapsed: [9:24:19.771048]  batch: 400  d_loss: -0.018756  g_loss: 0.418985\n",
      "Elapsed: [9:24:42.740624]  batch: 450  d_loss: -0.190355  g_loss: 0.540197\n",
      "Elapsed: [9:25:05.692984]  batch: 500  d_loss: -0.094760  g_loss: 0.368204\n",
      "Time taken for epoch: 246.340 secs\n",
      "ticker =  3501\n",
      "\n",
      "Epoch: 8\n",
      "Elapsed: [9:25:24.260904]  batch: 1  d_loss: -0.145395  g_loss: 0.305064\n",
      "Elapsed: [9:25:46.339307]  batch: 50  d_loss: -0.087598  g_loss: 0.539532\n",
      "Elapsed: [9:26:09.246657]  batch: 100  d_loss: -0.598720  g_loss: 0.849004\n",
      "Elapsed: [9:26:33.398647]  batch: 150  d_loss: -0.450919  g_loss: 0.666662\n",
      "Elapsed: [9:26:59.074562]  batch: 200  d_loss: 0.015115  g_loss: 0.340571\n",
      "Elapsed: [9:27:22.085208]  batch: 250  d_loss: -0.381634  g_loss: 0.460456\n",
      "Elapsed: [9:27:45.780124]  batch: 300  d_loss: -0.122049  g_loss: 0.504723\n",
      "Elapsed: [9:28:08.782470]  batch: 350  d_loss: -0.529200  g_loss: 0.948058\n",
      "Elapsed: [9:28:31.850398]  batch: 400  d_loss: -0.108612  g_loss: 0.267218\n",
      "Elapsed: [9:28:54.818609]  batch: 450  d_loss: -0.339428  g_loss: 0.699088\n",
      "Elapsed: [9:29:19.397690]  batch: 500  d_loss: -0.229886  g_loss: 0.505558\n",
      "Time taken for epoch: 253.745 secs\n",
      "ticker =  4001\n",
      "\n",
      "Epoch: 9\n",
      "Elapsed: [9:29:40.172909]  batch: 1  d_loss: -0.342380  g_loss: 0.567308\n",
      "Elapsed: [9:30:02.005625]  batch: 50  d_loss: -0.071382  g_loss: 0.320053\n",
      "Elapsed: [9:30:24.907175]  batch: 100  d_loss: 0.029891  g_loss: 0.280107\n",
      "Elapsed: [9:30:47.813034]  batch: 150  d_loss: -0.123669  g_loss: 0.498798\n",
      "Elapsed: [9:31:10.769325]  batch: 200  d_loss: -0.090801  g_loss: 0.388443\n",
      "Elapsed: [9:31:33.702073]  batch: 250  d_loss: -0.147139  g_loss: 0.547397\n",
      "Elapsed: [9:31:56.607115]  batch: 300  d_loss: -0.291247  g_loss: 0.559619\n",
      "Elapsed: [9:32:19.520347]  batch: 350  d_loss: -0.562960  g_loss: 0.878963\n",
      "Elapsed: [9:32:42.432715]  batch: 400  d_loss: -0.256834  g_loss: 0.587957\n",
      "Elapsed: [9:33:05.446945]  batch: 450  d_loss: -0.425869  g_loss: 0.817218\n",
      "Elapsed: [9:33:28.354666]  batch: 500  d_loss: -0.148874  g_loss: 0.452596\n",
      "Time taken for epoch: 248.415 secs\n",
      "ticker =  4501\n",
      "\n",
      "Epoch: 10\n",
      "Elapsed: [9:33:48.072286]  batch: 1  d_loss: -0.396260  g_loss: 0.789894\n",
      "Elapsed: [9:34:10.091309]  batch: 50  d_loss: -0.482655  g_loss: 0.847227\n",
      "Elapsed: [9:34:32.992171]  batch: 100  d_loss: -0.525303  g_loss: 0.882494\n",
      "Elapsed: [9:34:55.876900]  batch: 150  d_loss: -0.021369  g_loss: 0.248804\n",
      "Elapsed: [9:35:18.805302]  batch: 200  d_loss: -0.032469  g_loss: 0.299951\n",
      "Elapsed: [9:35:41.781948]  batch: 250  d_loss: -0.029236  g_loss: 0.244536\n",
      "Elapsed: [9:36:04.769320]  batch: 300  d_loss: -0.080048  g_loss: 0.451047\n",
      "Elapsed: [9:36:27.761175]  batch: 350  d_loss: -0.256066  g_loss: 0.710950\n",
      "Elapsed: [9:36:50.769359]  batch: 400  d_loss: -0.242085  g_loss: 0.437629\n",
      "Elapsed: [9:37:13.771762]  batch: 450  d_loss: -0.512581  g_loss: 0.787412\n",
      "Elapsed: [9:37:36.825724]  batch: 500  d_loss: 0.027661  g_loss: 0.013343\n",
      "Time taken for epoch: 248.269 secs\n",
      "ticker =  5001\n",
      "\n",
      "Epoch: 11\n",
      "Elapsed: [9:37:54.753408]  batch: 1  d_loss: -0.138495  g_loss: 0.483876\n",
      "Elapsed: [9:38:16.914103]  batch: 50  d_loss: -0.247186  g_loss: 0.555291\n",
      "Elapsed: [9:38:39.732398]  batch: 100  d_loss: -0.517452  g_loss: 0.832488\n",
      "Elapsed: [9:39:02.670684]  batch: 150  d_loss: -0.151434  g_loss: 0.453907\n",
      "Elapsed: [9:39:25.581496]  batch: 200  d_loss: -0.436107  g_loss: 0.813713\n",
      "Elapsed: [9:39:48.473054]  batch: 250  d_loss: -0.411906  g_loss: 0.778702\n",
      "Elapsed: [9:40:11.359815]  batch: 300  d_loss: -0.042490  g_loss: 0.371937\n",
      "Elapsed: [9:40:34.345659]  batch: 350  d_loss: -0.478563  g_loss: 0.883090\n",
      "Elapsed: [9:40:57.238992]  batch: 400  d_loss: -0.106920  g_loss: 0.487793\n",
      "Elapsed: [9:41:20.287867]  batch: 450  d_loss: -0.328809  g_loss: 0.519425\n",
      "Elapsed: [9:41:43.230009]  batch: 500  d_loss: -0.588301  g_loss: 0.754314\n",
      "Time taken for epoch: 246.152 secs\n",
      "ticker =  5501\n",
      "\n",
      "Epoch: 12\n",
      "Elapsed: [9:42:01.222563]  batch: 1  d_loss: -0.024365  g_loss: 0.270669\n",
      "Elapsed: [9:42:23.452699]  batch: 50  d_loss: 0.005318  g_loss: 0.178285\n",
      "Elapsed: [9:42:46.279596]  batch: 100  d_loss: -0.279565  g_loss: 0.546791\n",
      "Elapsed: [9:43:12.094599]  batch: 150  d_loss: -0.226209  g_loss: 0.600112\n",
      "Elapsed: [9:43:35.767918]  batch: 200  d_loss: -0.337378  g_loss: 0.712904\n",
      "Elapsed: [9:43:58.792023]  batch: 250  d_loss: -0.575079  g_loss: 0.836515\n",
      "Elapsed: [9:44:21.798267]  batch: 300  d_loss: -0.084069  g_loss: 0.349765\n",
      "Elapsed: [9:44:44.766092]  batch: 350  d_loss: -0.501603  g_loss: 0.819772\n",
      "Elapsed: [9:45:08.602222]  batch: 400  d_loss: -0.294026  g_loss: 0.634225\n",
      "Elapsed: [9:45:31.483251]  batch: 450  d_loss: -0.133051  g_loss: 0.449802\n",
      "Elapsed: [9:45:54.403758]  batch: 500  d_loss: -0.115314  g_loss: 0.514737\n",
      "Time taken for epoch: 251.204 secs\n",
      "ticker =  6001\n",
      "\n",
      "Epoch: 13\n",
      "Elapsed: [9:46:12.747633]  batch: 1  d_loss: -0.485244  g_loss: 0.796125\n",
      "Elapsed: [9:46:34.921555]  batch: 50  d_loss: 0.049393  g_loss: 0.012902\n",
      "Elapsed: [9:46:57.821347]  batch: 100  d_loss: -0.539262  g_loss: 0.732028\n",
      "Elapsed: [9:47:20.699675]  batch: 150  d_loss: -0.242793  g_loss: 0.426071\n",
      "Elapsed: [9:47:43.546605]  batch: 200  d_loss: -0.421409  g_loss: 0.661865\n",
      "Elapsed: [9:48:06.494940]  batch: 250  d_loss: -0.096406  g_loss: 0.300768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: [9:48:29.449456]  batch: 300  d_loss: -0.146893  g_loss: 0.567128\n",
      "Elapsed: [9:48:52.417532]  batch: 350  d_loss: 0.075587  g_loss: 0.032137\n",
      "Elapsed: [9:49:15.719265]  batch: 400  d_loss: -0.376391  g_loss: 0.656198\n",
      "Elapsed: [9:49:38.712755]  batch: 450  d_loss: -0.448668  g_loss: 0.746570\n",
      "Elapsed: [9:50:01.618520]  batch: 500  d_loss: -0.067988  g_loss: 0.554523\n",
      "Time taken for epoch: 246.896 secs\n",
      "ticker =  6501\n",
      "\n",
      "Epoch: 14\n",
      "Elapsed: [9:50:25.214967]  batch: 1  d_loss: -0.408245  g_loss: 0.744202\n",
      "Elapsed: [9:50:47.782266]  batch: 50  d_loss: -0.228984  g_loss: 0.477234\n",
      "Elapsed: [9:51:11.320945]  batch: 100  d_loss: -0.109082  g_loss: 0.551876\n",
      "Elapsed: [9:51:34.256160]  batch: 150  d_loss: 0.029167  g_loss: 0.195115\n",
      "Elapsed: [9:51:57.141140]  batch: 200  d_loss: -0.056726  g_loss: 0.188183\n",
      "Elapsed: [9:52:20.041898]  batch: 250  d_loss: -0.615538  g_loss: 0.824449\n",
      "Elapsed: [9:52:42.937457]  batch: 300  d_loss: 0.073851  g_loss: 0.027704\n",
      "Elapsed: [9:53:05.883186]  batch: 350  d_loss: -0.706828  g_loss: 0.896973\n",
      "Elapsed: [9:53:28.803676]  batch: 400  d_loss: 0.045863  g_loss: 0.165167\n",
      "Elapsed: [9:53:51.729494]  batch: 450  d_loss: -0.621100  g_loss: 0.939503\n",
      "Elapsed: [9:54:14.746924]  batch: 500  d_loss: -0.078010  g_loss: 0.356732\n",
      "Time taken for epoch: 252.939 secs\n",
      "ticker =  7001\n",
      "\n",
      "Epoch: 15\n",
      "Elapsed: [9:54:33.105077]  batch: 1  d_loss: -0.548706  g_loss: 0.918128\n",
      "Elapsed: [9:54:55.260427]  batch: 50  d_loss: -0.026891  g_loss: 0.142452\n",
      "Elapsed: [9:55:18.105073]  batch: 100  d_loss: -0.458607  g_loss: 0.774583\n",
      "Elapsed: [9:55:40.975535]  batch: 150  d_loss: -0.022252  g_loss: 0.299611\n",
      "Elapsed: [9:56:03.864493]  batch: 200  d_loss: -0.440075  g_loss: 0.792058\n",
      "Elapsed: [9:56:26.958022]  batch: 250  d_loss: 0.020785  g_loss: 0.216988\n",
      "Elapsed: [9:56:49.892239]  batch: 300  d_loss: -0.529259  g_loss: 0.824241\n",
      "Elapsed: [9:57:12.808466]  batch: 350  d_loss: 0.121114  g_loss: 0.085536\n",
      "Elapsed: [9:57:35.701465]  batch: 400  d_loss: 0.000908  g_loss: 0.104988\n",
      "Elapsed: [9:57:58.620894]  batch: 450  d_loss: -0.329789  g_loss: 0.675599\n",
      "Elapsed: [9:58:21.625201]  batch: 500  d_loss: 0.001297  g_loss: 0.056240\n",
      "Time taken for epoch: 246.670 secs\n",
      "ticker =  7501\n",
      "\n",
      "Epoch: 16\n",
      "Elapsed: [9:58:39.764252]  batch: 1  d_loss: 0.025048  g_loss: 0.070996\n",
      "Elapsed: [9:59:01.956388]  batch: 50  d_loss: -0.220475  g_loss: 0.520496\n",
      "Elapsed: [9:59:24.818020]  batch: 100  d_loss: -0.479319  g_loss: 0.830123\n",
      "Elapsed: [9:59:47.698839]  batch: 150  d_loss: -0.450526  g_loss: 0.697700\n",
      "Elapsed: [10:00:10.564454]  batch: 200  d_loss: -0.393708  g_loss: 0.823944\n",
      "Elapsed: [10:00:33.458190]  batch: 250  d_loss: -0.175754  g_loss: 0.320809\n",
      "Elapsed: [10:00:56.364892]  batch: 300  d_loss: -0.035601  g_loss: 0.253556\n",
      "Elapsed: [10:01:19.305595]  batch: 350  d_loss: 0.005176  g_loss: 0.272804\n",
      "Elapsed: [10:01:42.305979]  batch: 400  d_loss: -0.278128  g_loss: 0.618967\n",
      "Elapsed: [10:02:05.253107]  batch: 450  d_loss: -0.587453  g_loss: 0.923546\n",
      "Elapsed: [10:02:28.219617]  batch: 500  d_loss: -0.278677  g_loss: 0.699733\n",
      "Time taken for epoch: 246.490 secs\n",
      "ticker =  8001\n",
      "\n",
      "Epoch: 17\n",
      "Elapsed: [10:02:46.614594]  batch: 1  d_loss: -0.231483  g_loss: 0.696889\n",
      "Elapsed: [10:03:08.728680]  batch: 50  d_loss: -0.211171  g_loss: 0.518099\n",
      "Elapsed: [10:03:31.582480]  batch: 100  d_loss: -0.237204  g_loss: 0.416254\n",
      "Elapsed: [10:03:54.452409]  batch: 150  d_loss: -0.529923  g_loss: 0.781607\n",
      "Elapsed: [10:04:17.552817]  batch: 200  d_loss: -0.399681  g_loss: 0.548871\n",
      "Elapsed: [10:04:40.601551]  batch: 250  d_loss: -0.435451  g_loss: 0.770099\n",
      "Elapsed: [10:05:03.742396]  batch: 300  d_loss: -0.081773  g_loss: 0.410057\n",
      "Elapsed: [10:05:26.803008]  batch: 350  d_loss: -0.109221  g_loss: 0.483623\n",
      "Elapsed: [10:05:49.796435]  batch: 400  d_loss: -0.389822  g_loss: 0.772979\n",
      "Elapsed: [10:06:12.816466]  batch: 450  d_loss: -0.095803  g_loss: 0.530810\n",
      "Elapsed: [10:06:35.800079]  batch: 500  d_loss: -0.300504  g_loss: 0.627868\n",
      "Time taken for epoch: 247.319 secs\n",
      "ticker =  8501\n",
      "\n",
      "Epoch: 18\n",
      "Elapsed: [10:06:54.081685]  batch: 1  d_loss: -0.313851  g_loss: 0.627042\n",
      "Elapsed: [10:07:16.270678]  batch: 50  d_loss: -0.294731  g_loss: 0.553492\n",
      "Elapsed: [10:07:39.120624]  batch: 100  d_loss: -0.317168  g_loss: 0.544743\n",
      "Elapsed: [10:08:01.988516]  batch: 150  d_loss: -0.441005  g_loss: 0.739445\n",
      "Elapsed: [10:08:24.882225]  batch: 200  d_loss: -0.382751  g_loss: 0.756539\n",
      "Elapsed: [10:08:47.827288]  batch: 250  d_loss: -0.225719  g_loss: 0.561907\n",
      "Elapsed: [10:09:10.719544]  batch: 300  d_loss: -0.454204  g_loss: 0.819402\n",
      "Elapsed: [10:09:34.212417]  batch: 350  d_loss: -0.546199  g_loss: 0.873351\n",
      "Elapsed: [10:09:57.198314]  batch: 400  d_loss: -0.357973  g_loss: 0.617035\n",
      "Elapsed: [10:10:20.166771]  batch: 450  d_loss: -0.032341  g_loss: 0.320752\n",
      "Elapsed: [10:10:43.131841]  batch: 500  d_loss: -0.024444  g_loss: 0.337113\n",
      "Time taken for epoch: 247.037 secs\n",
      "ticker =  9001\n",
      "\n",
      "Epoch: 19\n",
      "Elapsed: [10:11:03.607536]  batch: 1  d_loss: -0.539128  g_loss: 0.867231\n",
      "Elapsed: [10:11:25.674724]  batch: 50  d_loss: -0.506584  g_loss: 0.748714\n",
      "Elapsed: [10:11:48.419273]  batch: 100  d_loss: -0.127142  g_loss: 0.319861\n",
      "Elapsed: [10:12:11.213087]  batch: 150  d_loss: -0.087325  g_loss: 0.321748\n",
      "Elapsed: [10:12:34.026258]  batch: 200  d_loss: -0.695177  g_loss: 0.894656\n",
      "Elapsed: [10:12:56.850012]  batch: 250  d_loss: -0.644081  g_loss: 1.042201\n",
      "Elapsed: [10:13:19.725933]  batch: 300  d_loss: -0.560489  g_loss: 0.886946\n",
      "Elapsed: [10:13:42.631496]  batch: 350  d_loss: -0.123118  g_loss: 0.408173\n",
      "Elapsed: [10:14:05.462497]  batch: 400  d_loss: -0.123839  g_loss: 0.456723\n",
      "Elapsed: [10:14:28.345743]  batch: 450  d_loss: 0.000131  g_loss: 0.217147\n",
      "Elapsed: [10:14:51.200372]  batch: 500  d_loss: -0.034379  g_loss: 0.187799\n",
      "Time taken for epoch: 247.840 secs\n",
      "ticker =  9501\n",
      "\n",
      "Epoch: 20\n",
      "Elapsed: [10:15:09.038408]  batch: 1  d_loss: -0.285486  g_loss: 0.532797\n",
      "Elapsed: [10:15:31.156610]  batch: 50  d_loss: -0.118230  g_loss: 0.382441\n",
      "Elapsed: [10:15:53.962267]  batch: 100  d_loss: 0.044235  g_loss: 0.214584\n",
      "Elapsed: [10:16:16.796757]  batch: 150  d_loss: -0.591111  g_loss: 0.919090\n",
      "Elapsed: [10:16:39.609321]  batch: 200  d_loss: -0.168041  g_loss: 0.452313\n",
      "Elapsed: [10:17:02.442614]  batch: 250  d_loss: -0.207507  g_loss: 0.551295\n",
      "Elapsed: [10:17:28.778274]  batch: 300  d_loss: -0.566760  g_loss: 0.886071\n",
      "Elapsed: [10:17:52.401255]  batch: 350  d_loss: -0.646222  g_loss: 0.836461\n",
      "Elapsed: [10:18:16.191313]  batch: 400  d_loss: -0.509481  g_loss: 0.898986\n",
      "Elapsed: [10:18:39.074334]  batch: 450  d_loss: -0.368691  g_loss: 0.608437\n",
      "Elapsed: [10:19:01.946709]  batch: 500  d_loss: -0.144719  g_loss: 0.361964\n",
      "Time taken for epoch: 250.586 secs\n",
      "ticker =  10001\n",
      "\n",
      "Epoch: 21\n",
      "Elapsed: [10:19:21.133959]  batch: 1  d_loss: -0.528084  g_loss: 0.793552\n",
      "Elapsed: [10:19:43.085217]  batch: 50  d_loss: -0.583848  g_loss: 0.828306\n",
      "Elapsed: [10:20:05.914264]  batch: 100  d_loss: -0.249393  g_loss: 0.640341\n",
      "Elapsed: [10:20:28.734552]  batch: 150  d_loss: -0.573588  g_loss: 0.930207\n",
      "Elapsed: [10:20:51.546402]  batch: 200  d_loss: -0.001778  g_loss: 0.318437\n",
      "Elapsed: [10:21:14.399245]  batch: 250  d_loss: -0.491806  g_loss: 0.821127\n",
      "Elapsed: [10:21:37.237158]  batch: 300  d_loss: -0.289977  g_loss: 0.561625\n",
      "Elapsed: [10:22:00.062284]  batch: 350  d_loss: -0.091695  g_loss: 0.710326\n",
      "Elapsed: [10:22:22.958207]  batch: 400  d_loss: -0.131020  g_loss: 0.424002\n",
      "Elapsed: [10:22:45.878493]  batch: 450  d_loss: -0.415559  g_loss: 0.780818\n",
      "Elapsed: [10:23:08.734695]  batch: 500  d_loss: -0.006831  g_loss: 0.363087\n",
      "Time taken for epoch: 246.591 secs\n",
      "ticker =  10501\n",
      "\n",
      "Epoch: 22\n",
      "Elapsed: [10:23:26.625100]  batch: 1  d_loss: -0.505387  g_loss: 0.846334\n",
      "Elapsed: [10:23:48.674874]  batch: 50  d_loss: -0.182735  g_loss: 0.292812\n",
      "Elapsed: [10:24:11.451117]  batch: 100  d_loss: 0.045514  g_loss: 0.070847\n",
      "Elapsed: [10:24:34.251866]  batch: 150  d_loss: -0.038348  g_loss: 0.237376\n",
      "Elapsed: [10:24:57.076888]  batch: 200  d_loss: -0.103979  g_loss: 0.456545\n",
      "Elapsed: [10:25:19.906158]  batch: 250  d_loss: -0.055853  g_loss: 0.167525\n",
      "Elapsed: [10:25:42.727948]  batch: 300  d_loss: 0.032324  g_loss: 0.303464\n",
      "Elapsed: [10:26:05.581724]  batch: 350  d_loss: -0.557028  g_loss: 0.914644\n",
      "Elapsed: [10:26:28.769304]  batch: 400  d_loss: -0.135779  g_loss: 0.474522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: [10:26:51.688379]  batch: 450  d_loss: -0.418052  g_loss: 0.762367\n",
      "Elapsed: [10:27:15.058281]  batch: 500  d_loss: -0.114389  g_loss: 0.525963\n",
      "Time taken for epoch: 246.200 secs\n",
      "ticker =  11001\n",
      "\n",
      "Epoch: 23\n",
      "Elapsed: [10:27:32.883880]  batch: 1  d_loss: -0.100677  g_loss: 0.526713\n",
      "Elapsed: [10:27:54.947373]  batch: 50  d_loss: -0.466028  g_loss: 0.887177\n",
      "Elapsed: [10:28:17.778267]  batch: 100  d_loss: 0.008752  g_loss: 0.336629\n",
      "Elapsed: [10:28:40.616321]  batch: 150  d_loss: -0.065025  g_loss: 0.408599\n",
      "Elapsed: [10:29:03.786828]  batch: 200  d_loss: 0.055035  g_loss: 0.019471\n",
      "Elapsed: [10:29:28.200948]  batch: 250  d_loss: -0.289531  g_loss: 0.502360\n",
      "Elapsed: [10:29:51.273554]  batch: 300  d_loss: -0.330378  g_loss: 0.747442\n",
      "Elapsed: [10:30:14.111592]  batch: 350  d_loss: 0.095855  g_loss: 0.196347\n",
      "Elapsed: [10:30:37.031774]  batch: 400  d_loss: -0.498309  g_loss: 0.920725\n",
      "Elapsed: [10:30:59.903633]  batch: 450  d_loss: -0.204132  g_loss: 0.508778\n",
      "Elapsed: [10:31:22.850116]  batch: 500  d_loss: -0.049645  g_loss: 0.236727\n",
      "Time taken for epoch: 247.550 secs\n",
      "ticker =  11501\n",
      "\n",
      "Epoch: 24\n",
      "Elapsed: [10:31:40.777085]  batch: 1  d_loss: -0.544850  g_loss: 0.728290\n",
      "Elapsed: [10:32:02.840624]  batch: 50  d_loss: -0.095870  g_loss: 0.442515\n",
      "Elapsed: [10:32:25.664723]  batch: 100  d_loss: -0.353757  g_loss: 0.694440\n",
      "Elapsed: [10:32:48.511169]  batch: 150  d_loss: -0.201961  g_loss: 0.592906\n",
      "Elapsed: [10:33:11.360165]  batch: 200  d_loss: -0.217741  g_loss: 0.633324\n",
      "Elapsed: [10:33:34.221858]  batch: 250  d_loss: -0.163877  g_loss: 0.403537\n",
      "Elapsed: [10:33:57.075030]  batch: 300  d_loss: 0.086968  g_loss: 0.080121\n",
      "Elapsed: [10:34:20.020158]  batch: 350  d_loss: -0.418854  g_loss: 0.703377\n",
      "Elapsed: [10:34:43.001652]  batch: 400  d_loss: -0.049614  g_loss: 0.248526\n",
      "Elapsed: [10:35:05.900485]  batch: 450  d_loss: 0.070408  g_loss: -0.027009\n",
      "Elapsed: [10:35:29.806144]  batch: 500  d_loss: 0.025151  g_loss: 0.191548\n",
      "Time taken for epoch: 247.367 secs\n",
      "ticker =  12001\n",
      "\n",
      "Epoch: 25\n",
      "Elapsed: [10:35:58.842827]  batch: 1  d_loss: -0.598762  g_loss: 0.853700\n",
      "Elapsed: [10:36:20.179125]  batch: 50  d_loss: -0.009659  g_loss: 0.306801\n",
      "Elapsed: [10:36:45.630589]  batch: 100  d_loss: -0.392827  g_loss: 0.618774\n",
      "Elapsed: [10:37:11.640180]  batch: 150  d_loss: -0.290045  g_loss: 0.582884\n",
      "Elapsed: [10:37:35.554053]  batch: 200  d_loss: -0.358879  g_loss: 0.625810\n",
      "Elapsed: [10:37:58.728951]  batch: 250  d_loss: -0.259582  g_loss: 0.662217\n",
      "Elapsed: [10:38:22.361389]  batch: 300  d_loss: -0.233966  g_loss: 0.527316\n",
      "Elapsed: [10:38:47.681219]  batch: 350  d_loss: -0.481685  g_loss: 0.752989\n",
      "Elapsed: [10:39:13.093708]  batch: 400  d_loss: 0.161908  g_loss: 0.142835\n",
      "Elapsed: [10:39:36.224571]  batch: 450  d_loss: -0.160731  g_loss: 0.535762\n",
      "Elapsed: [10:40:00.319616]  batch: 500  d_loss: -0.308935  g_loss: 0.581351\n",
      "Time taken for epoch: 269.751 secs\n",
      "ticker =  12501\n",
      "\n",
      "Epoch: 26\n",
      "Elapsed: [10:40:21.778443]  batch: 1  d_loss: 0.004434  g_loss: 0.350911\n",
      "Elapsed: [10:40:43.369760]  batch: 50  d_loss: -0.049456  g_loss: 0.351441\n",
      "Elapsed: [10:41:06.203797]  batch: 100  d_loss: -0.382182  g_loss: 0.658524\n",
      "Elapsed: [10:41:29.153228]  batch: 150  d_loss: -0.098323  g_loss: 0.589836\n",
      "Elapsed: [10:41:51.964473]  batch: 200  d_loss: 0.039631  g_loss: 0.094316\n",
      "Elapsed: [10:42:14.804611]  batch: 250  d_loss: -0.162932  g_loss: 0.424449\n",
      "Elapsed: [10:42:37.703305]  batch: 300  d_loss: 0.072333  g_loss: 0.047993\n",
      "Elapsed: [10:43:00.549208]  batch: 350  d_loss: -0.016699  g_loss: 0.243385\n",
      "Elapsed: [10:43:23.435155]  batch: 400  d_loss: -0.373343  g_loss: 0.640830\n",
      "Elapsed: [10:43:46.380865]  batch: 450  d_loss: -0.019353  g_loss: 0.343566\n",
      "Elapsed: [10:44:09.237978]  batch: 500  d_loss: -0.005923  g_loss: 0.200076\n",
      "Time taken for epoch: 248.461 secs\n",
      "ticker =  13001\n",
      "\n",
      "Epoch: 27\n",
      "Elapsed: [10:44:27.888438]  batch: 1  d_loss: -0.330962  g_loss: 0.583843\n",
      "Elapsed: [10:44:49.821625]  batch: 50  d_loss: -0.345687  g_loss: 0.597919\n",
      "Elapsed: [10:45:12.668224]  batch: 100  d_loss: -0.184782  g_loss: 0.482413\n",
      "Elapsed: [10:45:35.509662]  batch: 150  d_loss: -0.326089  g_loss: 0.627487\n",
      "Elapsed: [10:45:58.340666]  batch: 200  d_loss: -0.287590  g_loss: 0.617949\n",
      "Elapsed: [10:46:21.226888]  batch: 250  d_loss: -0.425150  g_loss: 0.710548\n",
      "Elapsed: [10:46:44.065931]  batch: 300  d_loss: 0.038324  g_loss: 0.122595\n",
      "Elapsed: [10:47:06.921485]  batch: 350  d_loss: -0.311186  g_loss: 0.586378\n",
      "Elapsed: [10:47:29.737759]  batch: 400  d_loss: -0.232987  g_loss: 0.613627\n",
      "Elapsed: [10:47:52.564442]  batch: 450  d_loss: -0.291045  g_loss: 0.478408\n",
      "Elapsed: [10:48:15.464339]  batch: 500  d_loss: -0.132954  g_loss: 0.420960\n",
      "Time taken for epoch: 245.959 secs\n",
      "ticker =  13501\n",
      "\n",
      "Epoch: 28\n",
      "Elapsed: [10:48:33.341389]  batch: 1  d_loss: -0.185553  g_loss: 0.426489\n",
      "Elapsed: [10:48:55.502483]  batch: 50  d_loss: -0.268365  g_loss: 0.506969\n",
      "Elapsed: [10:49:18.586050]  batch: 100  d_loss: -0.452642  g_loss: 0.665989\n",
      "Elapsed: [10:49:41.418021]  batch: 150  d_loss: -0.292670  g_loss: 0.452425\n",
      "Elapsed: [10:50:04.237332]  batch: 200  d_loss: -0.231792  g_loss: 0.536743\n",
      "Elapsed: [10:50:27.110234]  batch: 250  d_loss: -0.020908  g_loss: 0.173064\n",
      "Elapsed: [10:50:49.922130]  batch: 300  d_loss: -0.104636  g_loss: 0.469549\n",
      "Elapsed: [10:51:12.749431]  batch: 350  d_loss: -0.340829  g_loss: 0.664157\n",
      "Elapsed: [10:51:35.566631]  batch: 400  d_loss: -0.331230  g_loss: 0.660547\n",
      "Elapsed: [10:51:58.383259]  batch: 450  d_loss: -0.507592  g_loss: 0.703689\n",
      "Elapsed: [10:52:21.193362]  batch: 500  d_loss: -0.079899  g_loss: 0.287181\n",
      "Time taken for epoch: 245.621 secs\n",
      "ticker =  14001\n",
      "\n",
      "Epoch: 29\n",
      "Elapsed: [10:52:38.802612]  batch: 1  d_loss: -0.341538  g_loss: 0.569628\n",
      "Elapsed: [10:53:00.879448]  batch: 50  d_loss: -0.300698  g_loss: 0.758653\n",
      "Elapsed: [10:53:23.655270]  batch: 100  d_loss: -0.206307  g_loss: 0.654100\n",
      "Elapsed: [10:53:46.450427]  batch: 150  d_loss: -0.390884  g_loss: 0.600847\n",
      "Elapsed: [10:54:09.250348]  batch: 200  d_loss: 0.007696  g_loss: 0.335809\n",
      "Elapsed: [10:54:32.102445]  batch: 250  d_loss: -0.150125  g_loss: 0.324494\n",
      "Elapsed: [10:54:54.945917]  batch: 300  d_loss: 0.013703  g_loss: 0.154816\n",
      "Elapsed: [10:55:17.797027]  batch: 350  d_loss: -0.288624  g_loss: 0.515250\n",
      "Elapsed: [10:55:40.712364]  batch: 400  d_loss: -0.379084  g_loss: 0.591663\n",
      "Elapsed: [10:56:03.524508]  batch: 450  d_loss: -0.144542  g_loss: 0.667473\n",
      "Elapsed: [10:56:26.382841]  batch: 500  d_loss: -0.365853  g_loss: 0.573224\n",
      "Time taken for epoch: 245.131 secs\n",
      "ticker =  14501\n",
      "\n",
      "Epoch: 30\n",
      "Elapsed: [10:56:44.496191]  batch: 1  d_loss: -0.120087  g_loss: 0.474798\n",
      "Elapsed: [10:57:06.488928]  batch: 50  d_loss: -0.202493  g_loss: 0.700112\n",
      "Elapsed: [10:57:29.247369]  batch: 100  d_loss: -0.004015  g_loss: 0.356305\n",
      "Elapsed: [10:57:52.026804]  batch: 150  d_loss: 0.037175  g_loss: 0.061292\n",
      "Elapsed: [10:58:14.841831]  batch: 200  d_loss: -0.229596  g_loss: 0.659203\n",
      "Elapsed: [10:58:37.761202]  batch: 250  d_loss: -0.353100  g_loss: 0.577058\n",
      "Elapsed: [10:59:00.625731]  batch: 300  d_loss: -0.227176  g_loss: 0.454507\n",
      "Elapsed: [10:59:23.514303]  batch: 350  d_loss: 0.080722  g_loss: 0.131068\n",
      "Elapsed: [10:59:46.373783]  batch: 400  d_loss: -0.012994  g_loss: 0.270801\n",
      "Elapsed: [11:00:09.292575]  batch: 450  d_loss: -0.354198  g_loss: 0.638514\n",
      "Elapsed: [11:00:32.130726]  batch: 500  d_loss: 0.069701  g_loss: 0.054384\n",
      "Time taken for epoch: 245.299 secs\n",
      "ticker =  15001\n",
      "\n",
      "Epoch: 31\n",
      "Elapsed: [11:00:49.941655]  batch: 1  d_loss: -0.379874  g_loss: 0.511683\n",
      "Elapsed: [11:01:12.063394]  batch: 50  d_loss: -0.331695  g_loss: 0.615703\n",
      "Elapsed: [11:01:34.863569]  batch: 100  d_loss: -0.229709  g_loss: 0.537208\n",
      "Elapsed: [11:01:57.677209]  batch: 150  d_loss: -0.408587  g_loss: 0.719937\n",
      "Elapsed: [11:02:23.524638]  batch: 200  d_loss: -0.225933  g_loss: 0.516451\n",
      "Elapsed: [11:02:47.283320]  batch: 250  d_loss: -0.088669  g_loss: 0.218734\n",
      "Elapsed: [11:03:10.951316]  batch: 300  d_loss: -0.091105  g_loss: 0.444729\n",
      "Elapsed: [11:03:33.848337]  batch: 350  d_loss: -0.324420  g_loss: 0.567569\n",
      "Elapsed: [11:03:56.728470]  batch: 400  d_loss: -0.004990  g_loss: 0.187902\n",
      "Elapsed: [11:04:20.100889]  batch: 450  d_loss: -0.115051  g_loss: 0.433517\n",
      "Elapsed: [11:04:43.088916]  batch: 500  d_loss: -0.010505  g_loss: 0.247926\n",
      "Time taken for epoch: 250.751 secs\n",
      "ticker =  15501\n",
      "\n",
      "Epoch: 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: [11:05:02.063348]  batch: 1  d_loss: -0.294566  g_loss: 0.538529\n",
      "Elapsed: [11:05:24.042969]  batch: 50  d_loss: -0.289872  g_loss: 0.447346\n",
      "Elapsed: [11:05:46.867591]  batch: 100  d_loss: -0.154445  g_loss: 0.412152\n",
      "Elapsed: [11:06:09.700045]  batch: 150  d_loss: -0.498971  g_loss: 0.842882\n",
      "Elapsed: [11:06:32.615340]  batch: 200  d_loss: -0.198879  g_loss: 0.504366\n",
      "Elapsed: [11:06:55.443177]  batch: 250  d_loss: -0.042926  g_loss: 0.186766\n",
      "Elapsed: [11:07:18.286146]  batch: 300  d_loss: -0.024065  g_loss: 0.369699\n",
      "Elapsed: [11:07:41.124064]  batch: 350  d_loss: -0.286686  g_loss: 0.470618\n",
      "Elapsed: [11:08:03.987552]  batch: 400  d_loss: -0.159531  g_loss: 0.437069\n",
      "Elapsed: [11:08:26.873360]  batch: 450  d_loss: -0.172102  g_loss: 0.331119\n",
      "Elapsed: [11:08:49.719300]  batch: 500  d_loss: -0.133615  g_loss: 0.307979\n",
      "Time taken for epoch: 246.493 secs\n",
      "ticker =  16001\n",
      "\n",
      "Epoch: 33\n",
      "Elapsed: [11:09:07.529082]  batch: 1  d_loss: -0.016967  g_loss: 0.091174\n",
      "Elapsed: [11:09:29.914807]  batch: 50  d_loss: -0.041444  g_loss: 0.459376\n",
      "Elapsed: [11:09:52.751822]  batch: 100  d_loss: -0.385040  g_loss: 0.626484\n",
      "Elapsed: [11:10:17.144846]  batch: 150  d_loss: -0.362802  g_loss: 0.674757\n",
      "Elapsed: [11:10:42.338437]  batch: 200  d_loss: -0.401298  g_loss: 0.682450\n",
      "Elapsed: [11:11:05.401709]  batch: 250  d_loss: -0.157523  g_loss: 0.444877\n",
      "Elapsed: [11:11:29.065360]  batch: 300  d_loss: -0.332099  g_loss: 0.486160\n",
      "Elapsed: [11:11:52.107108]  batch: 350  d_loss: -0.005701  g_loss: 0.212151\n",
      "Elapsed: [11:12:15.088316]  batch: 400  d_loss: 0.027370  g_loss: 0.477668\n",
      "Elapsed: [11:12:38.030151]  batch: 450  d_loss: -0.105664  g_loss: 0.426744\n",
      "Elapsed: [11:13:00.982342]  batch: 500  d_loss: -0.179963  g_loss: 0.605789\n",
      "Time taken for epoch: 251.193 secs\n",
      "ticker =  16501\n",
      "\n",
      "Epoch: 34\n",
      "Elapsed: [11:13:18.669529]  batch: 1  d_loss: -0.164223  g_loss: 0.467141\n",
      "Elapsed: [11:13:40.629575]  batch: 50  d_loss: 0.036105  g_loss: -0.024555\n",
      "Elapsed: [11:14:03.425750]  batch: 100  d_loss: -0.213465  g_loss: 0.690772\n",
      "Elapsed: [11:14:26.296357]  batch: 150  d_loss: -0.279814  g_loss: 0.557501\n",
      "Elapsed: [11:14:49.100435]  batch: 200  d_loss: -0.068421  g_loss: 0.374307\n",
      "Elapsed: [11:15:11.958246]  batch: 250  d_loss: -0.228471  g_loss: 0.472868\n",
      "Elapsed: [11:15:34.775173]  batch: 300  d_loss: -0.177290  g_loss: 0.221744\n",
      "Elapsed: [11:15:57.579943]  batch: 350  d_loss: -0.254528  g_loss: 0.358334\n",
      "Elapsed: [11:16:20.511865]  batch: 400  d_loss: -0.315435  g_loss: 0.550775\n",
      "Elapsed: [11:16:43.463425]  batch: 450  d_loss: 0.015297  g_loss: 0.272192\n",
      "Elapsed: [11:17:06.302593]  batch: 500  d_loss: -0.154485  g_loss: 0.316013\n",
      "Time taken for epoch: 245.127 secs\n",
      "ticker =  17001\n",
      "\n",
      "Epoch: 35\n",
      "Elapsed: [11:17:25.362094]  batch: 1  d_loss: -0.118107  g_loss: 0.351689\n",
      "Elapsed: [11:17:47.230361]  batch: 50  d_loss: -0.053359  g_loss: 0.424492\n",
      "Elapsed: [11:18:09.995871]  batch: 100  d_loss: -0.031340  g_loss: 0.468273\n",
      "Elapsed: [11:18:32.821684]  batch: 150  d_loss: -0.296324  g_loss: 0.658432\n",
      "Elapsed: [11:18:55.844593]  batch: 200  d_loss: -0.319541  g_loss: 0.719065\n",
      "Elapsed: [11:19:18.665215]  batch: 250  d_loss: -0.150976  g_loss: 0.599988\n",
      "Elapsed: [11:19:41.510077]  batch: 300  d_loss: -0.077560  g_loss: 0.447770\n",
      "Elapsed: [11:20:04.331973]  batch: 350  d_loss: -0.013646  g_loss: 0.223812\n",
      "Elapsed: [11:20:27.169856]  batch: 400  d_loss: 0.005501  g_loss: 0.453106\n",
      "Elapsed: [11:20:49.978890]  batch: 450  d_loss: -0.156124  g_loss: 0.363782\n",
      "Elapsed: [11:21:12.913069]  batch: 500  d_loss: -0.435315  g_loss: 0.661738\n",
      "Time taken for epoch: 246.405 secs\n",
      "ticker =  17501\n",
      "\n",
      "Epoch: 36\n",
      "Elapsed: [11:21:30.604400]  batch: 1  d_loss: -0.021951  g_loss: 0.244280\n",
      "Elapsed: [11:21:52.640064]  batch: 50  d_loss: 0.042151  g_loss: 0.161383\n",
      "Elapsed: [11:22:15.423949]  batch: 100  d_loss: -0.117977  g_loss: 0.252067\n",
      "Elapsed: [11:22:38.271562]  batch: 150  d_loss: 0.013404  g_loss: 0.342506\n",
      "Elapsed: [11:23:01.070142]  batch: 200  d_loss: -0.237834  g_loss: 0.605496\n",
      "Elapsed: [11:23:23.896294]  batch: 250  d_loss: 0.013606  g_loss: 0.132966\n",
      "Elapsed: [11:23:46.747712]  batch: 300  d_loss: 0.008743  g_loss: 0.261560\n",
      "Elapsed: [11:24:09.608939]  batch: 350  d_loss: 0.029514  g_loss: 0.384438\n",
      "Elapsed: [11:24:32.536128]  batch: 400  d_loss: -0.034688  g_loss: 0.290171\n",
      "Elapsed: [11:24:55.371819]  batch: 450  d_loss: -0.097835  g_loss: 0.523286\n",
      "Elapsed: [11:25:18.217164]  batch: 500  d_loss: -0.030567  g_loss: 0.244625\n",
      "Time taken for epoch: 245.139 secs\n",
      "ticker =  18001\n",
      "\n",
      "Epoch: 37\n",
      "Elapsed: [11:25:37.716547]  batch: 1  d_loss: -0.202529  g_loss: 0.484649\n",
      "Elapsed: [11:25:59.617541]  batch: 50  d_loss: -0.156403  g_loss: 0.497606\n",
      "Elapsed: [11:26:22.460482]  batch: 100  d_loss: -0.334959  g_loss: 0.578254\n",
      "Elapsed: [11:26:45.459087]  batch: 150  d_loss: -0.233896  g_loss: 0.522501\n",
      "Elapsed: [11:27:08.272869]  batch: 200  d_loss: -0.396748  g_loss: 0.721382\n",
      "Elapsed: [11:27:31.070562]  batch: 250  d_loss: -0.238179  g_loss: 0.591621\n",
      "Elapsed: [11:27:53.860613]  batch: 300  d_loss: -0.221226  g_loss: 0.451388\n",
      "Elapsed: [11:28:16.682567]  batch: 350  d_loss: -0.407044  g_loss: 0.457482\n",
      "Elapsed: [11:28:39.632015]  batch: 400  d_loss: 0.145521  g_loss: 0.212263\n",
      "Elapsed: [11:29:02.529010]  batch: 450  d_loss: -0.219715  g_loss: 0.483574\n",
      "Elapsed: [11:29:27.133010]  batch: 500  d_loss: -0.207492  g_loss: 0.614851\n",
      "Time taken for epoch: 248.902 secs\n",
      "ticker =  18501\n",
      "\n",
      "Epoch: 38\n",
      "Elapsed: [11:29:46.204359]  batch: 1  d_loss: -0.142052  g_loss: 0.442837\n",
      "Elapsed: [11:30:08.061279]  batch: 50  d_loss: -0.397739  g_loss: 0.727112\n",
      "Elapsed: [11:30:30.879792]  batch: 100  d_loss: -0.457000  g_loss: 0.545220\n",
      "Elapsed: [11:30:53.683846]  batch: 150  d_loss: -0.264921  g_loss: 0.445789\n",
      "Elapsed: [11:31:16.506364]  batch: 200  d_loss: -0.375361  g_loss: 0.657406\n",
      "Elapsed: [11:31:39.317689]  batch: 250  d_loss: -0.332916  g_loss: 0.602765\n",
      "Elapsed: [11:32:02.145411]  batch: 300  d_loss: -0.105165  g_loss: 0.126099\n",
      "Elapsed: [11:32:24.982940]  batch: 350  d_loss: -0.020270  g_loss: 0.340930\n",
      "Elapsed: [11:32:47.829950]  batch: 400  d_loss: -0.035387  g_loss: 0.362401\n",
      "Elapsed: [11:33:10.775198]  batch: 450  d_loss: -0.049672  g_loss: 0.245098\n",
      "Elapsed: [11:33:33.628164]  batch: 500  d_loss: 0.077460  g_loss: 0.077289\n",
      "Time taken for epoch: 246.055 secs\n",
      "ticker =  19001\n",
      "\n",
      "Epoch: 39\n",
      "Elapsed: [11:33:51.639582]  batch: 1  d_loss: -0.449361  g_loss: 0.522895\n",
      "Elapsed: [11:34:14.457504]  batch: 50  d_loss: -0.169778  g_loss: 0.508684\n",
      "Elapsed: [11:34:37.300836]  batch: 100  d_loss: -0.202845  g_loss: 0.535542\n",
      "Elapsed: [11:35:00.126118]  batch: 150  d_loss: -0.371758  g_loss: 0.653609\n",
      "Elapsed: [11:35:22.987911]  batch: 200  d_loss: -0.189945  g_loss: 0.504221\n",
      "Elapsed: [11:35:45.829319]  batch: 250  d_loss: -0.339229  g_loss: 0.770078\n",
      "Elapsed: [11:36:08.666284]  batch: 300  d_loss: -0.157814  g_loss: 0.440462\n",
      "Elapsed: [11:36:31.512057]  batch: 350  d_loss: -0.253800  g_loss: 0.603517\n",
      "Elapsed: [11:36:54.477044]  batch: 400  d_loss: -0.319673  g_loss: 0.665900\n",
      "Elapsed: [11:37:21.999436]  batch: 450  d_loss: 0.055398  g_loss: 0.163185\n",
      "Elapsed: [11:37:45.529053]  batch: 500  d_loss: -0.137400  g_loss: 0.377535\n",
      "Time taken for epoch: 251.738 secs\n",
      "ticker =  19501\n",
      "\n",
      "Epoch: 40\n",
      "Elapsed: [11:38:05.748461]  batch: 1  d_loss: -0.269293  g_loss: 0.515848\n",
      "Elapsed: [11:38:27.498140]  batch: 50  d_loss: 0.054472  g_loss: 0.252628\n",
      "Elapsed: [11:38:50.312976]  batch: 100  d_loss: -0.283530  g_loss: 0.323650\n",
      "Elapsed: [11:39:13.138712]  batch: 150  d_loss: -0.094508  g_loss: 0.437715\n",
      "Elapsed: [11:39:35.985132]  batch: 200  d_loss: -0.277821  g_loss: 0.621055\n",
      "Elapsed: [11:39:58.848709]  batch: 250  d_loss: -0.131332  g_loss: 0.571338\n",
      "Elapsed: [11:40:21.672197]  batch: 300  d_loss: -0.355558  g_loss: 0.649326\n",
      "Elapsed: [11:40:44.497216]  batch: 350  d_loss: -0.067786  g_loss: 0.409129\n",
      "Elapsed: [11:41:07.321975]  batch: 400  d_loss: -0.298065  g_loss: 0.607274\n",
      "Elapsed: [11:41:30.185004]  batch: 450  d_loss: 0.059873  g_loss: 0.155299\n",
      "Elapsed: [11:41:53.176484]  batch: 500  d_loss: 0.053809  g_loss: 0.165799\n",
      "Time taken for epoch: 247.463 secs\n",
      "ticker =  20001\n",
      "\n",
      "Epoch: 41\n",
      "Elapsed: [11:42:11.219448]  batch: 1  d_loss: -0.359377  g_loss: 0.689621\n",
      "Elapsed: [11:42:33.220325]  batch: 50  d_loss: -0.431846  g_loss: 0.654727\n",
      "Elapsed: [11:42:56.015050]  batch: 100  d_loss: -0.002783  g_loss: 0.298119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: [11:43:18.847692]  batch: 150  d_loss: -0.197946  g_loss: 0.445543\n",
      "Elapsed: [11:43:41.685519]  batch: 200  d_loss: 0.124401  g_loss: 0.018422\n",
      "Elapsed: [11:44:04.552541]  batch: 250  d_loss: -0.009504  g_loss: 0.245061\n",
      "Elapsed: [11:44:27.427385]  batch: 300  d_loss: -0.168857  g_loss: 0.500745\n",
      "Elapsed: [11:44:50.283050]  batch: 350  d_loss: -0.403092  g_loss: 0.653512\n",
      "Elapsed: [11:45:13.237742]  batch: 400  d_loss: -0.172574  g_loss: 0.511272\n",
      "Elapsed: [11:45:36.088856]  batch: 450  d_loss: -0.047129  g_loss: 0.300143\n",
      "Elapsed: [11:45:58.917501]  batch: 500  d_loss: -0.368407  g_loss: 0.562246\n",
      "Time taken for epoch: 245.620 secs\n",
      "ticker =  20501\n",
      "\n",
      "Epoch: 42\n",
      "Elapsed: [11:46:16.871932]  batch: 1  d_loss: 0.061845  g_loss: 0.277613\n",
      "Elapsed: [11:46:38.890992]  batch: 50  d_loss: -0.333816  g_loss: 0.708785\n",
      "Elapsed: [11:47:01.683084]  batch: 100  d_loss: -0.384758  g_loss: 0.578716\n",
      "Elapsed: [11:47:24.487967]  batch: 150  d_loss: -0.287932  g_loss: 0.635241\n",
      "Elapsed: [11:47:47.282948]  batch: 200  d_loss: -0.327109  g_loss: 0.438306\n",
      "Elapsed: [11:48:10.121463]  batch: 250  d_loss: -0.511477  g_loss: 0.751913\n",
      "Elapsed: [11:48:32.969630]  batch: 300  d_loss: -0.207075  g_loss: 0.486473\n",
      "Elapsed: [11:48:55.803032]  batch: 350  d_loss: -0.204168  g_loss: 0.363486\n",
      "Elapsed: [11:49:18.954355]  batch: 400  d_loss: 0.014980  g_loss: 0.201380\n",
      "Elapsed: [11:49:41.893439]  batch: 450  d_loss: -0.176659  g_loss: 0.621855\n",
      "Elapsed: [11:50:08.790601]  batch: 500  d_loss: -0.013867  g_loss: 0.226702\n",
      "Time taken for epoch: 249.983 secs\n",
      "ticker =  21001\n",
      "\n",
      "Epoch: 43\n",
      "Elapsed: [11:50:31.998109]  batch: 1  d_loss: -0.317719  g_loss: 0.615602\n",
      "Elapsed: [11:50:56.888081]  batch: 50  d_loss: -0.170831  g_loss: 0.498661\n",
      "Elapsed: [11:51:20.137114]  batch: 100  d_loss: -0.301650  g_loss: 0.682244\n",
      "Elapsed: [11:51:43.759218]  batch: 150  d_loss: -0.400289  g_loss: 0.546113\n",
      "Elapsed: [11:52:06.688253]  batch: 200  d_loss: 0.030995  g_loss: 0.313015\n",
      "Elapsed: [11:52:29.572577]  batch: 250  d_loss: -0.019960  g_loss: 0.370090\n",
      "Elapsed: [11:52:52.497449]  batch: 300  d_loss: -0.177066  g_loss: 0.366829\n",
      "Elapsed: [11:53:15.476588]  batch: 350  d_loss: -0.138499  g_loss: 0.472366\n",
      "Elapsed: [11:53:38.407731]  batch: 400  d_loss: -0.438315  g_loss: 0.472858\n",
      "Elapsed: [11:54:01.504467]  batch: 450  d_loss: -0.242674  g_loss: 0.553199\n",
      "Elapsed: [11:54:24.482931]  batch: 500  d_loss: -0.151763  g_loss: 0.482177\n",
      "Time taken for epoch: 255.143 secs\n",
      "ticker =  21501\n",
      "\n",
      "Epoch: 44\n",
      "Elapsed: [11:54:43.724142]  batch: 1  d_loss: -0.186363  g_loss: 0.532958\n",
      "Elapsed: [11:55:05.720499]  batch: 50  d_loss: -0.084874  g_loss: 0.398351\n",
      "Elapsed: [11:55:28.677516]  batch: 100  d_loss: 0.109726  g_loss: -0.128210\n",
      "Elapsed: [11:55:51.781681]  batch: 150  d_loss: -0.206822  g_loss: 0.615983\n",
      "Elapsed: [11:56:14.704393]  batch: 200  d_loss: -0.298245  g_loss: 0.676793\n",
      "Elapsed: [11:56:37.648221]  batch: 250  d_loss: -0.427111  g_loss: 0.575259\n",
      "Elapsed: [11:57:00.830705]  batch: 300  d_loss: -0.201171  g_loss: 0.536515\n",
      "Elapsed: [11:57:25.277704]  batch: 350  d_loss: -0.218526  g_loss: 0.658963\n",
      "Elapsed: [11:57:48.250280]  batch: 400  d_loss: -0.160199  g_loss: 0.532187\n",
      "Elapsed: [11:58:11.183414]  batch: 450  d_loss: 0.066212  g_loss: 0.076868\n",
      "Elapsed: [11:58:34.271918]  batch: 500  d_loss: 0.014172  g_loss: 0.269256\n",
      "Time taken for epoch: 249.682 secs\n",
      "ticker =  22001\n",
      "\n",
      "Epoch: 45\n",
      "Elapsed: [11:58:53.016716]  batch: 1  d_loss: -0.270928  g_loss: 0.656866\n",
      "Elapsed: [11:59:15.067029]  batch: 50  d_loss: -0.259849  g_loss: 0.511674\n",
      "Elapsed: [11:59:37.951260]  batch: 100  d_loss: 0.051641  g_loss: 0.202184\n",
      "Elapsed: [12:00:00.834363]  batch: 150  d_loss: -0.391389  g_loss: 0.454314\n",
      "Elapsed: [12:00:23.706091]  batch: 200  d_loss: 0.104625  g_loss: 0.087399\n",
      "Elapsed: [12:00:46.648128]  batch: 250  d_loss: -0.537895  g_loss: 0.634076\n",
      "Elapsed: [12:01:09.624430]  batch: 300  d_loss: -0.048581  g_loss: 0.415842\n",
      "Elapsed: [12:01:32.701985]  batch: 350  d_loss: 0.060784  g_loss: 0.054400\n",
      "Elapsed: [12:01:55.672497]  batch: 400  d_loss: -0.196004  g_loss: 0.576190\n",
      "Elapsed: [12:02:18.724569]  batch: 450  d_loss: -0.413737  g_loss: 0.746563\n",
      "Elapsed: [12:02:41.685834]  batch: 500  d_loss: -0.185436  g_loss: 0.605902\n",
      "Time taken for epoch: 247.127 secs\n",
      "ticker =  22501\n",
      "\n",
      "Epoch: 46\n",
      "Elapsed: [12:03:00.504693]  batch: 1  d_loss: -0.136528  g_loss: 0.595196\n",
      "Elapsed: [12:03:22.523994]  batch: 50  d_loss: -0.504483  g_loss: 0.660928\n",
      "Elapsed: [12:03:45.461636]  batch: 100  d_loss: 0.060316  g_loss: 0.214981\n",
      "Elapsed: [12:04:08.372728]  batch: 150  d_loss: -0.224587  g_loss: 0.479449\n",
      "Elapsed: [12:04:36.225704]  batch: 200  d_loss: -0.421007  g_loss: 0.747417\n",
      "Elapsed: [12:04:59.388368]  batch: 250  d_loss: 0.008393  g_loss: 0.287751\n",
      "Elapsed: [12:05:23.065885]  batch: 300  d_loss: 0.026095  g_loss: 0.302682\n",
      "Elapsed: [12:05:45.998968]  batch: 350  d_loss: -0.067613  g_loss: 0.411054\n",
      "Elapsed: [12:06:09.156506]  batch: 400  d_loss: -0.310407  g_loss: 0.529697\n",
      "Elapsed: [12:06:32.210731]  batch: 450  d_loss: -0.212654  g_loss: 0.512658\n",
      "Elapsed: [12:06:55.171934]  batch: 500  d_loss: -0.210896  g_loss: 0.534561\n",
      "Time taken for epoch: 253.372 secs\n",
      "ticker =  23001\n",
      "\n",
      "Epoch: 47\n",
      "Elapsed: [12:07:14.013082]  batch: 1  d_loss: 0.001538  g_loss: 0.361002\n",
      "Elapsed: [12:07:36.005807]  batch: 50  d_loss: -0.081587  g_loss: 0.405855\n",
      "Elapsed: [12:07:58.857093]  batch: 100  d_loss: -0.242847  g_loss: 0.605569\n",
      "Elapsed: [12:08:21.805703]  batch: 150  d_loss: -0.131132  g_loss: 0.368792\n",
      "Elapsed: [12:08:44.734386]  batch: 200  d_loss: 0.029550  g_loss: 0.330621\n",
      "Elapsed: [12:09:07.621193]  batch: 250  d_loss: -0.390968  g_loss: 0.547537\n",
      "Elapsed: [12:09:30.589007]  batch: 300  d_loss: -0.103939  g_loss: 0.389608\n",
      "Elapsed: [12:09:53.508106]  batch: 350  d_loss: -0.270774  g_loss: 0.565044\n",
      "Elapsed: [12:10:16.431613]  batch: 400  d_loss: -0.043010  g_loss: 0.347275\n",
      "Elapsed: [12:10:39.510718]  batch: 450  d_loss: -0.003995  g_loss: 0.223718\n",
      "Elapsed: [12:11:02.437661]  batch: 500  d_loss: -0.101159  g_loss: 0.480849\n",
      "Time taken for epoch: 247.153 secs\n",
      "ticker =  23501\n",
      "\n",
      "Epoch: 48\n",
      "Elapsed: [12:11:21.313728]  batch: 1  d_loss: -0.152500  g_loss: 0.396136\n",
      "Elapsed: [12:11:43.430896]  batch: 50  d_loss: -0.351601  g_loss: 0.500917\n",
      "Elapsed: [12:12:06.386539]  batch: 100  d_loss: -0.248933  g_loss: 0.573712\n",
      "Elapsed: [12:12:29.283586]  batch: 150  d_loss: -0.414817  g_loss: 0.687957\n",
      "Elapsed: [12:12:52.232315]  batch: 200  d_loss: -0.327688  g_loss: 0.657381\n",
      "Elapsed: [12:13:15.183848]  batch: 250  d_loss: -0.077762  g_loss: 0.464153\n",
      "Elapsed: [12:13:38.128251]  batch: 300  d_loss: -0.230433  g_loss: 0.597791\n",
      "Elapsed: [12:14:01.067450]  batch: 350  d_loss: -0.350341  g_loss: 0.580179\n",
      "Elapsed: [12:14:24.035346]  batch: 400  d_loss: 0.035489  g_loss: 0.385771\n",
      "Elapsed: [12:14:46.985804]  batch: 450  d_loss: -0.114070  g_loss: 0.395452\n",
      "Elapsed: [12:15:10.022896]  batch: 500  d_loss: -0.339766  g_loss: 0.644093\n",
      "Time taken for epoch: 247.348 secs\n",
      "ticker =  24001\n",
      "\n",
      "Epoch: 49\n",
      "Elapsed: [12:15:28.595679]  batch: 1  d_loss: 0.073873  g_loss: 0.193407\n",
      "Elapsed: [12:15:50.601461]  batch: 50  d_loss: -0.093965  g_loss: 0.422485\n",
      "Elapsed: [12:16:13.464079]  batch: 100  d_loss: -0.301923  g_loss: 0.543127\n",
      "Elapsed: [12:16:36.366174]  batch: 150  d_loss: -0.257618  g_loss: 0.552936\n",
      "Elapsed: [12:16:59.253236]  batch: 200  d_loss: -0.042451  g_loss: 0.425540\n",
      "Elapsed: [12:17:22.172308]  batch: 250  d_loss: -0.217339  g_loss: 0.524638\n",
      "Elapsed: [12:17:45.069834]  batch: 300  d_loss: -0.115291  g_loss: 0.530025\n",
      "Elapsed: [12:18:08.021536]  batch: 350  d_loss: -0.003947  g_loss: 0.281266\n",
      "Elapsed: [12:18:30.983698]  batch: 400  d_loss: 0.025557  g_loss: 0.291222\n",
      "Elapsed: [12:18:53.887626]  batch: 450  d_loss: -0.273519  g_loss: 0.523794\n",
      "Elapsed: [12:19:17.107713]  batch: 500  d_loss: -0.260354  g_loss: 0.544561\n",
      "Time taken for epoch: 246.892 secs\n",
      "ticker =  24501\n",
      "\n",
      "Epoch: 50\n",
      "Elapsed: [12:19:35.464490]  batch: 1  d_loss: 0.087775  g_loss: 0.261919\n",
      "Elapsed: [12:19:57.542098]  batch: 50  d_loss: 0.125157  g_loss: -0.097944\n",
      "Elapsed: [12:20:20.487314]  batch: 100  d_loss: -0.104818  g_loss: 0.446663\n",
      "Elapsed: [12:20:43.382274]  batch: 150  d_loss: -0.323629  g_loss: 0.527800\n",
      "Elapsed: [12:21:06.297983]  batch: 200  d_loss: -0.363449  g_loss: 0.635846\n",
      "Elapsed: [12:21:29.205299]  batch: 250  d_loss: -0.193952  g_loss: 0.365834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: [12:21:52.068820]  batch: 300  d_loss: -0.129313  g_loss: 0.505619\n",
      "Elapsed: [12:22:14.935496]  batch: 350  d_loss: -0.283575  g_loss: 0.685740\n",
      "Elapsed: [12:22:37.969591]  batch: 400  d_loss: -0.385293  g_loss: 0.644142\n",
      "Elapsed: [12:23:00.847033]  batch: 450  d_loss: -0.247667  g_loss: 0.413675\n",
      "Elapsed: [12:23:23.781325]  batch: 500  d_loss: -0.386836  g_loss: 0.642485\n",
      "Time taken for epoch: 246.518 secs\n",
      "ticker =  25001\n",
      "\n",
      "Epoch: 51\n",
      "Elapsed: [12:23:42.338673]  batch: 1  d_loss: -0.059681  g_loss: 0.416623\n",
      "Elapsed: [12:24:04.402638]  batch: 50  d_loss: -0.491170  g_loss: 0.626887\n",
      "Elapsed: [12:24:27.243624]  batch: 100  d_loss: 0.029429  g_loss: 0.368787\n",
      "Elapsed: [12:24:50.064048]  batch: 150  d_loss: -0.158903  g_loss: 0.584510\n",
      "Elapsed: [12:25:12.931231]  batch: 200  d_loss: 0.055010  g_loss: 0.204318\n",
      "Elapsed: [12:25:35.931249]  batch: 250  d_loss: -0.176446  g_loss: 0.594470\n",
      "Elapsed: [12:25:58.790118]  batch: 300  d_loss: -0.073260  g_loss: 0.209381\n",
      "Elapsed: [12:26:21.660857]  batch: 350  d_loss: 0.034451  g_loss: 0.244621\n",
      "Elapsed: [12:26:44.535438]  batch: 400  d_loss: -0.137462  g_loss: 0.546173\n",
      "Elapsed: [12:27:07.723184]  batch: 450  d_loss: -0.034550  g_loss: 0.277444\n",
      "Elapsed: [12:27:30.608673]  batch: 500  d_loss: -0.399550  g_loss: 0.628599\n",
      "Time taken for epoch: 246.599 secs\n",
      "ticker =  25501\n",
      "\n",
      "Epoch: 52\n",
      "Elapsed: [12:27:48.969490]  batch: 1  d_loss: 0.108730  g_loss: -0.031717\n",
      "Elapsed: [12:28:10.989936]  batch: 50  d_loss: 0.055721  g_loss: 0.564500\n",
      "Elapsed: [12:28:33.804046]  batch: 100  d_loss: 0.050246  g_loss: 0.024063\n",
      "Elapsed: [12:28:56.690254]  batch: 150  d_loss: -0.138667  g_loss: 0.529097\n",
      "Elapsed: [12:29:20.966080]  batch: 200  d_loss: -0.315989  g_loss: 0.644044\n",
      "Elapsed: [12:29:44.402535]  batch: 250  d_loss: -0.188121  g_loss: 0.478060\n",
      "Elapsed: [12:30:07.279875]  batch: 300  d_loss: 0.124285  g_loss: 0.074431\n",
      "Elapsed: [12:30:30.230942]  batch: 350  d_loss: -0.018637  g_loss: 0.441267\n",
      "Elapsed: [12:30:53.179644]  batch: 400  d_loss: -0.052331  g_loss: 0.375485\n",
      "Elapsed: [12:31:16.149799]  batch: 450  d_loss: -0.124886  g_loss: 0.336550\n",
      "Elapsed: [12:31:39.147859]  batch: 500  d_loss: -0.100979  g_loss: 0.352318\n",
      "Time taken for epoch: 248.321 secs\n",
      "ticker =  26001\n",
      "\n",
      "Epoch: 53\n",
      "Elapsed: [12:31:57.292123]  batch: 1  d_loss: -0.245236  g_loss: 0.471782\n",
      "Elapsed: [12:32:19.329681]  batch: 50  d_loss: -0.322160  g_loss: 0.759997\n",
      "Elapsed: [12:32:45.623998]  batch: 100  d_loss: 0.037713  g_loss: 0.239218\n",
      "Elapsed: [12:33:08.885756]  batch: 150  d_loss: -0.067969  g_loss: 0.339214\n",
      "Elapsed: [12:33:32.462642]  batch: 200  d_loss: -0.313506  g_loss: 0.537522\n",
      "Elapsed: [12:33:55.402380]  batch: 250  d_loss: -0.030292  g_loss: 0.174946\n",
      "Elapsed: [12:34:18.327606]  batch: 300  d_loss: -0.250003  g_loss: 0.602943\n",
      "Elapsed: [12:34:41.358903]  batch: 350  d_loss: -0.215134  g_loss: 0.555736\n",
      "Elapsed: [12:35:04.266022]  batch: 400  d_loss: 0.093403  g_loss: -0.071624\n",
      "Elapsed: [12:35:27.189810]  batch: 450  d_loss: -0.158313  g_loss: 0.541517\n",
      "Elapsed: [12:35:50.132231]  batch: 500  d_loss: -0.032501  g_loss: 0.088230\n",
      "Time taken for epoch: 250.780 secs\n",
      "ticker =  26501\n",
      "\n",
      "Epoch: 54\n",
      "Elapsed: [12:36:09.207066]  batch: 1  d_loss: -0.318614  g_loss: 0.563773\n",
      "Elapsed: [12:36:31.203726]  batch: 50  d_loss: 0.051655  g_loss: 0.424009\n",
      "Elapsed: [12:36:54.040260]  batch: 100  d_loss: 0.011572  g_loss: 0.243413\n",
      "Elapsed: [12:37:16.927956]  batch: 150  d_loss: -0.038276  g_loss: 0.394754\n",
      "Elapsed: [12:37:39.788277]  batch: 200  d_loss: -0.268621  g_loss: 0.611811\n",
      "Elapsed: [12:38:02.656542]  batch: 250  d_loss: -0.033678  g_loss: 0.317579\n",
      "Elapsed: [12:38:25.571704]  batch: 300  d_loss: -0.045963  g_loss: 0.234337\n",
      "Elapsed: [12:38:48.496937]  batch: 350  d_loss: 0.180104  g_loss: 0.037607\n",
      "Elapsed: [12:39:11.492637]  batch: 400  d_loss: -0.295143  g_loss: 0.761303\n",
      "Elapsed: [12:39:34.443175]  batch: 450  d_loss: -0.141877  g_loss: 0.531519\n",
      "Elapsed: [12:39:57.345884]  batch: 500  d_loss: -0.165724  g_loss: 0.490070\n",
      "Time taken for epoch: 247.095 secs\n",
      "ticker =  27001\n",
      "\n",
      "Epoch: 55\n",
      "Elapsed: [12:40:16.077291]  batch: 1  d_loss: -0.125258  g_loss: 0.516503\n",
      "Elapsed: [12:40:38.120986]  batch: 50  d_loss: -0.069845  g_loss: 0.313459\n",
      "Elapsed: [12:41:00.940535]  batch: 100  d_loss: 0.017054  g_loss: 0.495525\n",
      "Elapsed: [12:41:23.815362]  batch: 150  d_loss: 0.033416  g_loss: 0.258796\n",
      "Elapsed: [12:41:46.675571]  batch: 200  d_loss: -0.106766  g_loss: 0.652807\n",
      "Elapsed: [12:42:09.840155]  batch: 250  d_loss: -0.218212  g_loss: 0.491055\n",
      "Elapsed: [12:42:32.722382]  batch: 300  d_loss: 0.044625  g_loss: 0.288395\n",
      "Elapsed: [12:42:55.590123]  batch: 350  d_loss: -0.107323  g_loss: 0.577689\n",
      "Elapsed: [12:43:18.502870]  batch: 400  d_loss: -0.324343  g_loss: 0.686449\n",
      "Elapsed: [12:43:41.495952]  batch: 450  d_loss: -0.239815  g_loss: 0.542846\n",
      "Elapsed: [12:44:04.405122]  batch: 500  d_loss: 0.092000  g_loss: 0.106330\n",
      "Time taken for epoch: 246.854 secs\n",
      "ticker =  27501\n",
      "\n",
      "Epoch: 56\n",
      "Elapsed: [12:44:22.748598]  batch: 1  d_loss: -0.208657  g_loss: 0.625535\n",
      "Elapsed: [12:44:44.829414]  batch: 50  d_loss: -0.054701  g_loss: 0.318262\n",
      "Elapsed: [12:45:07.630407]  batch: 100  d_loss: -0.098493  g_loss: 0.481883\n",
      "Elapsed: [12:45:30.488117]  batch: 150  d_loss: 0.127264  g_loss: 0.186281\n",
      "Elapsed: [12:45:53.339594]  batch: 200  d_loss: -0.009950  g_loss: 0.367993\n",
      "Elapsed: [12:46:16.188223]  batch: 250  d_loss: 0.082567  g_loss: 0.333443\n",
      "Elapsed: [12:46:39.104379]  batch: 300  d_loss: -0.192299  g_loss: 0.581098\n",
      "Elapsed: [12:47:02.000447]  batch: 350  d_loss: 0.062116  g_loss: -0.042017\n",
      "Elapsed: [12:47:24.926409]  batch: 400  d_loss: -0.210067  g_loss: 0.639766\n",
      "Elapsed: [12:47:47.805228]  batch: 450  d_loss: -0.244707  g_loss: 0.590497\n",
      "Elapsed: [12:48:10.742373]  batch: 500  d_loss: -0.137153  g_loss: 0.592470\n",
      "Time taken for epoch: 246.121 secs\n",
      "ticker =  28001\n",
      "\n",
      "Epoch: 57\n",
      "Elapsed: [12:48:29.432617]  batch: 1  d_loss: -0.068967  g_loss: 0.563726\n",
      "Elapsed: [12:48:51.483008]  batch: 50  d_loss: 0.092520  g_loss: 0.292235\n",
      "Elapsed: [12:49:14.400854]  batch: 100  d_loss: -0.047094  g_loss: 0.482708\n",
      "Elapsed: [12:49:37.579972]  batch: 150  d_loss: 0.109497  g_loss: -0.003444\n",
      "Elapsed: [12:50:00.745559]  batch: 200  d_loss: 0.057363  g_loss: 0.285177\n",
      "Elapsed: [12:50:23.772262]  batch: 250  d_loss: -0.191484  g_loss: 0.538267\n",
      "Elapsed: [12:50:46.758781]  batch: 300  d_loss: -0.106928  g_loss: 0.492959\n",
      "Elapsed: [12:51:09.708972]  batch: 350  d_loss: -0.214283  g_loss: 0.426926\n",
      "Elapsed: [12:51:32.607881]  batch: 400  d_loss: 0.006808  g_loss: 0.389287\n",
      "Elapsed: [12:51:55.501773]  batch: 450  d_loss: -0.053129  g_loss: 0.311796\n",
      "Elapsed: [12:52:18.402689]  batch: 500  d_loss: -0.255071  g_loss: 0.514570\n",
      "Time taken for epoch: 247.515 secs\n",
      "ticker =  28501\n",
      "\n",
      "Epoch: 58\n",
      "Elapsed: [12:52:38.096495]  batch: 1  d_loss: 0.088615  g_loss: 0.036950\n",
      "Elapsed: [12:53:00.138498]  batch: 50  d_loss: 0.047931  g_loss: 0.281886\n",
      "Elapsed: [12:53:22.971957]  batch: 100  d_loss: 0.079658  g_loss: 0.345073\n",
      "Elapsed: [12:53:45.829948]  batch: 150  d_loss: 0.096970  g_loss: 0.112076\n",
      "Elapsed: [12:54:08.676054]  batch: 200  d_loss: 0.139371  g_loss: 0.169760\n",
      "Elapsed: [12:54:31.597797]  batch: 250  d_loss: -0.250032  g_loss: 0.640349\n",
      "Elapsed: [12:54:54.666049]  batch: 300  d_loss: -0.033507  g_loss: 0.451379\n",
      "Elapsed: [12:55:17.589014]  batch: 350  d_loss: 0.014880  g_loss: -0.103144\n",
      "Elapsed: [12:55:40.609360]  batch: 400  d_loss: -0.206584  g_loss: 0.473238\n",
      "Elapsed: [12:56:03.514385]  batch: 450  d_loss: 0.019018  g_loss: 0.332713\n",
      "Elapsed: [12:56:26.419341]  batch: 500  d_loss: 0.076299  g_loss: 0.250571\n",
      "Time taken for epoch: 247.851 secs\n",
      "ticker =  29001\n",
      "\n",
      "Epoch: 59\n",
      "Elapsed: [12:56:44.776889]  batch: 1  d_loss: -0.095604  g_loss: 0.341330\n",
      "Elapsed: [12:57:06.966563]  batch: 50  d_loss: -0.272899  g_loss: 0.544560\n",
      "Elapsed: [12:57:29.778659]  batch: 100  d_loss: -0.070921  g_loss: 0.428184\n",
      "Elapsed: [12:57:52.632935]  batch: 150  d_loss: 0.023821  g_loss: 0.302330\n",
      "Elapsed: [12:58:15.467007]  batch: 200  d_loss: -0.220087  g_loss: 0.773536\n",
      "Elapsed: [12:58:38.392506]  batch: 250  d_loss: 0.015030  g_loss: 0.362852\n",
      "Elapsed: [12:59:01.309219]  batch: 300  d_loss: -0.205833  g_loss: 0.430658\n",
      "Elapsed: [12:59:24.212907]  batch: 350  d_loss: -0.085487  g_loss: 0.317501\n",
      "Elapsed: [12:59:47.131636]  batch: 400  d_loss: 0.068524  g_loss: 0.305324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: [13:00:10.125803]  batch: 450  d_loss: 0.002850  g_loss: 0.173489\n",
      "Elapsed: [13:00:36.044287]  batch: 500  d_loss: -0.042049  g_loss: 0.532178\n",
      "Time taken for epoch: 249.416 secs\n",
      "ticker =  29501\n",
      "\n",
      "Epoch: 60\n",
      "Elapsed: [13:00:57.280572]  batch: 1  d_loss: -0.095231  g_loss: 0.449128\n",
      "Elapsed: [13:01:19.103510]  batch: 50  d_loss: -0.022039  g_loss: 0.357376\n",
      "Elapsed: [13:01:42.698292]  batch: 100  d_loss: 0.012073  g_loss: 0.217917\n",
      "Elapsed: [13:02:05.548436]  batch: 150  d_loss: -0.126832  g_loss: 0.493536\n",
      "Elapsed: [13:02:28.476798]  batch: 200  d_loss: -0.184964  g_loss: 0.476818\n",
      "Elapsed: [13:02:51.400688]  batch: 250  d_loss: -0.106136  g_loss: 0.304217\n",
      "Elapsed: [13:03:14.363326]  batch: 300  d_loss: 0.175449  g_loss: -0.039905\n",
      "Elapsed: [13:03:37.254582]  batch: 350  d_loss: -0.157249  g_loss: 0.436187\n",
      "Elapsed: [13:04:00.161669]  batch: 400  d_loss: -0.159799  g_loss: 0.402079\n",
      "Elapsed: [13:04:23.319007]  batch: 450  d_loss: -0.153022  g_loss: 0.505853\n",
      "Elapsed: [13:04:46.321642]  batch: 500  d_loss: -0.043665  g_loss: 0.456084\n",
      "Time taken for epoch: 250.129 secs\n",
      "ticker =  30001\n",
      "\n",
      "Epoch: 61\n",
      "Elapsed: [13:05:05.396147]  batch: 1  d_loss: -0.102518  g_loss: 0.429676\n",
      "Elapsed: [13:05:27.430854]  batch: 50  d_loss: -0.054538  g_loss: 0.523202\n",
      "Elapsed: [13:05:50.315948]  batch: 100  d_loss: -0.135075  g_loss: 0.510192\n",
      "Elapsed: [13:06:13.171348]  batch: 150  d_loss: 0.025601  g_loss: 0.436191\n",
      "Elapsed: [13:06:36.141245]  batch: 200  d_loss: 0.016157  g_loss: 0.286410\n",
      "Elapsed: [13:06:59.035378]  batch: 250  d_loss: -0.114592  g_loss: 0.261454\n",
      "Elapsed: [13:07:21.961003]  batch: 300  d_loss: -0.088073  g_loss: 0.210505\n",
      "Elapsed: [13:07:44.861454]  batch: 350  d_loss: -0.159277  g_loss: 0.616451\n",
      "Elapsed: [13:08:07.738492]  batch: 400  d_loss: 0.101251  g_loss: 0.185636\n",
      "Elapsed: [13:08:30.651268]  batch: 450  d_loss: -0.106557  g_loss: 0.587329\n",
      "Elapsed: [13:08:53.574520]  batch: 500  d_loss: -0.128739  g_loss: 0.584843\n",
      "Time taken for epoch: 247.042 secs\n",
      "ticker =  30501\n",
      "\n",
      "Epoch: 62\n",
      "Elapsed: [13:09:12.080297]  batch: 1  d_loss: 0.085877  g_loss: 0.312517\n",
      "Elapsed: [13:09:34.155474]  batch: 50  d_loss: -0.052966  g_loss: 0.439377\n",
      "Elapsed: [13:09:56.997516]  batch: 100  d_loss: -0.018934  g_loss: 0.370508\n",
      "Elapsed: [13:10:19.900702]  batch: 150  d_loss: 0.087567  g_loss: 0.575759\n",
      "Elapsed: [13:10:42.800950]  batch: 200  d_loss: -0.098259  g_loss: 0.473222\n",
      "Elapsed: [13:11:05.716000]  batch: 250  d_loss: 0.151268  g_loss: -0.056386\n",
      "Elapsed: [13:11:28.634140]  batch: 300  d_loss: -0.060697  g_loss: 0.450710\n",
      "Elapsed: [13:11:51.551293]  batch: 350  d_loss: 0.004779  g_loss: 0.171209\n",
      "Elapsed: [13:12:14.810616]  batch: 400  d_loss: -0.063110  g_loss: 0.385317\n",
      "Elapsed: [13:12:37.713329]  batch: 450  d_loss: -0.058360  g_loss: 0.296747\n",
      "Elapsed: [13:13:00.711857]  batch: 500  d_loss: 0.053774  g_loss: 0.403081\n",
      "Time taken for epoch: 246.929 secs\n",
      "ticker =  31001\n",
      "\n",
      "Epoch: 63\n",
      "Elapsed: [13:13:19.720749]  batch: 1  d_loss: -0.088999  g_loss: 0.430277\n",
      "Elapsed: [13:13:41.801685]  batch: 50  d_loss: -0.019321  g_loss: 0.310488\n",
      "Elapsed: [13:14:04.636699]  batch: 100  d_loss: 0.081241  g_loss: -0.157477\n",
      "Elapsed: [13:14:27.527199]  batch: 150  d_loss: 1.054805  g_loss: -0.028318\n",
      "Elapsed: [13:14:50.367205]  batch: 200  d_loss: 0.720923  g_loss: -0.376920\n",
      "Elapsed: [13:15:13.243463]  batch: 250  d_loss: -0.218897  g_loss: 0.412913\n",
      "Elapsed: [13:15:36.084060]  batch: 300  d_loss: 0.150462  g_loss: -0.056304\n",
      "Elapsed: [13:15:58.949401]  batch: 350  d_loss: -0.021269  g_loss: 0.591967\n",
      "Elapsed: [13:16:21.815389]  batch: 400  d_loss: -0.108661  g_loss: 0.501061\n",
      "Elapsed: [13:16:44.768743]  batch: 450  d_loss: 0.202558  g_loss: 0.078021\n",
      "Elapsed: [13:17:07.670430]  batch: 500  d_loss: 0.274468  g_loss: 0.159327\n",
      "Time taken for epoch: 246.773 secs\n",
      "ticker =  31501\n",
      "\n",
      "Epoch: 64\n",
      "Elapsed: [13:17:26.400245]  batch: 1  d_loss: 0.094106  g_loss: 0.129378\n",
      "Elapsed: [13:17:48.447139]  batch: 50  d_loss: -0.051375  g_loss: 0.112070\n",
      "Elapsed: [13:18:11.274077]  batch: 100  d_loss: -0.120887  g_loss: 0.259493\n",
      "Elapsed: [13:18:34.173442]  batch: 150  d_loss: -0.105844  g_loss: 0.470246\n",
      "Elapsed: [13:18:57.057253]  batch: 200  d_loss: 0.049846  g_loss: 0.447397\n",
      "Elapsed: [13:19:19.974917]  batch: 250  d_loss: -0.062006  g_loss: 0.325504\n",
      "Elapsed: [13:19:43.217413]  batch: 300  d_loss: 0.015892  g_loss: 0.404770\n",
      "Elapsed: [13:20:06.106127]  batch: 350  d_loss: 0.054179  g_loss: -0.230683\n",
      "Elapsed: [13:20:29.006181]  batch: 400  d_loss: 0.028311  g_loss: 0.456969\n",
      "Elapsed: [13:20:51.922122]  batch: 450  d_loss: 0.113812  g_loss: 0.159444\n",
      "Elapsed: [13:21:14.914313]  batch: 500  d_loss: -0.049137  g_loss: 0.277514\n",
      "Time taken for epoch: 247.063 secs\n",
      "ticker =  32001\n",
      "\n",
      "Epoch: 65\n",
      "Elapsed: [13:21:34.876184]  batch: 1  d_loss: -0.077967  g_loss: 0.603007\n",
      "Elapsed: [13:21:56.913315]  batch: 50  d_loss: 0.011746  g_loss: 0.448265\n",
      "Elapsed: [13:22:19.785577]  batch: 100  d_loss: -0.016920  g_loss: 0.420215\n",
      "Elapsed: [13:22:42.652733]  batch: 150  d_loss: 0.126006  g_loss: 0.230699\n",
      "Elapsed: [13:23:05.541779]  batch: 200  d_loss: -0.029333  g_loss: 0.252518\n",
      "Elapsed: [13:23:28.467185]  batch: 250  d_loss: -0.008941  g_loss: 0.375532\n",
      "Elapsed: [13:23:51.394883]  batch: 300  d_loss: 0.082427  g_loss: 0.343824\n",
      "Elapsed: [13:24:14.326247]  batch: 350  d_loss: -0.178568  g_loss: 0.322866\n",
      "Elapsed: [13:24:37.218857]  batch: 400  d_loss: 0.022411  g_loss: 0.350962\n",
      "Elapsed: [13:25:00.115863]  batch: 450  d_loss: -0.027958  g_loss: -0.225289\n",
      "Elapsed: [13:25:23.014393]  batch: 500  d_loss: -0.030201  g_loss: 0.508305\n",
      "Time taken for epoch: 247.909 secs\n",
      "ticker =  32501\n",
      "\n",
      "Epoch: 66\n",
      "Elapsed: [13:25:42.147621]  batch: 1  d_loss: 0.135371  g_loss: 0.194373\n",
      "Elapsed: [13:26:04.098742]  batch: 50  d_loss: 0.072819  g_loss: 0.301707\n",
      "Elapsed: [13:26:26.933045]  batch: 100  d_loss: 0.052566  g_loss: 0.581190\n",
      "Elapsed: [13:26:49.783251]  batch: 150  d_loss: -0.058942  g_loss: 0.484698\n",
      "Elapsed: [13:27:12.949485]  batch: 200  d_loss: 0.062306  g_loss: 0.279503\n",
      "Elapsed: [13:27:35.849771]  batch: 250  d_loss: 0.037756  g_loss: 0.435274\n",
      "Elapsed: [13:27:58.754224]  batch: 300  d_loss: -0.071671  g_loss: 0.292277\n",
      "Elapsed: [13:28:21.657527]  batch: 350  d_loss: 0.017650  g_loss: 0.178677\n",
      "Elapsed: [13:28:44.650925]  batch: 400  d_loss: -0.000147  g_loss: 0.115696\n",
      "Elapsed: [13:29:08.888871]  batch: 450  d_loss: 0.057808  g_loss: 0.374820\n",
      "Elapsed: [13:29:36.620318]  batch: 500  d_loss: -0.033185  g_loss: 0.327712\n",
      "Time taken for epoch: 254.262 secs\n",
      "ticker =  33001\n",
      "\n",
      "Epoch: 67\n",
      "Elapsed: [13:30:01.600818]  batch: 1  d_loss: -0.002966  g_loss: 0.392409\n",
      "Elapsed: [13:30:22.874151]  batch: 50  d_loss: 0.103540  g_loss: 0.486747\n",
      "Elapsed: [13:30:47.285641]  batch: 100  d_loss: 0.101069  g_loss: 0.220894\n",
      "Elapsed: [13:31:13.271837]  batch: 150  d_loss: 0.029811  g_loss: 0.280692\n",
      "Elapsed: [13:31:38.275536]  batch: 200  d_loss: -0.144454  g_loss: 0.433374\n",
      "Elapsed: [13:32:01.218899]  batch: 250  d_loss: 0.119260  g_loss: 0.180656\n",
      "Elapsed: [13:32:24.118037]  batch: 300  d_loss: 0.097675  g_loss: 0.272200\n",
      "Elapsed: [13:32:46.998987]  batch: 350  d_loss: 0.019216  g_loss: 0.162462\n",
      "Elapsed: [13:33:09.985996]  batch: 400  d_loss: 0.060207  g_loss: -0.101415\n",
      "Elapsed: [13:33:32.930680]  batch: 450  d_loss: 0.055255  g_loss: 0.171386\n",
      "Elapsed: [13:33:55.826899]  batch: 500  d_loss: 0.027425  g_loss: 0.022646\n",
      "Time taken for epoch: 257.919 secs\n",
      "ticker =  33501\n",
      "\n",
      "Epoch: 68\n",
      "Elapsed: [13:34:14.729484]  batch: 1  d_loss: -0.023074  g_loss: 0.309225\n",
      "Elapsed: [13:34:36.742679]  batch: 50  d_loss: -0.079593  g_loss: 0.452771\n",
      "Elapsed: [13:34:59.648927]  batch: 100  d_loss: 0.067394  g_loss: 0.389345\n",
      "Elapsed: [13:35:22.649532]  batch: 150  d_loss: -0.025403  g_loss: 0.311479\n",
      "Elapsed: [13:35:45.572675]  batch: 200  d_loss: 0.267306  g_loss: 0.497488\n",
      "Elapsed: [13:36:08.549094]  batch: 250  d_loss: 0.152097  g_loss: -0.111358\n",
      "Elapsed: [13:36:31.500515]  batch: 300  d_loss: -0.031408  g_loss: 0.270750\n",
      "Elapsed: [13:36:54.485810]  batch: 350  d_loss: 0.119166  g_loss: 0.079950\n",
      "Elapsed: [13:37:17.454493]  batch: 400  d_loss: 0.124537  g_loss: -0.281901\n",
      "Elapsed: [13:37:40.516289]  batch: 450  d_loss: 0.043814  g_loss: -0.017658\n",
      "Elapsed: [13:38:03.523818]  batch: 500  d_loss: 0.144401  g_loss: 0.161280\n",
      "Time taken for epoch: 247.543 secs\n",
      "ticker =  34001\n",
      "\n",
      "Epoch: 69\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: [13:38:22.065970]  batch: 1  d_loss: 0.035374  g_loss: 0.248208\n",
      "Elapsed: [13:38:44.073690]  batch: 50  d_loss: 0.092111  g_loss: 0.167330\n",
      "Elapsed: [13:39:07.028202]  batch: 100  d_loss: 0.194578  g_loss: 0.155669\n",
      "Elapsed: [13:39:29.948271]  batch: 150  d_loss: 0.122778  g_loss: -0.314841\n",
      "Elapsed: [13:39:52.855824]  batch: 200  d_loss: 0.091287  g_loss: 0.375228\n",
      "Elapsed: [13:40:15.759160]  batch: 250  d_loss: 0.092587  g_loss: 0.268172\n",
      "Elapsed: [13:40:38.680720]  batch: 300  d_loss: 0.403019  g_loss: 0.296065\n",
      "Elapsed: [13:41:01.584381]  batch: 350  d_loss: 0.165594  g_loss: 0.407584\n",
      "Elapsed: [13:41:26.200339]  batch: 400  d_loss: 0.444132  g_loss: 0.598436\n",
      "Elapsed: [13:41:49.217527]  batch: 450  d_loss: 0.183075  g_loss: 0.403207\n",
      "Elapsed: [13:42:12.135605]  batch: 500  d_loss: 0.194313  g_loss: 0.608752\n",
      "Time taken for epoch: 248.439 secs\n",
      "ticker =  34501\n",
      "\n",
      "Epoch: 70\n",
      "Elapsed: [13:42:31.126741]  batch: 1  d_loss: 0.089810  g_loss: 0.530183\n",
      "Elapsed: [13:42:53.210432]  batch: 50  d_loss: 0.133598  g_loss: 1.036564\n",
      "Elapsed: [13:43:16.048423]  batch: 100  d_loss: -0.012060  g_loss: 0.836178\n",
      "Elapsed: [13:43:39.001199]  batch: 150  d_loss: 0.003144  g_loss: 0.812907\n",
      "Elapsed: [13:44:01.940080]  batch: 200  d_loss: 0.422306  g_loss: 0.228026\n",
      "Elapsed: [13:44:25.029273]  batch: 250  d_loss: 0.010335  g_loss: 0.344522\n",
      "Elapsed: [13:44:47.966645]  batch: 300  d_loss: 0.050698  g_loss: 0.033169\n",
      "Elapsed: [13:45:11.013122]  batch: 350  d_loss: 0.012491  g_loss: 0.104611\n",
      "Elapsed: [13:45:35.329382]  batch: 400  d_loss: -0.051624  g_loss: -0.114417\n",
      "Elapsed: [13:46:00.120100]  batch: 450  d_loss: 0.103579  g_loss: -0.339728\n",
      "Elapsed: [13:46:25.971754]  batch: 500  d_loss: 0.187386  g_loss: -0.094408\n",
      "Time taken for epoch: 254.123 secs\n",
      "ticker =  35001\n",
      "\n",
      "Epoch: 71\n",
      "Elapsed: [13:46:54.888977]  batch: 1  d_loss: -0.073131  g_loss: 0.203903\n",
      "Elapsed: [13:47:18.486981]  batch: 50  d_loss: 0.248374  g_loss: 0.360903\n",
      "Elapsed: [13:47:44.151750]  batch: 100  d_loss: 0.028153  g_loss: 0.120925\n",
      "Elapsed: [13:48:09.755852]  batch: 150  d_loss: 0.071661  g_loss: 0.323125\n",
      "Elapsed: [13:48:35.243003]  batch: 200  d_loss: 0.106961  g_loss: -0.024765\n",
      "Elapsed: [13:49:00.941300]  batch: 250  d_loss: 0.161414  g_loss: -0.116586\n",
      "Elapsed: [13:49:26.899253]  batch: 300  d_loss: 0.009056  g_loss: -0.009856\n",
      "Elapsed: [13:49:51.731109]  batch: 350  d_loss: 0.274881  g_loss: 0.475806\n",
      "Elapsed: [13:50:14.621400]  batch: 400  d_loss: -0.024436  g_loss: 0.319312\n",
      "Elapsed: [13:50:37.587805]  batch: 450  d_loss: 0.247166  g_loss: 0.473611\n",
      "Elapsed: [13:51:00.503891]  batch: 500  d_loss: 0.034977  g_loss: -0.218794\n",
      "Time taken for epoch: 273.789 secs\n",
      "ticker =  35501\n",
      "\n",
      "Epoch: 72\n",
      "Elapsed: [13:51:19.465083]  batch: 1  d_loss: 0.166092  g_loss: -0.553521\n",
      "Elapsed: [13:51:41.403959]  batch: 50  d_loss: 0.164840  g_loss: -0.312123\n",
      "Elapsed: [13:52:04.255549]  batch: 100  d_loss: -0.010135  g_loss: 0.215487\n",
      "Elapsed: [13:52:27.121949]  batch: 150  d_loss: 0.036331  g_loss: -1.006768\n",
      "Elapsed: [13:52:50.001139]  batch: 200  d_loss: 0.836284  g_loss: -0.678789\n",
      "Elapsed: [13:53:12.914165]  batch: 250  d_loss: 0.098481  g_loss: 0.426625\n",
      "Elapsed: [13:53:35.868272]  batch: 300  d_loss: 0.091636  g_loss: 0.362185\n",
      "Elapsed: [13:53:58.888994]  batch: 350  d_loss: 0.111789  g_loss: -0.312316\n",
      "Elapsed: [13:54:21.878531]  batch: 400  d_loss: 0.215341  g_loss: -0.293124\n",
      "Elapsed: [13:54:44.826861]  batch: 450  d_loss: 0.135246  g_loss: 0.129182\n",
      "Elapsed: [13:55:07.740510]  batch: 500  d_loss: -0.074467  g_loss: 0.121357\n",
      "Time taken for epoch: 247.020 secs\n",
      "ticker =  36001\n",
      "\n",
      "Epoch: 73\n",
      "Elapsed: [13:55:28.979854]  batch: 1  d_loss: 0.122853  g_loss: -0.053326\n",
      "Elapsed: [13:55:50.865582]  batch: 50  d_loss: 0.091931  g_loss: -0.358751\n",
      "Elapsed: [13:56:13.669593]  batch: 100  d_loss: 0.021552  g_loss: -0.025908\n",
      "Elapsed: [13:56:36.493500]  batch: 150  d_loss: 0.074754  g_loss: 0.165696\n",
      "Elapsed: [13:56:59.345356]  batch: 200  d_loss: 0.125541  g_loss: -0.109785\n",
      "Elapsed: [13:57:22.235325]  batch: 250  d_loss: 0.170600  g_loss: -0.075586\n",
      "Elapsed: [13:57:45.095914]  batch: 300  d_loss: 0.288080  g_loss: -0.426598\n",
      "Elapsed: [13:58:07.972577]  batch: 350  d_loss: 0.096148  g_loss: 0.008876\n",
      "Elapsed: [13:58:30.956265]  batch: 400  d_loss: 0.159842  g_loss: 0.008530\n",
      "Elapsed: [13:58:53.865750]  batch: 450  d_loss: 0.118668  g_loss: 0.655301\n",
      "Elapsed: [13:59:16.808865]  batch: 500  d_loss: 0.062698  g_loss: 0.189708\n",
      "Time taken for epoch: 248.902 secs\n",
      "ticker =  36501\n",
      "\n",
      "Epoch: 74\n",
      "Elapsed: [13:59:41.039682]  batch: 1  d_loss: -0.095700  g_loss: 0.288142\n",
      "Elapsed: [14:00:03.736749]  batch: 50  d_loss: -0.084400  g_loss: 0.708247\n",
      "Elapsed: [14:00:27.064942]  batch: 100  d_loss: 0.042132  g_loss: -0.123321\n",
      "Elapsed: [14:00:50.079191]  batch: 150  d_loss: 0.001143  g_loss: 0.152015\n",
      "Elapsed: [14:01:13.080880]  batch: 200  d_loss: -0.007712  g_loss: 0.336419\n",
      "Elapsed: [14:01:36.051283]  batch: 250  d_loss: 0.161933  g_loss: 0.578738\n",
      "Elapsed: [14:01:58.976634]  batch: 300  d_loss: 0.025674  g_loss: 0.160142\n",
      "Elapsed: [14:02:21.934474]  batch: 350  d_loss: 0.211183  g_loss: -0.048467\n",
      "Elapsed: [14:02:44.815411]  batch: 400  d_loss: -0.129393  g_loss: 0.342692\n",
      "Elapsed: [14:03:07.753640]  batch: 450  d_loss: 0.046466  g_loss: 1.184853\n",
      "Elapsed: [14:03:30.644847]  batch: 500  d_loss: 0.073027  g_loss: 0.022580\n",
      "Time taken for epoch: 253.654 secs\n",
      "ticker =  37001\n",
      "\n",
      "Epoch: 75\n",
      "Elapsed: [14:03:49.339174]  batch: 1  d_loss: 0.049974  g_loss: 0.000649\n",
      "Elapsed: [14:04:11.447817]  batch: 50  d_loss: 0.099184  g_loss: -0.088371\n",
      "Elapsed: [14:04:34.563763]  batch: 100  d_loss: 0.102108  g_loss: 0.341209\n",
      "Elapsed: [14:04:57.466080]  batch: 150  d_loss: 0.076660  g_loss: 0.061081\n",
      "Elapsed: [14:05:20.454808]  batch: 200  d_loss: 0.132720  g_loss: -0.104652\n",
      "Elapsed: [14:05:43.299353]  batch: 250  d_loss: 0.173237  g_loss: 0.266134\n",
      "Elapsed: [14:06:06.180319]  batch: 300  d_loss: 0.093595  g_loss: 0.115128\n",
      "Elapsed: [14:06:29.136536]  batch: 350  d_loss: 0.018403  g_loss: -0.143022\n",
      "Elapsed: [14:06:52.121265]  batch: 400  d_loss: -0.023696  g_loss: 0.215110\n",
      "Elapsed: [14:07:15.563550]  batch: 450  d_loss: 0.108271  g_loss: 0.033270\n",
      "Elapsed: [14:07:39.282899]  batch: 500  d_loss: -0.080918  g_loss: 0.411309\n",
      "Time taken for epoch: 248.734 secs\n",
      "ticker =  37501\n",
      "\n",
      "Epoch: 76\n",
      "Elapsed: [14:08:00.828701]  batch: 1  d_loss: 0.110464  g_loss: -0.189721\n",
      "Elapsed: [14:08:25.211793]  batch: 50  d_loss: 0.067711  g_loss: 0.493703\n",
      "Elapsed: [14:08:49.426682]  batch: 100  d_loss: 0.094192  g_loss: 0.206072\n",
      "Elapsed: [14:09:12.356478]  batch: 150  d_loss: 0.004419  g_loss: 0.004333\n",
      "Elapsed: [14:09:35.354461]  batch: 200  d_loss: 0.008803  g_loss: 0.026001\n",
      "Elapsed: [14:09:58.264719]  batch: 250  d_loss: -0.033678  g_loss: 0.126994\n",
      "Elapsed: [14:10:25.464981]  batch: 300  d_loss: 0.135078  g_loss: 0.219134\n",
      "Elapsed: [14:10:48.439106]  batch: 350  d_loss: -0.048019  g_loss: 0.140720\n",
      "Elapsed: [14:11:12.432156]  batch: 400  d_loss: 0.116123  g_loss: 0.122269\n",
      "Elapsed: [14:11:35.410606]  batch: 450  d_loss: 0.064459  g_loss: 0.326967\n",
      "Elapsed: [14:11:58.413202]  batch: 500  d_loss: -0.075478  g_loss: 0.146524\n",
      "Time taken for epoch: 258.647 secs\n",
      "ticker =  38001\n",
      "\n",
      "Epoch: 77\n",
      "Elapsed: [14:12:18.174166]  batch: 1  d_loss: 0.073343  g_loss: -0.025552\n",
      "Elapsed: [14:12:40.116978]  batch: 50  d_loss: 0.082469  g_loss: 0.269684\n",
      "Elapsed: [14:13:03.269088]  batch: 100  d_loss: 0.014860  g_loss: 0.217514\n",
      "Elapsed: [14:13:26.186863]  batch: 150  d_loss: 0.007073  g_loss: -0.147572\n",
      "Elapsed: [14:13:49.127700]  batch: 200  d_loss: 0.071288  g_loss: 0.437737\n",
      "Elapsed: [14:14:12.125191]  batch: 250  d_loss: -0.014333  g_loss: 0.138211\n",
      "Elapsed: [14:14:35.089175]  batch: 300  d_loss: -0.030550  g_loss: 0.484537\n",
      "Elapsed: [14:14:58.171747]  batch: 350  d_loss: -0.007903  g_loss: -0.000914\n",
      "Elapsed: [14:15:21.125430]  batch: 400  d_loss: 0.269015  g_loss: -0.253731\n",
      "Elapsed: [14:15:44.034132]  batch: 450  d_loss: -0.025811  g_loss: 0.119585\n",
      "Elapsed: [14:16:06.937372]  batch: 500  d_loss: 0.104549  g_loss: -0.261711\n",
      "Time taken for epoch: 248.273 secs\n",
      "ticker =  38501\n",
      "\n",
      "Epoch: 78\n",
      "Elapsed: [14:16:27.087076]  batch: 1  d_loss: 0.012200  g_loss: -0.162319\n",
      "Elapsed: [14:16:49.038471]  batch: 50  d_loss: -0.006863  g_loss: 0.415883\n",
      "Elapsed: [14:17:11.892187]  batch: 100  d_loss: 0.060041  g_loss: -0.167048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: [14:17:34.784496]  batch: 150  d_loss: 0.105187  g_loss: 0.777494\n",
      "Elapsed: [14:17:57.684910]  batch: 200  d_loss: 0.090701  g_loss: -0.287584\n",
      "Elapsed: [14:18:20.609326]  batch: 250  d_loss: -0.202811  g_loss: 0.365162\n",
      "Elapsed: [14:18:43.484560]  batch: 300  d_loss: 0.028329  g_loss: 0.400856\n",
      "Elapsed: [14:19:06.395774]  batch: 350  d_loss: 0.194788  g_loss: -0.070354\n",
      "Elapsed: [14:19:29.419794]  batch: 400  d_loss: 0.027867  g_loss: 0.397916\n",
      "Elapsed: [14:19:52.357114]  batch: 450  d_loss: 0.159504  g_loss: 0.525904\n",
      "Elapsed: [14:20:15.807231]  batch: 500  d_loss: 0.210591  g_loss: 0.324328\n",
      "Time taken for epoch: 248.685 secs\n",
      "ticker =  39001\n",
      "\n",
      "Epoch: 79\n",
      "Elapsed: [14:20:34.564231]  batch: 1  d_loss: 0.014029  g_loss: 0.221353\n",
      "Elapsed: [14:20:56.651267]  batch: 50  d_loss: -0.043450  g_loss: 0.651846\n",
      "Elapsed: [14:21:19.542668]  batch: 100  d_loss: 0.142206  g_loss: 0.128937\n",
      "Elapsed: [14:21:42.432819]  batch: 150  d_loss: -0.096096  g_loss: 0.477984\n",
      "Elapsed: [14:22:05.347961]  batch: 200  d_loss: -0.013308  g_loss: 0.011636\n",
      "Elapsed: [14:22:28.282944]  batch: 250  d_loss: 0.090283  g_loss: 0.124961\n",
      "Elapsed: [14:22:51.207093]  batch: 300  d_loss: -0.020109  g_loss: -0.406303\n",
      "Elapsed: [14:23:14.316386]  batch: 350  d_loss: 0.062129  g_loss: 0.214621\n",
      "Elapsed: [14:23:37.255176]  batch: 400  d_loss: 0.048527  g_loss: 0.296723\n",
      "Elapsed: [14:24:00.272033]  batch: 450  d_loss: -0.011459  g_loss: -0.060855\n",
      "Elapsed: [14:24:23.251222]  batch: 500  d_loss: 0.099798  g_loss: 0.635556\n",
      "Time taken for epoch: 247.281 secs\n",
      "ticker =  39501\n",
      "\n",
      "Epoch: 80\n",
      "Elapsed: [14:24:42.455920]  batch: 1  d_loss: 0.056276  g_loss: 0.484423\n",
      "Elapsed: [14:25:04.542570]  batch: 50  d_loss: 0.040418  g_loss: -0.297171\n",
      "Elapsed: [14:25:27.444241]  batch: 100  d_loss: 0.046644  g_loss: -0.523990\n",
      "Elapsed: [14:25:50.322143]  batch: 150  d_loss: 0.139353  g_loss: 0.316843\n",
      "Elapsed: [14:26:13.322576]  batch: 200  d_loss: 0.185340  g_loss: -0.355900\n",
      "Elapsed: [14:26:36.870199]  batch: 250  d_loss: 0.262184  g_loss: -0.115688\n",
      "Elapsed: [14:27:00.948534]  batch: 300  d_loss: 0.043086  g_loss: 0.338000\n",
      "Elapsed: [14:27:32.055847]  batch: 350  d_loss: 0.060223  g_loss: 0.259368\n",
      "Elapsed: [14:28:09.595890]  batch: 400  d_loss: -0.230664  g_loss: 0.422243\n",
      "Elapsed: [14:28:41.747081]  batch: 450  d_loss: 8.495645  g_loss: -0.121255\n",
      "Elapsed: [14:29:14.684122]  batch: 500  d_loss: 0.195827  g_loss: 0.303761\n",
      "Time taken for epoch: 292.337 secs\n",
      "ticker =  40001\n",
      "\n",
      "Epoch: 81\n",
      "Elapsed: [14:29:59.011455]  batch: 1  d_loss: 0.047408  g_loss: 0.419734\n",
      "Elapsed: [14:30:21.238802]  batch: 50  d_loss: 0.099034  g_loss: 0.548351\n",
      "Elapsed: [14:30:46.702111]  batch: 100  d_loss: 0.090207  g_loss: 0.168092\n",
      "Elapsed: [14:31:25.349291]  batch: 150  d_loss: 0.239674  g_loss: 0.148210\n",
      "Elapsed: [14:40:30.584476]  batch: 200  d_loss: 0.053377  g_loss: 0.484040\n",
      "Elapsed: [14:41:02.244311]  batch: 250  d_loss: 0.094036  g_loss: 0.778498\n",
      "Elapsed: [14:41:36.205710]  batch: 300  d_loss: 0.120765  g_loss: 0.471133\n",
      "Elapsed: [14:42:13.210803]  batch: 350  d_loss: 0.094557  g_loss: 0.749822\n",
      "Elapsed: [14:58:00.598510]  batch: 400  d_loss: 0.032048  g_loss: -0.069541\n",
      "Elapsed: [15:10:12.442841]  batch: 450  d_loss: 0.029605  g_loss: 0.363203\n",
      "Elapsed: [15:25:40.579807]  batch: 500  d_loss: 0.203476  g_loss: 0.199833\n",
      "Time taken for epoch: 3389.291 secs\n",
      "ticker =  40501\n",
      "\n",
      "Epoch: 82\n",
      "Elapsed: [15:26:58.346288]  batch: 1  d_loss: 0.066702  g_loss: 0.301002\n",
      "Elapsed: [15:39:30.183778]  batch: 50  d_loss: 0.132320  g_loss: 0.507512\n",
      "Elapsed: [15:55:55.072142]  batch: 100  d_loss: -0.050820  g_loss: -0.307307\n",
      "Elapsed: [16:09:54.150106]  batch: 150  d_loss: 0.094905  g_loss: -0.309068\n",
      "Elapsed: [16:21:55.750624]  batch: 200  d_loss: 0.130044  g_loss: 0.482716\n",
      "Elapsed: [16:22:22.802394]  batch: 250  d_loss: 0.019210  g_loss: 0.777255\n",
      "Elapsed: [16:22:46.367268]  batch: 300  d_loss: 0.107743  g_loss: -0.246668\n",
      "Elapsed: [16:23:11.662864]  batch: 350  d_loss: 0.092028  g_loss: 0.829562\n",
      "Elapsed: [16:23:40.792795]  batch: 400  d_loss: 0.083791  g_loss: -0.215093\n",
      "Elapsed: [16:24:07.619874]  batch: 450  d_loss: 0.072518  g_loss: -0.179740\n",
      "Elapsed: [16:24:32.863354]  batch: 500  d_loss: 0.027364  g_loss: 0.208689\n",
      "Time taken for epoch: 3528.577 secs\n",
      "ticker =  41001\n",
      "\n",
      "Epoch: 83\n",
      "Elapsed: [16:24:56.420145]  batch: 1  d_loss: 0.035880  g_loss: 0.052680\n",
      "Elapsed: [16:25:19.869967]  batch: 50  d_loss: 0.302899  g_loss: 0.072319\n",
      "Elapsed: [16:25:45.337882]  batch: 100  d_loss: 0.074468  g_loss: 0.310982\n",
      "Elapsed: [16:26:09.772528]  batch: 150  d_loss: 0.215165  g_loss: -0.232418\n",
      "Elapsed: [16:26:43.938120]  batch: 200  d_loss: 0.033933  g_loss: -0.281778\n",
      "Elapsed: [16:27:10.129579]  batch: 250  d_loss: 0.030430  g_loss: 0.448402\n",
      "Elapsed: [16:27:34.012841]  batch: 300  d_loss: 0.014031  g_loss: 0.236640\n",
      "Elapsed: [16:27:58.290665]  batch: 350  d_loss: 0.025334  g_loss: -0.451585\n",
      "Elapsed: [16:28:20.949933]  batch: 400  d_loss: 0.173959  g_loss: -0.015091\n",
      "Elapsed: [16:28:43.613999]  batch: 450  d_loss: 0.017362  g_loss: -0.122749\n",
      "Elapsed: [16:29:06.854416]  batch: 500  d_loss: 0.054747  g_loss: 0.470970\n",
      "Time taken for epoch: 272.030 secs\n",
      "ticker =  41501\n",
      "\n",
      "Epoch: 84\n",
      "Elapsed: [16:29:28.070292]  batch: 1  d_loss: -0.000908  g_loss: 0.620628\n",
      "Elapsed: [16:29:50.178246]  batch: 50  d_loss: -0.059655  g_loss: 0.413434\n",
      "Elapsed: [16:30:12.832882]  batch: 100  d_loss: -0.048204  g_loss: 0.185789\n",
      "Elapsed: [16:30:38.004076]  batch: 150  d_loss: 0.000551  g_loss: -0.113926\n",
      "Elapsed: [16:31:01.270821]  batch: 200  d_loss: 0.189046  g_loss: -0.361756\n",
      "Elapsed: [16:31:23.985426]  batch: 250  d_loss: 0.158866  g_loss: 0.280504\n",
      "Elapsed: [16:31:47.228881]  batch: 300  d_loss: -0.010895  g_loss: 0.293706\n",
      "Elapsed: [16:32:10.852054]  batch: 350  d_loss: 0.262056  g_loss: -0.114559\n",
      "Elapsed: [16:32:34.078317]  batch: 400  d_loss: 0.047756  g_loss: -0.132196\n",
      "Elapsed: [16:32:57.758180]  batch: 450  d_loss: 0.102655  g_loss: 0.523970\n",
      "Elapsed: [16:33:20.425698]  batch: 500  d_loss: 0.077369  g_loss: 0.240734\n",
      "Time taken for epoch: 253.056 secs\n",
      "ticker =  42001\n",
      "\n",
      "Epoch: 85\n",
      "Elapsed: [16:33:36.939346]  batch: 1  d_loss: -0.052299  g_loss: 0.452856\n",
      "Elapsed: [16:33:59.116877]  batch: 50  d_loss: 0.137799  g_loss: 0.070214\n",
      "Elapsed: [16:34:21.758192]  batch: 100  d_loss: -0.022952  g_loss: -0.150622\n",
      "Elapsed: [16:34:44.438612]  batch: 150  d_loss: 0.044316  g_loss: -0.008143\n",
      "Elapsed: [16:35:07.071536]  batch: 200  d_loss: 0.024972  g_loss: 0.028467\n",
      "Elapsed: [16:35:29.748003]  batch: 250  d_loss: -0.061939  g_loss: 0.165737\n",
      "Elapsed: [16:35:52.434209]  batch: 300  d_loss: -0.074502  g_loss: 0.049493\n",
      "Elapsed: [16:36:16.669631]  batch: 350  d_loss: -0.000798  g_loss: -0.054700\n",
      "Elapsed: [16:36:40.019186]  batch: 400  d_loss: 0.011418  g_loss: 0.291243\n",
      "Elapsed: [16:37:03.284835]  batch: 450  d_loss: 0.141552  g_loss: 0.151744\n",
      "Elapsed: [16:37:26.106276]  batch: 500  d_loss: -0.094208  g_loss: 0.562379\n",
      "Time taken for epoch: 245.653 secs\n",
      "ticker =  42501\n",
      "\n",
      "Epoch: 86\n",
      "Elapsed: [16:37:43.753758]  batch: 1  d_loss: 0.116675  g_loss: 0.538176\n",
      "Elapsed: [16:38:06.039425]  batch: 50  d_loss: -0.134108  g_loss: 0.171985\n",
      "Elapsed: [16:38:28.824707]  batch: 100  d_loss: -0.029426  g_loss: -0.010882\n",
      "Elapsed: [16:38:51.694018]  batch: 150  d_loss: -0.067883  g_loss: 0.341916\n",
      "Elapsed: [16:39:14.567439]  batch: 200  d_loss: 0.068224  g_loss: 0.157675\n",
      "Elapsed: [16:39:37.471692]  batch: 250  d_loss: 0.189937  g_loss: 0.016408\n",
      "Elapsed: [16:40:00.388254]  batch: 300  d_loss: 0.222062  g_loss: 0.161878\n",
      "Elapsed: [16:40:23.292489]  batch: 350  d_loss: 0.096945  g_loss: -0.336907\n",
      "Elapsed: [16:40:46.261443]  batch: 400  d_loss: 0.061788  g_loss: 0.428739\n",
      "Elapsed: [16:41:09.314680]  batch: 450  d_loss: 0.031271  g_loss: 0.623962\n",
      "Elapsed: [16:41:32.120403]  batch: 500  d_loss: 0.364054  g_loss: 0.113268\n",
      "Time taken for epoch: 245.753 secs\n",
      "ticker =  43001\n",
      "\n",
      "Epoch: 87\n",
      "Elapsed: [16:41:49.431090]  batch: 1  d_loss: 0.121367  g_loss: 0.555738\n",
      "Elapsed: [16:42:11.712731]  batch: 50  d_loss: 0.011263  g_loss: 0.727541\n",
      "Elapsed: [16:42:34.534900]  batch: 100  d_loss: 0.064983  g_loss: 0.535437\n",
      "Elapsed: [16:42:57.366385]  batch: 150  d_loss: 0.023171  g_loss: -0.178232\n",
      "Elapsed: [16:43:20.178115]  batch: 200  d_loss: -0.029280  g_loss: 0.411807\n",
      "Elapsed: [16:43:43.182920]  batch: 250  d_loss: 0.026961  g_loss: -0.213215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: [16:44:06.067172]  batch: 300  d_loss: 0.147751  g_loss: -0.386112\n",
      "Elapsed: [16:44:28.979168]  batch: 350  d_loss: 0.058058  g_loss: 0.560739\n",
      "Elapsed: [16:44:51.871457]  batch: 400  d_loss: 0.062055  g_loss: 0.419923\n",
      "Elapsed: [16:45:14.837240]  batch: 450  d_loss: 0.094120  g_loss: 0.024202\n",
      "Elapsed: [16:45:37.707770]  batch: 500  d_loss: -0.001384  g_loss: 0.061236\n",
      "Time taken for epoch: 245.467 secs\n",
      "ticker =  43501\n",
      "\n",
      "Epoch: 88\n",
      "Elapsed: [16:45:54.884732]  batch: 1  d_loss: -0.060096  g_loss: -0.208675\n",
      "Elapsed: [16:46:17.095059]  batch: 50  d_loss: -0.017174  g_loss: -0.085136\n",
      "Elapsed: [16:46:39.911474]  batch: 100  d_loss: 0.046929  g_loss: 0.246092\n",
      "Elapsed: [16:47:02.712173]  batch: 150  d_loss: 0.138618  g_loss: 0.109116\n",
      "Elapsed: [16:47:25.499681]  batch: 200  d_loss: -0.108198  g_loss: 0.704670\n",
      "Elapsed: [16:47:48.309875]  batch: 250  d_loss: 0.055056  g_loss: 0.200693\n",
      "Elapsed: [16:48:11.195334]  batch: 300  d_loss: 0.057961  g_loss: 0.305005\n",
      "Elapsed: [16:48:34.085599]  batch: 350  d_loss: 0.071062  g_loss: -0.492228\n",
      "Elapsed: [16:48:56.978512]  batch: 400  d_loss: -0.039791  g_loss: 0.119873\n",
      "Elapsed: [16:49:20.120695]  batch: 450  d_loss: 0.125971  g_loss: -0.018168\n",
      "Elapsed: [16:49:43.084290]  batch: 500  d_loss: 0.154106  g_loss: 0.267679\n",
      "Time taken for epoch: 245.230 secs\n",
      "ticker =  44001\n",
      "\n",
      "Epoch: 89\n",
      "Elapsed: [16:50:05.703247]  batch: 1  d_loss: 0.041579  g_loss: 0.062175\n",
      "Elapsed: [16:50:28.854951]  batch: 50  d_loss: 0.022704  g_loss: -0.101568\n",
      "Elapsed: [16:50:51.864107]  batch: 100  d_loss: -0.006702  g_loss: 0.478760\n",
      "Elapsed: [16:51:14.755688]  batch: 150  d_loss: 0.082974  g_loss: 0.204440\n",
      "Elapsed: [16:51:37.594408]  batch: 200  d_loss: -0.032708  g_loss: -0.229960\n",
      "Elapsed: [16:52:00.492124]  batch: 250  d_loss: 0.089936  g_loss: 0.390193\n",
      "Elapsed: [16:52:23.452389]  batch: 300  d_loss: 0.075042  g_loss: 0.223786\n",
      "Elapsed: [16:52:46.462315]  batch: 350  d_loss: -0.008147  g_loss: 0.495155\n",
      "Elapsed: [16:53:09.369424]  batch: 400  d_loss: 0.033415  g_loss: 0.152785\n",
      "Elapsed: [16:53:32.299476]  batch: 450  d_loss: 0.121219  g_loss: -0.141275\n",
      "Elapsed: [16:53:55.309941]  batch: 500  d_loss: 0.148408  g_loss: 0.020859\n",
      "Time taken for epoch: 252.164 secs\n",
      "ticker =  44501\n",
      "\n",
      "Epoch: 90\n",
      "Elapsed: [16:54:13.429160]  batch: 1  d_loss: 0.077549  g_loss: 0.926844\n",
      "Elapsed: [16:54:35.739460]  batch: 50  d_loss: 0.013314  g_loss: 0.160534\n",
      "Elapsed: [16:54:58.563679]  batch: 100  d_loss: -0.094092  g_loss: 0.393334\n",
      "Elapsed: [16:55:21.410740]  batch: 150  d_loss: 0.078430  g_loss: 0.186906\n",
      "Elapsed: [16:55:44.274009]  batch: 200  d_loss: 0.094181  g_loss: 0.126868\n",
      "Elapsed: [16:56:07.209136]  batch: 250  d_loss: -0.058524  g_loss: -0.125810\n",
      "Elapsed: [16:56:30.173075]  batch: 300  d_loss: 0.020606  g_loss: 0.481529\n",
      "Elapsed: [16:56:53.096247]  batch: 350  d_loss: 0.083886  g_loss: 0.919338\n",
      "Elapsed: [16:57:16.082921]  batch: 400  d_loss: -0.039940  g_loss: 0.203195\n",
      "Elapsed: [16:57:39.567011]  batch: 450  d_loss: -0.105842  g_loss: 0.688845\n",
      "Elapsed: [16:58:05.441396]  batch: 500  d_loss: 0.085338  g_loss: 0.004169\n",
      "Time taken for epoch: 250.226 secs\n",
      "ticker =  45001\n",
      "\n",
      "Epoch: 91\n",
      "Elapsed: [16:58:24.299874]  batch: 1  d_loss: 0.148322  g_loss: -0.248734\n",
      "Elapsed: [16:58:46.702587]  batch: 50  d_loss: 0.055010  g_loss: 0.494558\n",
      "Elapsed: [16:59:09.592875]  batch: 100  d_loss: -0.030499  g_loss: 0.540597\n",
      "Elapsed: [16:59:32.436921]  batch: 150  d_loss: 0.174209  g_loss: 0.329446\n",
      "Elapsed: [16:59:55.344888]  batch: 200  d_loss: -0.001627  g_loss: 0.509648\n",
      "Elapsed: [17:00:18.180191]  batch: 250  d_loss: 0.000131  g_loss: 0.748464\n",
      "Elapsed: [17:00:41.045177]  batch: 300  d_loss: -0.022491  g_loss: 0.639633\n",
      "Elapsed: [17:01:03.923709]  batch: 350  d_loss: 0.013617  g_loss: -0.169525\n",
      "Elapsed: [17:01:26.797159]  batch: 400  d_loss: 0.520281  g_loss: -0.075631\n",
      "Elapsed: [17:01:49.736532]  batch: 450  d_loss: 0.114700  g_loss: 0.177391\n",
      "Elapsed: [17:02:12.616265]  batch: 500  d_loss: 0.269411  g_loss: -0.504260\n",
      "Time taken for epoch: 246.609 secs\n",
      "ticker =  45501\n",
      "\n",
      "Epoch: 92\n",
      "Elapsed: [17:02:30.354259]  batch: 1  d_loss: 0.167530  g_loss: -0.122296\n",
      "Elapsed: [17:02:52.668839]  batch: 50  d_loss: 0.013952  g_loss: 0.447558\n",
      "Elapsed: [17:03:15.475202]  batch: 100  d_loss: 0.107337  g_loss: 0.131699\n",
      "Elapsed: [17:03:38.332447]  batch: 150  d_loss: 0.096730  g_loss: 0.168658\n",
      "Elapsed: [17:04:01.210712]  batch: 200  d_loss: 0.072467  g_loss: -0.386267\n",
      "Elapsed: [17:04:24.584179]  batch: 250  d_loss: 0.053964  g_loss: -0.094458\n",
      "Elapsed: [17:04:47.491676]  batch: 300  d_loss: 0.083095  g_loss: 0.121292\n",
      "Elapsed: [17:05:10.353505]  batch: 350  d_loss: 0.081666  g_loss: 0.527677\n",
      "Elapsed: [17:05:33.255145]  batch: 400  d_loss: 0.152603  g_loss: -0.191729\n",
      "Elapsed: [17:05:56.191148]  batch: 450  d_loss: 0.172634  g_loss: 0.054648\n",
      "Elapsed: [17:06:19.231817]  batch: 500  d_loss: 0.077025  g_loss: 0.397372\n",
      "Time taken for epoch: 246.483 secs\n",
      "ticker =  46001\n",
      "\n",
      "Epoch: 93\n",
      "Elapsed: [17:06:37.594368]  batch: 1  d_loss: -0.142861  g_loss: 0.588920\n",
      "Elapsed: [17:06:59.959716]  batch: 50  d_loss: 0.100603  g_loss: 0.196038\n",
      "Elapsed: [17:07:22.827219]  batch: 100  d_loss: 0.056059  g_loss: 0.350898\n",
      "Elapsed: [17:07:45.671870]  batch: 150  d_loss: 0.068670  g_loss: 0.256344\n",
      "Elapsed: [17:08:08.517795]  batch: 200  d_loss: 0.082610  g_loss: 0.291917\n",
      "Elapsed: [17:08:31.365245]  batch: 250  d_loss: -0.046825  g_loss: 0.439018\n",
      "Elapsed: [17:08:54.340449]  batch: 300  d_loss: 0.093870  g_loss: 0.077710\n",
      "Elapsed: [17:09:17.208705]  batch: 350  d_loss: 0.021221  g_loss: 0.323634\n",
      "Elapsed: [17:09:40.153872]  batch: 400  d_loss: -0.011801  g_loss: 0.144899\n",
      "Elapsed: [17:10:03.044529]  batch: 450  d_loss: 0.070283  g_loss: 0.154566\n",
      "Elapsed: [17:10:25.933162]  batch: 500  d_loss: 0.000815  g_loss: 0.216309\n",
      "Time taken for epoch: 246.571 secs\n",
      "ticker =  46501\n",
      "\n",
      "Epoch: 94\n",
      "Elapsed: [17:10:43.402682]  batch: 1  d_loss: 0.144651  g_loss: -0.013269\n",
      "Elapsed: [17:11:05.736118]  batch: 50  d_loss: -0.092225  g_loss: -0.364751\n",
      "Elapsed: [17:11:28.523501]  batch: 100  d_loss: 0.194665  g_loss: -0.101179\n",
      "Elapsed: [17:11:51.348616]  batch: 150  d_loss: 0.162553  g_loss: -0.095055\n",
      "Elapsed: [17:12:14.266031]  batch: 200  d_loss: 0.083086  g_loss: -0.040428\n",
      "Elapsed: [17:12:37.084935]  batch: 250  d_loss: -0.110651  g_loss: 0.470511\n",
      "Elapsed: [17:12:59.933505]  batch: 300  d_loss: -0.022370  g_loss: -0.361292\n",
      "Elapsed: [17:13:22.774875]  batch: 350  d_loss: 0.106590  g_loss: 0.190251\n",
      "Elapsed: [17:13:45.689346]  batch: 400  d_loss: 0.032392  g_loss: 0.291388\n",
      "Elapsed: [17:14:08.566673]  batch: 450  d_loss: 0.099945  g_loss: 0.225337\n",
      "Elapsed: [17:14:31.499614]  batch: 500  d_loss: -0.074240  g_loss: -0.183486\n",
      "Time taken for epoch: 245.296 secs\n",
      "ticker =  47001\n",
      "\n",
      "Epoch: 95\n",
      "Elapsed: [17:14:49.413448]  batch: 1  d_loss: 0.184553  g_loss: -0.651182\n",
      "Elapsed: [17:15:11.744229]  batch: 50  d_loss: -0.015688  g_loss: 0.430285\n",
      "Elapsed: [17:15:34.637169]  batch: 100  d_loss: -0.025753  g_loss: 0.872028\n",
      "Elapsed: [17:15:57.533065]  batch: 150  d_loss: 0.078625  g_loss: -0.246774\n",
      "Elapsed: [17:16:20.616848]  batch: 200  d_loss: 0.356599  g_loss: 0.314736\n",
      "Elapsed: [17:16:43.506493]  batch: 250  d_loss: -0.070615  g_loss: 0.513823\n",
      "Elapsed: [17:17:06.436073]  batch: 300  d_loss: 0.032191  g_loss: 0.093381\n",
      "Elapsed: [17:17:29.514748]  batch: 350  d_loss: 0.010508  g_loss: -0.176741\n",
      "Elapsed: [17:17:52.507362]  batch: 400  d_loss: -0.078033  g_loss: 0.572419\n",
      "Elapsed: [17:18:15.671373]  batch: 450  d_loss: -0.100331  g_loss: 0.176785\n",
      "Elapsed: [17:18:39.190675]  batch: 500  d_loss: -0.064460  g_loss: 0.235869\n",
      "Time taken for epoch: 247.511 secs\n",
      "ticker =  47501\n",
      "\n",
      "Epoch: 96\n",
      "Elapsed: [17:18:56.897753]  batch: 1  d_loss: 0.179598  g_loss: 0.132487\n",
      "Elapsed: [17:19:19.296354]  batch: 50  d_loss: -0.011284  g_loss: 0.269837\n",
      "Elapsed: [17:19:42.164468]  batch: 100  d_loss: 0.027965  g_loss: 0.201334\n",
      "Elapsed: [17:20:04.989952]  batch: 150  d_loss: -0.171113  g_loss: 0.888845\n",
      "Elapsed: [17:20:28.012691]  batch: 200  d_loss: 0.135523  g_loss: 0.075073\n",
      "Elapsed: [17:20:50.982874]  batch: 250  d_loss: -0.061695  g_loss: 0.413786\n",
      "Elapsed: [17:21:13.930605]  batch: 300  d_loss: -0.058024  g_loss: 0.612448\n",
      "Elapsed: [17:21:36.906538]  batch: 350  d_loss: 0.114183  g_loss: 0.221837\n",
      "Elapsed: [17:21:59.832014]  batch: 400  d_loss: 0.049321  g_loss: 0.345998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: [17:22:22.820888]  batch: 450  d_loss: -0.052869  g_loss: 0.080959\n",
      "Elapsed: [17:22:45.913258]  batch: 500  d_loss: -0.014224  g_loss: 0.578332\n",
      "Time taken for epoch: 246.577 secs\n",
      "ticker =  48001\n",
      "\n",
      "Epoch: 97\n",
      "Elapsed: [17:23:03.934938]  batch: 1  d_loss: -0.051359  g_loss: 0.448343\n",
      "Elapsed: [17:23:26.571892]  batch: 50  d_loss: -0.013194  g_loss: 0.065709\n",
      "Elapsed: [17:23:50.343783]  batch: 100  d_loss: -0.040684  g_loss: 0.458121\n",
      "Elapsed: [17:24:13.915754]  batch: 150  d_loss: 0.154628  g_loss: 0.310233\n",
      "Elapsed: [17:24:37.376250]  batch: 200  d_loss: 0.057774  g_loss: 0.598757\n",
      "Elapsed: [17:25:01.039267]  batch: 250  d_loss: -0.021667  g_loss: 0.318554\n",
      "Elapsed: [17:25:24.418385]  batch: 300  d_loss: -0.082808  g_loss: 0.290967\n",
      "Elapsed: [17:25:47.272367]  batch: 350  d_loss: 0.038658  g_loss: -0.167015\n",
      "Elapsed: [17:26:10.143067]  batch: 400  d_loss: 0.037448  g_loss: -0.270276\n",
      "Elapsed: [17:26:33.025965]  batch: 450  d_loss: 0.084030  g_loss: 0.833411\n",
      "Elapsed: [17:26:55.998666]  batch: 500  d_loss: 0.026370  g_loss: 0.325782\n",
      "Time taken for epoch: 249.916 secs\n",
      "ticker =  48501\n",
      "\n",
      "Epoch: 98\n",
      "Elapsed: [17:27:14.164036]  batch: 1  d_loss: 0.074639  g_loss: 0.190287\n",
      "Elapsed: [17:27:36.459740]  batch: 50  d_loss: 0.092450  g_loss: -0.074069\n",
      "Elapsed: [17:27:59.311719]  batch: 100  d_loss: -0.031355  g_loss: 0.264662\n",
      "Elapsed: [17:28:22.273270]  batch: 150  d_loss: 0.027558  g_loss: 0.836699\n",
      "Elapsed: [17:28:45.475018]  batch: 200  d_loss: -0.081215  g_loss: 0.496615\n",
      "Elapsed: [17:29:09.151100]  batch: 250  d_loss: 0.127099  g_loss: -0.117237\n",
      "Elapsed: [17:29:33.036892]  batch: 300  d_loss: 0.061866  g_loss: 0.200031\n",
      "Elapsed: [17:29:55.959184]  batch: 350  d_loss: 0.004999  g_loss: 0.054092\n",
      "Elapsed: [17:30:18.961175]  batch: 400  d_loss: -0.090619  g_loss: 0.312908\n",
      "Elapsed: [17:30:41.914274]  batch: 450  d_loss: 0.089200  g_loss: 0.112524\n",
      "Elapsed: [17:31:04.810058]  batch: 500  d_loss: 0.087998  g_loss: 0.100004\n",
      "Time taken for epoch: 248.587 secs\n",
      "ticker =  49001\n",
      "\n",
      "Epoch: 99\n",
      "Elapsed: [17:31:22.616159]  batch: 1  d_loss: 0.029542  g_loss: 0.509888\n",
      "Elapsed: [17:31:44.956542]  batch: 50  d_loss: 0.035309  g_loss: 0.367752\n",
      "Elapsed: [17:32:07.787808]  batch: 100  d_loss: -0.001340  g_loss: -0.178396\n",
      "Elapsed: [17:32:31.041456]  batch: 150  d_loss: -0.000784  g_loss: 0.029644\n",
      "Elapsed: [17:32:53.986603]  batch: 200  d_loss: -0.037580  g_loss: -0.051837\n",
      "Elapsed: [17:33:17.054353]  batch: 250  d_loss: 0.024116  g_loss: 0.717025\n",
      "Elapsed: [17:33:39.981046]  batch: 300  d_loss: 0.025906  g_loss: 0.190840\n",
      "Elapsed: [17:34:03.111920]  batch: 350  d_loss: 0.019346  g_loss: 0.259438\n",
      "Elapsed: [17:34:26.123626]  batch: 400  d_loss: -0.021880  g_loss: 0.507689\n",
      "Elapsed: [17:34:49.123529]  batch: 450  d_loss: 0.001079  g_loss: 0.841774\n",
      "Elapsed: [17:35:12.096418]  batch: 500  d_loss: -0.117513  g_loss: 0.596735\n",
      "Time taken for epoch: 247.164 secs\n",
      "ticker =  49501\n",
      "\n",
      "Epoch: 100\n",
      "Elapsed: [17:35:29.883743]  batch: 1  d_loss: 0.028413  g_loss: 0.307815\n",
      "Elapsed: [17:35:52.238878]  batch: 50  d_loss: 0.025373  g_loss: -0.121249\n",
      "Elapsed: [17:36:15.066787]  batch: 100  d_loss: -0.125012  g_loss: 0.413331\n",
      "Elapsed: [17:36:37.875851]  batch: 150  d_loss: 0.214309  g_loss: -0.015546\n",
      "Elapsed: [17:37:00.710951]  batch: 200  d_loss: -0.078643  g_loss: 0.380826\n",
      "Elapsed: [17:37:23.693279]  batch: 250  d_loss: -0.035955  g_loss: 0.255923\n",
      "Elapsed: [17:37:47.019050]  batch: 300  d_loss: -0.061555  g_loss: 0.782842\n",
      "Elapsed: [17:38:10.012430]  batch: 350  d_loss: -0.050504  g_loss: -0.086470\n",
      "Elapsed: [17:38:32.960729]  batch: 400  d_loss: -0.047025  g_loss: -0.010545\n",
      "Elapsed: [17:38:55.906521]  batch: 450  d_loss: -0.148497  g_loss: -0.569274\n",
      "Elapsed: [17:39:18.789221]  batch: 500  d_loss: 0.032619  g_loss: 0.483302\n",
      "Time taken for epoch: 246.599 secs\n",
      "ticker =  50001\n",
      "\n",
      "Epoch: 101\n",
      "Elapsed: [17:39:36.088505]  batch: 1  d_loss: 0.222132  g_loss: -0.169308\n",
      "Elapsed: [17:39:58.686641]  batch: 50  d_loss: 0.026537  g_loss: 0.417702\n",
      "Elapsed: [17:40:22.185208]  batch: 100  d_loss: 0.049228  g_loss: 0.021559\n",
      "Elapsed: [17:40:45.538897]  batch: 150  d_loss: -0.034876  g_loss: 0.210573\n",
      "Elapsed: [17:41:09.178731]  batch: 200  d_loss: 0.001671  g_loss: -0.082771\n",
      "Elapsed: [17:41:32.336036]  batch: 250  d_loss: -0.058893  g_loss: 0.334369\n",
      "Elapsed: [17:41:55.522960]  batch: 300  d_loss: -0.007974  g_loss: 0.588790\n",
      "Elapsed: [17:42:18.619153]  batch: 350  d_loss: 0.057384  g_loss: -0.035817\n",
      "Elapsed: [17:42:42.019913]  batch: 400  d_loss: -0.020989  g_loss: 0.078989\n",
      "Elapsed: [17:43:13.508653]  batch: 450  d_loss: -0.113716  g_loss: 0.450402\n",
      "Elapsed: [17:45:47.775756]  batch: 500  d_loss: 0.067083  g_loss: 0.000363\n",
      "Time taken for epoch: 388.547 secs\n",
      "ticker =  50501\n",
      "\n",
      "Epoch: 102\n",
      "Elapsed: [17:45:56.485021]  batch: 1  d_loss: -0.089763  g_loss: 0.340466\n",
      "Elapsed: [17:46:18.066189]  batch: 50  d_loss: -0.037383  g_loss: 0.033239\n",
      "Elapsed: [17:46:40.391570]  batch: 100  d_loss: 0.010426  g_loss: -0.076057\n",
      "Elapsed: [17:47:02.808989]  batch: 150  d_loss: 0.005095  g_loss: 0.417282\n",
      "Elapsed: [17:47:28.804302]  batch: 200  d_loss: 0.017703  g_loss: -0.158195\n",
      "Elapsed: [17:47:55.363596]  batch: 250  d_loss: -0.085111  g_loss: 0.688059\n",
      "Elapsed: [17:48:23.024899]  batch: 300  d_loss: 0.057285  g_loss: 0.502789\n",
      "Elapsed: [17:48:51.578999]  batch: 350  d_loss: -0.053434  g_loss: -0.057943\n",
      "Elapsed: [17:49:19.125890]  batch: 400  d_loss: 0.075120  g_loss: -0.091574\n",
      "Elapsed: [17:49:48.323650]  batch: 450  d_loss: -0.055802  g_loss: 0.198244\n",
      "Elapsed: [17:50:17.597819]  batch: 500  d_loss: 0.048938  g_loss: -0.164682\n",
      "Time taken for epoch: 269.911 secs\n",
      "ticker =  51001\n",
      "\n",
      "Epoch: 103\n",
      "Elapsed: [17:50:29.314322]  batch: 1  d_loss: 0.170029  g_loss: -0.001692\n",
      "Elapsed: [17:50:55.229686]  batch: 50  d_loss: 0.091683  g_loss: 0.121062\n",
      "Elapsed: [17:51:24.617310]  batch: 100  d_loss: 0.032364  g_loss: 0.435905\n",
      "Elapsed: [17:51:56.707346]  batch: 150  d_loss: 0.059635  g_loss: -0.090561\n",
      "Elapsed: [17:52:26.077698]  batch: 200  d_loss: -0.001814  g_loss: 0.286617\n",
      "Elapsed: [17:52:58.322920]  batch: 250  d_loss: -0.022308  g_loss: 0.242920\n",
      "Elapsed: [17:53:30.347744]  batch: 300  d_loss: 0.054017  g_loss: 0.165419\n",
      "Elapsed: [17:53:59.692577]  batch: 350  d_loss: 0.025450  g_loss: 0.115400\n",
      "Elapsed: [17:54:29.060236]  batch: 400  d_loss: -0.196450  g_loss: -0.160239\n",
      "Elapsed: [17:55:01.112897]  batch: 450  d_loss: -0.118386  g_loss: 0.494274\n",
      "Elapsed: [17:55:33.088213]  batch: 500  d_loss: -0.010195  g_loss: 0.665882\n",
      "Time taken for epoch: 315.340 secs\n",
      "ticker =  51501\n",
      "\n",
      "Epoch: 104\n",
      "Elapsed: [17:55:44.637036]  batch: 1  d_loss: -0.155855  g_loss: 0.699530\n",
      "Elapsed: [17:56:11.118988]  batch: 50  d_loss: -0.062661  g_loss: 0.454528\n",
      "Elapsed: [17:56:40.459442]  batch: 100  d_loss: 0.021310  g_loss: 0.182086\n",
      "Elapsed: [17:57:09.229343]  batch: 150  d_loss: -0.016519  g_loss: 0.084844\n",
      "Elapsed: [17:57:36.346730]  batch: 200  d_loss: -0.046920  g_loss: 0.569988\n",
      "Elapsed: [17:58:03.467299]  batch: 250  d_loss: -0.020617  g_loss: 0.425833\n",
      "Elapsed: [17:58:30.512148]  batch: 300  d_loss: 0.110128  g_loss: -0.010385\n",
      "Elapsed: [17:58:57.477966]  batch: 350  d_loss: -0.075347  g_loss: 0.727362\n",
      "Elapsed: [17:59:24.365373]  batch: 400  d_loss: -0.035147  g_loss: 0.343087\n",
      "Elapsed: [17:59:51.249107]  batch: 450  d_loss: 0.015957  g_loss: 0.283159\n",
      "Elapsed: [18:00:17.225415]  batch: 500  d_loss: -0.071636  g_loss: 0.294306\n",
      "Time taken for epoch: 284.243 secs\n",
      "ticker =  52001\n",
      "\n",
      "Epoch: 105\n",
      "Elapsed: [18:00:36.927752]  batch: 1  d_loss: 0.019500  g_loss: 0.074809\n",
      "Elapsed: [18:00:59.478010]  batch: 50  d_loss: -0.054826  g_loss: 0.125798\n",
      "Elapsed: [18:01:23.792372]  batch: 100  d_loss: 0.017560  g_loss: 0.130593\n",
      "Elapsed: [18:01:49.335124]  batch: 150  d_loss: -0.127018  g_loss: 0.298019\n",
      "Elapsed: [18:02:16.301102]  batch: 200  d_loss: 0.114992  g_loss: 0.468166\n",
      "Elapsed: [18:02:42.649731]  batch: 250  d_loss: -0.075665  g_loss: 0.767483\n",
      "Elapsed: [18:03:08.998414]  batch: 300  d_loss: -0.130097  g_loss: 0.475771\n",
      "Elapsed: [18:03:35.893845]  batch: 350  d_loss: -0.047117  g_loss: 0.167363\n",
      "Elapsed: [18:04:01.778782]  batch: 400  d_loss: 0.065289  g_loss: 0.142162\n",
      "Elapsed: [18:04:29.849181]  batch: 450  d_loss: -0.046405  g_loss: 0.320873\n",
      "Elapsed: [18:04:57.700796]  batch: 500  d_loss: 0.020012  g_loss: 0.192730\n",
      "Time taken for epoch: 281.018 secs\n",
      "ticker =  52501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 106\n",
      "Elapsed: [18:05:20.610935]  batch: 1  d_loss: -0.093692  g_loss: 0.313877\n",
      "Elapsed: [18:05:43.989455]  batch: 50  d_loss: 0.017829  g_loss: 0.004962\n",
      "Elapsed: [18:06:08.114472]  batch: 100  d_loss: 0.077041  g_loss: 0.482959\n",
      "Elapsed: [18:06:32.809328]  batch: 150  d_loss: -0.071262  g_loss: 0.229988\n",
      "Elapsed: [18:07:02.145631]  batch: 200  d_loss: 0.010544  g_loss: 0.421873\n",
      "Elapsed: [18:07:26.473672]  batch: 250  d_loss: -0.051489  g_loss: 0.061582\n",
      "Elapsed: [18:07:51.295424]  batch: 300  d_loss: -0.187003  g_loss: 0.005416\n",
      "Elapsed: [18:08:16.910426]  batch: 350  d_loss: 0.124511  g_loss: -0.116334\n",
      "Elapsed: [18:08:41.470147]  batch: 400  d_loss: -0.013159  g_loss: 0.093161\n",
      "Elapsed: [18:09:06.352391]  batch: 450  d_loss: -0.117774  g_loss: 0.720112\n",
      "Elapsed: [18:09:31.195037]  batch: 500  d_loss: 0.002042  g_loss: -0.384205\n",
      "Time taken for epoch: 272.364 secs\n",
      "ticker =  53001\n",
      "\n",
      "Epoch: 107\n",
      "Elapsed: [18:09:51.919825]  batch: 1  d_loss: -0.066864  g_loss: -0.340056\n",
      "Elapsed: [18:10:14.331434]  batch: 50  d_loss: 0.014761  g_loss: 0.102850\n",
      "Elapsed: [18:10:37.285838]  batch: 100  d_loss: -0.139641  g_loss: 0.294482\n",
      "Elapsed: [18:11:00.291972]  batch: 150  d_loss: 0.144213  g_loss: 0.152152\n",
      "Elapsed: [18:11:23.361257]  batch: 200  d_loss: -0.035246  g_loss: -0.035260\n",
      "Elapsed: [18:14:03.921602]  batch: 250  d_loss: -0.113499  g_loss: 0.432755\n",
      "Elapsed: [1 day, 0:45:06.698286]  batch: 300  d_loss: 0.013098  g_loss: 0.149209\n",
      "Elapsed: [1 day, 0:45:31.787561]  batch: 350  d_loss: 0.040057  g_loss: 0.368767\n",
      "Elapsed: [1 day, 0:45:57.592217]  batch: 400  d_loss: 0.011770  g_loss: 0.127092\n",
      "Elapsed: [1 day, 0:46:24.108617]  batch: 450  d_loss: -0.060613  g_loss: -0.020249\n",
      "Elapsed: [1 day, 0:46:51.124719]  batch: 500  d_loss: 0.126327  g_loss: 0.936910\n",
      "Time taken for epoch: 23848.747 secs\n",
      "ticker =  53501\n",
      "\n",
      "Epoch: 108\n",
      "Elapsed: [1 day, 0:47:00.839725]  batch: 1  d_loss: 0.010757  g_loss: 0.467765\n",
      "Elapsed: [1 day, 0:47:27.493675]  batch: 50  d_loss: 0.042754  g_loss: 0.418597\n",
      "Elapsed: [1 day, 0:47:54.583802]  batch: 100  d_loss: 0.127695  g_loss: -0.210386\n",
      "Elapsed: [1 day, 0:48:21.811302]  batch: 150  d_loss: -0.023351  g_loss: 0.335779\n",
      "Elapsed: [1 day, 0:48:48.768130]  batch: 200  d_loss: -0.121731  g_loss: 0.466613\n",
      "Elapsed: [1 day, 0:49:15.523093]  batch: 250  d_loss: 0.012144  g_loss: 0.591217\n",
      "Elapsed: [1 day, 0:49:42.194264]  batch: 300  d_loss: -0.066590  g_loss: 0.293774\n",
      "Elapsed: [1 day, 0:50:08.871343]  batch: 350  d_loss: -0.078945  g_loss: 0.468476\n",
      "Elapsed: [1 day, 0:50:35.597489]  batch: 400  d_loss: 0.021125  g_loss: 0.068744\n",
      "Elapsed: [1 day, 0:51:02.241881]  batch: 450  d_loss: -0.063822  g_loss: 0.103670\n",
      "Elapsed: [1 day, 0:51:28.780865]  batch: 500  d_loss: 0.070775  g_loss: 0.211038\n",
      "Time taken for epoch: 277.616 secs\n",
      "ticker =  54001\n",
      "\n",
      "Epoch: 109\n",
      "Elapsed: [1 day, 0:51:41.681157]  batch: 1  d_loss: -0.049834  g_loss: 0.551315\n",
      "Elapsed: [1 day, 0:52:07.391926]  batch: 50  d_loss: -0.280820  g_loss: 1.039929\n",
      "Elapsed: [1 day, 0:52:33.839279]  batch: 100  d_loss: -0.171993  g_loss: 0.201143\n",
      "Elapsed: [1 day, 0:53:00.341377]  batch: 150  d_loss: 0.060940  g_loss: 0.022219\n",
      "Elapsed: [1 day, 0:53:27.422059]  batch: 200  d_loss: 0.066813  g_loss: -0.071465\n",
      "Elapsed: [1 day, 0:53:53.837539]  batch: 250  d_loss: 0.084041  g_loss: -0.293447\n",
      "Elapsed: [1 day, 0:54:20.309366]  batch: 300  d_loss: -0.151647  g_loss: 0.113597\n",
      "Elapsed: [1 day, 0:54:46.834199]  batch: 350  d_loss: 0.058812  g_loss: 0.617748\n",
      "Elapsed: [1 day, 0:55:13.471744]  batch: 400  d_loss: -0.092985  g_loss: 0.230093\n",
      "Elapsed: [1 day, 0:55:39.847700]  batch: 450  d_loss: 0.079325  g_loss: -0.658551\n",
      "Elapsed: [1 day, 0:56:06.192982]  batch: 500  d_loss: 0.059849  g_loss: 0.041528\n",
      "Time taken for epoch: 277.320 secs\n",
      "ticker =  54501\n",
      "\n",
      "Epoch: 110\n",
      "Elapsed: [1 day, 0:56:20.013299]  batch: 1  d_loss: -0.067715  g_loss: 0.275968\n",
      "Elapsed: [1 day, 0:56:45.755295]  batch: 50  d_loss: -0.067930  g_loss: 0.405675\n",
      "Elapsed: [1 day, 0:57:12.327259]  batch: 100  d_loss: -0.097022  g_loss: 0.356707\n",
      "Elapsed: [1 day, 0:57:38.993309]  batch: 150  d_loss: 0.032234  g_loss: 0.203641\n",
      "Elapsed: [1 day, 0:58:05.699439]  batch: 200  d_loss: 0.125020  g_loss: 0.056309\n",
      "Elapsed: [1 day, 0:58:32.385791]  batch: 250  d_loss: 0.024886  g_loss: 0.417994\n",
      "Elapsed: [1 day, 0:58:59.115350]  batch: 300  d_loss: -0.211302  g_loss: 0.135040\n",
      "Elapsed: [1 day, 0:59:26.091868]  batch: 350  d_loss: 0.055226  g_loss: 0.360449\n",
      "Elapsed: [1 day, 0:59:53.317648]  batch: 400  d_loss: 0.063768  g_loss: 0.014459\n",
      "Elapsed: [1 day, 1:00:21.657158]  batch: 450  d_loss: 0.136737  g_loss: -0.298103\n",
      "Elapsed: [1 day, 1:00:49.248713]  batch: 500  d_loss: -0.158246  g_loss: 0.447293\n",
      "Time taken for epoch: 283.012 secs\n",
      "ticker =  55001\n",
      "\n",
      "Epoch: 111\n",
      "Elapsed: [1 day, 1:01:07.592541]  batch: 1  d_loss: -0.083172  g_loss: 0.440096\n",
      "Elapsed: [1 day, 1:01:34.480441]  batch: 50  d_loss: -0.185393  g_loss: 0.861414\n",
      "Elapsed: [1 day, 1:02:02.366125]  batch: 100  d_loss: -0.091824  g_loss: 0.634558\n",
      "Elapsed: [1 day, 1:02:30.556904]  batch: 150  d_loss: 0.051971  g_loss: -0.275446\n",
      "Elapsed: [1 day, 1:02:58.878866]  batch: 200  d_loss: 0.095003  g_loss: -0.258493\n",
      "Elapsed: [1 day, 1:03:27.321168]  batch: 250  d_loss: 0.141390  g_loss: 0.048011\n",
      "Elapsed: [1 day, 1:03:55.866771]  batch: 300  d_loss: 0.077302  g_loss: 0.053277\n",
      "Elapsed: [1 day, 1:04:24.501454]  batch: 350  d_loss: -0.134185  g_loss: 0.197787\n",
      "Elapsed: [1 day, 1:04:53.178396]  batch: 400  d_loss: -0.103031  g_loss: 0.410773\n",
      "Elapsed: [1 day, 1:05:21.944642]  batch: 450  d_loss: 0.047093  g_loss: -0.285553\n",
      "Elapsed: [1 day, 1:05:50.751104]  batch: 500  d_loss: -0.206255  g_loss: 0.253750\n",
      "Time taken for epoch: 301.345 secs\n",
      "ticker =  55501\n",
      "\n",
      "Epoch: 112\n",
      "Elapsed: [1 day, 1:06:08.876322]  batch: 1  d_loss: 0.105767  g_loss: -0.124802\n",
      "Elapsed: [1 day, 1:06:36.629942]  batch: 50  d_loss: -0.164076  g_loss: 0.250427\n",
      "Elapsed: [1 day, 1:07:05.299217]  batch: 100  d_loss: -0.151837  g_loss: 0.771839\n",
      "Elapsed: [1 day, 1:07:33.974122]  batch: 150  d_loss: 0.057890  g_loss: 0.182922\n",
      "Elapsed: [1 day, 1:08:02.801143]  batch: 200  d_loss: -0.144869  g_loss: 0.606687\n",
      "Elapsed: [1 day, 1:08:31.831498]  batch: 250  d_loss: 0.088833  g_loss: 0.440386\n",
      "Elapsed: [1 day, 1:09:00.798201]  batch: 300  d_loss: 0.133459  g_loss: 0.176423\n",
      "Elapsed: [1 day, 1:09:29.806633]  batch: 350  d_loss: -0.247346  g_loss: 0.415316\n",
      "Elapsed: [1 day, 1:10:03.318215]  batch: 400  d_loss: -0.169085  g_loss: 0.244525\n",
      "Elapsed: [1 day, 1:10:32.439317]  batch: 450  d_loss: 0.128659  g_loss: -0.098516\n",
      "Elapsed: [1 day, 1:11:01.666512]  batch: 500  d_loss: 0.045480  g_loss: 0.306792\n",
      "Time taken for epoch: 310.667 secs\n",
      "ticker =  56001\n",
      "\n",
      "Epoch: 113\n",
      "Elapsed: [1 day, 1:11:21.715672]  batch: 1  d_loss: -0.043033  g_loss: 0.283102\n",
      "Elapsed: [1 day, 1:11:49.897026]  batch: 50  d_loss: -0.220385  g_loss: 0.643776\n",
      "Elapsed: [1 day, 1:12:18.953908]  batch: 100  d_loss: 0.038015  g_loss: 0.271163\n",
      "Elapsed: [1 day, 1:12:51.147249]  batch: 150  d_loss: 0.005482  g_loss: 0.344791\n",
      "Elapsed: [1 day, 1:13:24.676838]  batch: 200  d_loss: 0.161508  g_loss: 0.104846\n",
      "Elapsed: [1 day, 1:13:59.920566]  batch: 250  d_loss: 0.108629  g_loss: 0.110527\n",
      "Elapsed: [1 day, 1:14:31.970187]  batch: 300  d_loss: 0.027729  g_loss: 0.396379\n",
      "Elapsed: [1 day, 1:15:06.828197]  batch: 350  d_loss: 0.108873  g_loss: -0.147362\n",
      "Elapsed: [1 day, 1:15:41.681612]  batch: 400  d_loss: -0.125153  g_loss: 0.370277\n",
      "Elapsed: [1 day, 1:16:16.477100]  batch: 450  d_loss: -0.147670  g_loss: 0.476291\n",
      "Elapsed: [1 day, 1:16:51.226900]  batch: 500  d_loss: -0.096418  g_loss: 0.779392\n",
      "Time taken for epoch: 349.434 secs\n",
      "ticker =  56501\n",
      "\n",
      "Epoch: 114\n",
      "Elapsed: [1 day, 1:17:10.451413]  batch: 1  d_loss: 0.131203  g_loss: 0.093178\n",
      "Elapsed: [1 day, 1:17:38.856166]  batch: 50  d_loss: -0.259184  g_loss: 0.804140\n",
      "Elapsed: [1 day, 1:18:11.169551]  batch: 100  d_loss: -0.040434  g_loss: 0.554674\n",
      "Elapsed: [1 day, 1:18:54.178372]  batch: 150  d_loss: -0.332249  g_loss: 0.408144\n",
      "Elapsed: [1 day, 1:19:34.485207]  batch: 200  d_loss: 0.199503  g_loss: 0.206998\n",
      "Elapsed: [1 day, 1:20:14.737842]  batch: 250  d_loss: 0.081748  g_loss: 0.265232\n",
      "Elapsed: [1 day, 1:20:57.928715]  batch: 300  d_loss: -0.044108  g_loss: 0.213298\n",
      "Elapsed: [1 day, 1:21:32.811200]  batch: 350  d_loss: -0.108988  g_loss: 0.214512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: [1 day, 1:22:10.250345]  batch: 400  d_loss: -0.032602  g_loss: 0.330644\n",
      "Elapsed: [1 day, 1:22:49.087701]  batch: 450  d_loss: -0.130776  g_loss: 0.462800\n",
      "Elapsed: [1 day, 1:23:27.578562]  batch: 500  d_loss: -0.263249  g_loss: 0.573325\n",
      "Time taken for epoch: 396.033 secs\n",
      "ticker =  57001\n",
      "\n",
      "Epoch: 115\n",
      "Elapsed: [1 day, 1:23:45.666726]  batch: 1  d_loss: 0.115810  g_loss: 0.061311\n",
      "Elapsed: [1 day, 1:24:13.027415]  batch: 50  d_loss: 0.103409  g_loss: -0.137037\n",
      "Elapsed: [1 day, 1:24:40.697996]  batch: 100  d_loss: -0.293041  g_loss: 0.204121\n",
      "Elapsed: [1 day, 1:25:08.169663]  batch: 150  d_loss: -0.044587  g_loss: 0.137129\n",
      "Elapsed: [1 day, 1:25:35.477599]  batch: 200  d_loss: 0.076299  g_loss: -0.159976\n",
      "Elapsed: [1 day, 1:26:02.676718]  batch: 250  d_loss: 0.025309  g_loss: 0.088826\n",
      "Elapsed: [1 day, 1:26:29.776229]  batch: 300  d_loss: -0.049574  g_loss: 0.372916\n",
      "Elapsed: [1 day, 1:26:56.863929]  batch: 350  d_loss: -0.060775  g_loss: 0.476003\n",
      "Elapsed: [1 day, 1:27:23.880986]  batch: 400  d_loss: 0.080315  g_loss: 0.169504\n",
      "Elapsed: [1 day, 1:27:50.993164]  batch: 450  d_loss: 0.111300  g_loss: 0.250692\n",
      "Elapsed: [1 day, 1:28:18.052362]  batch: 500  d_loss: -0.224835  g_loss: 0.418685\n",
      "Time taken for epoch: 290.289 secs\n",
      "ticker =  57501\n",
      "\n",
      "Epoch: 116\n",
      "Elapsed: [1 day, 1:28:32.606797]  batch: 1  d_loss: 0.072341  g_loss: -0.123126\n",
      "Elapsed: [1 day, 1:28:58.835593]  batch: 50  d_loss: 0.108467  g_loss: 0.057277\n",
      "Elapsed: [1 day, 1:29:25.589174]  batch: 100  d_loss: -0.014487  g_loss: 0.243095\n",
      "Elapsed: [1 day, 1:29:52.524847]  batch: 150  d_loss: -0.267327  g_loss: 0.552098\n",
      "Elapsed: [1 day, 1:30:19.400342]  batch: 200  d_loss: -0.164442  g_loss: 0.481254\n",
      "Elapsed: [1 day, 1:30:46.292473]  batch: 250  d_loss: -0.095801  g_loss: 0.429890\n",
      "Elapsed: [1 day, 1:31:13.244914]  batch: 300  d_loss: -0.023888  g_loss: 0.805174\n",
      "Elapsed: [1 day, 1:31:40.172278]  batch: 350  d_loss: -0.081590  g_loss: 0.201304\n",
      "Elapsed: [1 day, 1:32:07.134300]  batch: 400  d_loss: -0.013755  g_loss: 0.591782\n",
      "Elapsed: [1 day, 1:32:33.991105]  batch: 450  d_loss: -0.152264  g_loss: 0.347957\n",
      "Elapsed: [1 day, 1:33:00.896999]  batch: 500  d_loss: 0.049283  g_loss: 0.174967\n",
      "Time taken for epoch: 282.715 secs\n",
      "ticker =  58001\n",
      "\n",
      "Epoch: 117\n",
      "Elapsed: [1 day, 1:33:14.285547]  batch: 1  d_loss: -0.208375  g_loss: 0.525096\n",
      "Elapsed: [1 day, 1:33:40.383128]  batch: 50  d_loss: 0.077059  g_loss: 0.306071\n",
      "Elapsed: [1 day, 1:34:07.454361]  batch: 100  d_loss: -0.046755  g_loss: 0.328962\n",
      "Elapsed: [1 day, 1:34:34.338304]  batch: 150  d_loss: -0.091774  g_loss: 0.152422\n",
      "Elapsed: [1 day, 1:35:01.215154]  batch: 200  d_loss: 0.026250  g_loss: 0.080649\n",
      "Elapsed: [1 day, 1:35:28.006637]  batch: 250  d_loss: 0.185709  g_loss: 0.138253\n",
      "Elapsed: [1 day, 1:35:54.800437]  batch: 300  d_loss: 0.135015  g_loss: -0.056472\n",
      "Elapsed: [1 day, 1:36:21.527987]  batch: 350  d_loss: -0.194721  g_loss: 0.557557\n",
      "Elapsed: [1 day, 1:36:48.290541]  batch: 400  d_loss: -0.129528  g_loss: 0.351171\n",
      "Elapsed: [1 day, 1:37:15.037277]  batch: 450  d_loss: -0.090132  g_loss: 0.393091\n",
      "Elapsed: [1 day, 1:37:41.825273]  batch: 500  d_loss: 0.022511  g_loss: 0.161739\n",
      "Time taken for epoch: 280.830 secs\n",
      "ticker =  58501\n",
      "\n",
      "Epoch: 118\n",
      "Elapsed: [1 day, 1:37:55.156832]  batch: 1  d_loss: -0.164908  g_loss: 0.433433\n",
      "Elapsed: [1 day, 1:38:21.354703]  batch: 50  d_loss: -0.288762  g_loss: 0.534105\n",
      "Elapsed: [1 day, 1:38:48.248056]  batch: 100  d_loss: -0.004856  g_loss: 0.253362\n",
      "Elapsed: [1 day, 1:39:15.210747]  batch: 150  d_loss: 0.104237  g_loss: 0.113953\n",
      "Elapsed: [1 day, 1:39:42.141850]  batch: 200  d_loss: -0.200410  g_loss: 0.388212\n",
      "Elapsed: [1 day, 1:40:09.189357]  batch: 250  d_loss: -0.014909  g_loss: 0.568166\n",
      "Elapsed: [1 day, 1:40:36.212759]  batch: 300  d_loss: -0.008597  g_loss: 0.207373\n",
      "Elapsed: [1 day, 1:41:03.080952]  batch: 350  d_loss: -0.129990  g_loss: 0.387091\n",
      "Elapsed: [1 day, 1:41:29.917800]  batch: 400  d_loss: 0.081926  g_loss: 0.058359\n",
      "Elapsed: [1 day, 1:41:56.737477]  batch: 450  d_loss: 0.053613  g_loss: -0.044839\n",
      "Elapsed: [1 day, 1:42:23.566407]  batch: 500  d_loss: -0.094113  g_loss: 0.134280\n",
      "Time taken for epoch: 281.665 secs\n",
      "ticker =  59001\n",
      "\n",
      "Epoch: 119\n",
      "Elapsed: [1 day, 1:42:37.198995]  batch: 1  d_loss: -0.137895  g_loss: 0.159544\n",
      "Elapsed: [1 day, 1:43:03.200140]  batch: 50  d_loss: 0.013261  g_loss: 0.079392\n",
      "Elapsed: [1 day, 1:43:29.948668]  batch: 100  d_loss: -0.127192  g_loss: 0.649547\n",
      "Elapsed: [1 day, 1:43:56.831013]  batch: 150  d_loss: 0.113468  g_loss: 0.249047\n",
      "Elapsed: [1 day, 1:44:23.762830]  batch: 200  d_loss: -0.216351  g_loss: 0.528626\n",
      "Elapsed: [1 day, 1:44:50.683023]  batch: 250  d_loss: -0.245549  g_loss: 0.606104\n",
      "Elapsed: [1 day, 1:45:17.480282]  batch: 300  d_loss: 0.054973  g_loss: 0.188212\n",
      "Elapsed: [1 day, 1:45:44.274441]  batch: 350  d_loss: -0.019165  g_loss: -0.009629\n",
      "Elapsed: [1 day, 1:46:10.933580]  batch: 400  d_loss: -0.090287  g_loss: 0.551354\n",
      "Elapsed: [1 day, 1:46:37.629961]  batch: 450  d_loss: -0.151718  g_loss: 0.268572\n",
      "Elapsed: [1 day, 1:47:04.327855]  batch: 500  d_loss: 0.056483  g_loss: 0.087465\n",
      "Time taken for epoch: 280.610 secs\n",
      "ticker =  59501\n",
      "\n",
      "Epoch: 120\n",
      "Elapsed: [1 day, 1:47:18.520051]  batch: 1  d_loss: -0.059162  g_loss: 0.431861\n",
      "Elapsed: [1 day, 1:47:44.648274]  batch: 50  d_loss: 0.023655  g_loss: -0.111025\n",
      "Elapsed: [1 day, 1:49:52.762913]  batch: 100  d_loss: -0.067108  g_loss: 0.212563\n",
      "Elapsed: [1 day, 1:55:03.920844]  batch: 150  d_loss: 0.086514  g_loss: 0.019958\n",
      "Elapsed: [1 day, 1:56:15.870557]  batch: 200  d_loss: 0.069060  g_loss: 0.161108\n",
      "Elapsed: [1 day, 1:56:42.680078]  batch: 250  d_loss: -0.128254  g_loss: 0.390564\n",
      "Elapsed: [1 day, 1:57:09.950022]  batch: 300  d_loss: 0.024732  g_loss: -0.009723\n",
      "Elapsed: [1 day, 1:57:37.641096]  batch: 350  d_loss: -0.099398  g_loss: -0.053057\n",
      "Elapsed: [1 day, 1:58:05.751928]  batch: 400  d_loss: 0.035128  g_loss: 0.217284\n",
      "Elapsed: [1 day, 1:58:34.759076]  batch: 450  d_loss: 0.087452  g_loss: 0.111567\n",
      "Elapsed: [1 day, 1:59:03.559066]  batch: 500  d_loss: -0.133342  g_loss: 0.462764\n",
      "Time taken for epoch: 719.177 secs\n",
      "ticker =  60001\n",
      "\n",
      "Epoch: 121\n",
      "Elapsed: [1 day, 1:59:15.727100]  batch: 1  d_loss: 0.041671  g_loss: 0.062091\n",
      "Elapsed: [1 day, 1:59:43.498084]  batch: 50  d_loss: 0.128912  g_loss: 0.141847\n",
      "Elapsed: [1 day, 2:00:11.639332]  batch: 100  d_loss: -0.147043  g_loss: 0.565033\n",
      "Elapsed: [1 day, 2:00:39.538953]  batch: 150  d_loss: 0.147216  g_loss: 0.053159\n",
      "Elapsed: [1 day, 2:01:07.199392]  batch: 200  d_loss: -0.002288  g_loss: 0.344955\n",
      "Elapsed: [1 day, 2:01:34.465302]  batch: 250  d_loss: 0.157279  g_loss: 0.207679\n",
      "Elapsed: [1 day, 2:02:01.656260]  batch: 300  d_loss: 0.109432  g_loss: -0.031544\n",
      "Elapsed: [1 day, 2:02:28.836433]  batch: 350  d_loss: -0.084176  g_loss: 0.209766\n",
      "Elapsed: [1 day, 2:02:56.006945]  batch: 400  d_loss: -0.223051  g_loss: 0.361162\n",
      "Elapsed: [1 day, 2:03:23.092048]  batch: 450  d_loss: 0.050738  g_loss: 0.181632\n",
      "Elapsed: [1 day, 2:03:50.197809]  batch: 500  d_loss: -0.082488  g_loss: 0.390884\n",
      "Time taken for epoch: 286.524 secs\n",
      "ticker =  60501\n",
      "\n",
      "Epoch: 122\n",
      "Elapsed: [1 day, 2:04:05.027490]  batch: 1  d_loss: -0.049161  g_loss: 0.468175\n",
      "Elapsed: [1 day, 2:04:31.183054]  batch: 50  d_loss: -0.098980  g_loss: 0.226692\n",
      "Elapsed: [1 day, 2:04:57.957716]  batch: 100  d_loss: 0.083031  g_loss: 0.164184\n",
      "Elapsed: [1 day, 2:05:24.798801]  batch: 150  d_loss: -0.175431  g_loss: 0.598758\n",
      "Elapsed: [1 day, 2:05:51.641818]  batch: 200  d_loss: 0.061907  g_loss: 0.509982\n",
      "Elapsed: [1 day, 2:06:18.406393]  batch: 250  d_loss: 0.080265  g_loss: 0.236422\n",
      "Elapsed: [1 day, 2:06:45.205457]  batch: 300  d_loss: 0.057204  g_loss: 0.235278\n",
      "Elapsed: [1 day, 2:07:11.985118]  batch: 350  d_loss: -0.096540  g_loss: 0.365241\n",
      "Elapsed: [1 day, 2:07:38.698650]  batch: 400  d_loss: -0.456155  g_loss: 0.392877\n",
      "Elapsed: [1 day, 2:08:05.421643]  batch: 450  d_loss: -0.012776  g_loss: -0.129986\n",
      "Elapsed: [1 day, 2:08:32.163767]  batch: 500  d_loss: -0.059108  g_loss: 0.175381\n",
      "Time taken for epoch: 281.666 secs\n",
      "ticker =  61001\n",
      "\n",
      "Epoch: 123\n",
      "Elapsed: [1 day, 2:08:44.988691]  batch: 1  d_loss: -0.021967  g_loss: 0.477770\n",
      "Elapsed: [1 day, 2:09:11.148958]  batch: 50  d_loss: 0.057796  g_loss: -0.016019\n",
      "Elapsed: [1 day, 2:09:38.026577]  batch: 100  d_loss: -0.083001  g_loss: 0.402360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: [1 day, 2:10:04.900585]  batch: 150  d_loss: -0.001904  g_loss: 0.230042\n",
      "Elapsed: [1 day, 2:10:31.900769]  batch: 200  d_loss: -0.005804  g_loss: -0.062081\n",
      "Elapsed: [1 day, 2:10:58.916009]  batch: 250  d_loss: 0.134343  g_loss: 0.025283\n",
      "Elapsed: [1 day, 2:11:25.921493]  batch: 300  d_loss: -0.191755  g_loss: 0.312973\n",
      "Elapsed: [1 day, 2:11:52.823910]  batch: 350  d_loss: 0.111555  g_loss: 0.184752\n",
      "Elapsed: [1 day, 2:12:19.716978]  batch: 400  d_loss: -0.117635  g_loss: 0.642344\n",
      "Elapsed: [1 day, 2:14:51.717611]  batch: 450  d_loss: 0.040024  g_loss: 0.537198\n",
      "Elapsed: [1 day, 2:18:03.100704]  batch: 500  d_loss: 0.104055  g_loss: -0.281895\n",
      "Time taken for epoch: 570.780 secs\n",
      "ticker =  61501\n",
      "\n",
      "Epoch: 124\n",
      "Elapsed: [1 day, 2:18:12.508041]  batch: 1  d_loss: -0.049527  g_loss: 0.270488\n",
      "Elapsed: [1 day, 2:18:39.096927]  batch: 50  d_loss: 0.046193  g_loss: 0.656962\n",
      "Elapsed: [1 day, 2:19:06.715847]  batch: 100  d_loss: -0.309023  g_loss: 0.707811\n",
      "Elapsed: [1 day, 2:19:34.691582]  batch: 150  d_loss: 0.157092  g_loss: -0.046234\n",
      "Elapsed: [1 day, 2:20:02.558345]  batch: 200  d_loss: -0.001820  g_loss: 0.128098\n",
      "Elapsed: [1 day, 2:20:30.543385]  batch: 250  d_loss: -0.214386  g_loss: 0.217312\n",
      "Elapsed: [1 day, 2:20:58.944930]  batch: 300  d_loss: -0.082471  g_loss: 0.502605\n",
      "Elapsed: [1 day, 2:21:27.689514]  batch: 350  d_loss: -0.085580  g_loss: 0.464926\n",
      "Elapsed: [1 day, 2:21:56.693013]  batch: 400  d_loss: 0.132971  g_loss: -0.303294\n",
      "Elapsed: [1 day, 2:22:25.616948]  batch: 450  d_loss: -0.012332  g_loss: 0.469370\n",
      "Elapsed: [1 day, 2:22:54.208110]  batch: 500  d_loss: -0.237554  g_loss: 0.399325\n",
      "Time taken for epoch: 291.084 secs\n",
      "ticker =  62001\n",
      "\n",
      "Epoch: 125\n",
      "Elapsed: [1 day, 2:23:08.542264]  batch: 1  d_loss: 0.060898  g_loss: -0.033001\n",
      "Elapsed: [1 day, 2:23:36.685869]  batch: 50  d_loss: 0.086875  g_loss: 0.113192\n",
      "Elapsed: [1 day, 2:24:05.200902]  batch: 100  d_loss: 0.004819  g_loss: 0.290202\n",
      "Elapsed: [1 day, 2:25:15.619378]  batch: 150  d_loss: 0.028354  g_loss: 0.447623\n",
      "Elapsed: [1 day, 2:25:43.114275]  batch: 200  d_loss: 0.104499  g_loss: 0.236971\n",
      "Elapsed: [1 day, 2:26:11.390427]  batch: 250  d_loss: -0.133421  g_loss: 0.303942\n",
      "Elapsed: [1 day, 2:26:39.925924]  batch: 300  d_loss: 0.059764  g_loss: 0.275421\n",
      "Elapsed: [1 day, 2:27:09.050117]  batch: 350  d_loss: 0.113651  g_loss: 0.140324\n",
      "Elapsed: [1 day, 2:27:38.290262]  batch: 400  d_loss: -0.025084  g_loss: 0.401165\n",
      "Elapsed: [1 day, 2:28:07.820295]  batch: 450  d_loss: -0.208086  g_loss: 0.793673\n",
      "Elapsed: [1 day, 2:28:40.187416]  batch: 500  d_loss: -0.073057  g_loss: 0.087500\n",
      "Time taken for epoch: 346.108 secs\n",
      "ticker =  62501\n",
      "\n",
      "Epoch: 126\n",
      "Elapsed: [1 day, 2:29:04.116958]  batch: 1  d_loss: -0.211793  g_loss: 0.386145\n",
      "Elapsed: [1 day, 2:29:33.406659]  batch: 50  d_loss: 0.023979  g_loss: 0.222625\n",
      "Elapsed: [1 day, 2:30:02.890220]  batch: 100  d_loss: -0.030480  g_loss: 0.192947\n",
      "Elapsed: [1 day, 2:30:46.037763]  batch: 150  d_loss: -0.280732  g_loss: 0.527562\n",
      "Elapsed: [1 day, 2:31:29.434345]  batch: 200  d_loss: -0.121460  g_loss: 0.460598\n",
      "Elapsed: [1 day, 2:32:15.435823]  batch: 250  d_loss: 0.107712  g_loss: -0.179087\n",
      "Elapsed: [1 day, 2:32:58.821812]  batch: 300  d_loss: -0.272943  g_loss: 0.917165\n",
      "Elapsed: [1 day, 2:33:52.829386]  batch: 350  d_loss: -0.015128  g_loss: 0.136701\n",
      "Elapsed: [1 day, 2:34:44.119432]  batch: 400  d_loss: -0.093844  g_loss: 0.186061\n",
      "Elapsed: [1 day, 2:35:38.268489]  batch: 450  d_loss: 0.047041  g_loss: 0.112248\n",
      "Elapsed: [1 day, 2:36:29.660988]  batch: 500  d_loss: -0.011980  g_loss: 0.214157\n",
      "Time taken for epoch: 469.209 secs\n",
      "ticker =  63001\n",
      "\n",
      "Epoch: 127\n",
      "Elapsed: [1 day, 2:36:51.387245]  batch: 1  d_loss: 0.027906  g_loss: 0.304553\n",
      "Elapsed: [1 day, 2:37:32.720053]  batch: 50  d_loss: -0.131778  g_loss: 0.531877\n",
      "Elapsed: [1 day, 2:38:26.789555]  batch: 100  d_loss: 0.096564  g_loss: 0.421214\n",
      "Elapsed: [1 day, 2:39:24.051781]  batch: 150  d_loss: 0.148960  g_loss: 0.229801\n",
      "Elapsed: [1 day, 2:40:18.424286]  batch: 200  d_loss: 0.088691  g_loss: -0.099385\n",
      "Elapsed: [1 day, 2:41:15.411412]  batch: 250  d_loss: -0.180277  g_loss: 0.518017\n",
      "Elapsed: [1 day, 2:42:11.696606]  batch: 300  d_loss: 0.019254  g_loss: 0.179320\n",
      "Elapsed: [1 day, 2:43:06.814674]  batch: 350  d_loss: 0.139833  g_loss: -0.058267\n",
      "Elapsed: [1 day, 2:44:01.200511]  batch: 400  d_loss: -0.023012  g_loss: -0.014650\n",
      "Elapsed: [1 day, 2:44:55.866807]  batch: 450  d_loss: -0.044478  g_loss: 0.451032\n",
      "Elapsed: [1 day, 2:45:50.262511]  batch: 500  d_loss: -0.135676  g_loss: 0.355813\n",
      "Time taken for epoch: 560.314 secs\n",
      "ticker =  63501\n",
      "\n",
      "Epoch: 128\n",
      "Elapsed: [1 day, 2:46:11.894297]  batch: 1  d_loss: -0.028496  g_loss: 0.334108\n",
      "Elapsed: [1 day, 2:46:50.220906]  batch: 50  d_loss: 0.023627  g_loss: 0.427880\n",
      "Elapsed: [1 day, 2:47:47.567095]  batch: 100  d_loss: -0.164870  g_loss: 0.661936\n",
      "Elapsed: [1 day, 2:48:44.816605]  batch: 150  d_loss: -0.196529  g_loss: 0.631551\n",
      "Elapsed: [1 day, 2:49:39.502167]  batch: 200  d_loss: 0.126810  g_loss: 0.269140\n",
      "Elapsed: [1 day, 2:50:37.164181]  batch: 250  d_loss: -0.285765  g_loss: 0.545758\n",
      "Elapsed: [1 day, 2:51:37.087636]  batch: 300  d_loss: -0.082664  g_loss: 0.446445\n",
      "Elapsed: [1 day, 2:52:33.342892]  batch: 350  d_loss: -0.289566  g_loss: 0.511710\n",
      "Elapsed: [1 day, 2:53:29.640167]  batch: 400  d_loss: 0.041286  g_loss: 0.246676\n",
      "Elapsed: [1 day, 2:54:25.380178]  batch: 450  d_loss: -0.261693  g_loss: 0.880227\n",
      "Elapsed: [1 day, 2:55:22.781351]  batch: 500  d_loss: -0.076043  g_loss: 0.470092\n",
      "Time taken for epoch: 572.336 secs\n",
      "ticker =  64001\n",
      "\n",
      "Epoch: 129\n",
      "Elapsed: [1 day, 2:55:45.080789]  batch: 1  d_loss: 0.016465  g_loss: 0.304721\n",
      "Elapsed: [1 day, 2:56:30.062306]  batch: 50  d_loss: 0.095585  g_loss: 0.234726\n",
      "Elapsed: [1 day, 2:57:26.576110]  batch: 100  d_loss: 0.215502  g_loss: -0.302859\n",
      "Elapsed: [1 day, 2:58:22.846188]  batch: 150  d_loss: 0.112132  g_loss: 0.013301\n",
      "Elapsed: [1 day, 2:59:23.953307]  batch: 200  d_loss: -0.064665  g_loss: 0.522341\n",
      "Elapsed: [1 day, 3:00:25.676640]  batch: 250  d_loss: -0.039125  g_loss: 0.188969\n",
      "Elapsed: [1 day, 3:01:28.620791]  batch: 300  d_loss: -0.086016  g_loss: -0.184868\n",
      "Elapsed: [1 day, 3:02:26.057659]  batch: 350  d_loss: 0.049633  g_loss: -0.040901\n",
      "Elapsed: [1 day, 3:03:28.872710]  batch: 400  d_loss: -0.263671  g_loss: 0.627773\n",
      "Elapsed: [1 day, 3:04:28.875939]  batch: 450  d_loss: -0.001696  g_loss: -0.079781\n",
      "Elapsed: [1 day, 3:05:31.896716]  batch: 500  d_loss: 0.049776  g_loss: -0.070663\n",
      "Time taken for epoch: 609.116 secs\n",
      "ticker =  64501\n",
      "\n",
      "Epoch: 130\n",
      "Elapsed: [1 day, 3:05:52.967575]  batch: 1  d_loss: -0.250142  g_loss: 0.288525\n",
      "Elapsed: [1 day, 3:06:40.706519]  batch: 50  d_loss: -0.014545  g_loss: 0.098496\n",
      "Elapsed: [1 day, 3:07:37.939500]  batch: 100  d_loss: 0.113324  g_loss: -0.047786\n",
      "Elapsed: [1 day, 3:08:40.382154]  batch: 150  d_loss: -0.183870  g_loss: 0.393191\n",
      "Elapsed: [1 day, 3:09:40.165783]  batch: 200  d_loss: 0.061965  g_loss: 0.193664\n",
      "Elapsed: [1 day, 3:10:47.965842]  batch: 250  d_loss: 0.060801  g_loss: 0.136590\n",
      "Elapsed: [1 day, 3:11:53.186666]  batch: 300  d_loss: -0.165251  g_loss: 0.719970\n",
      "Elapsed: [1 day, 3:13:06.309231]  batch: 350  d_loss: -0.149075  g_loss: 0.153928\n",
      "Elapsed: [1 day, 3:14:16.823752]  batch: 400  d_loss: -0.266673  g_loss: 0.903631\n",
      "Elapsed: [1 day, 3:15:27.540946]  batch: 450  d_loss: 0.088838  g_loss: -0.152958\n",
      "Elapsed: [1 day, 3:16:40.975984]  batch: 500  d_loss: -0.129968  g_loss: 0.678219\n",
      "Time taken for epoch: 668.630 secs\n",
      "ticker =  65001\n",
      "\n",
      "Epoch: 131\n",
      "Elapsed: [1 day, 3:17:02.942573]  batch: 1  d_loss: 0.097032  g_loss: 0.142095\n",
      "Elapsed: [1 day, 3:17:56.210721]  batch: 50  d_loss: -0.017927  g_loss: 0.413073\n",
      "Elapsed: [1 day, 3:19:09.588792]  batch: 100  d_loss: -0.120031  g_loss: 0.430802\n",
      "Elapsed: [1 day, 3:20:23.000954]  batch: 150  d_loss: -0.063523  g_loss: 0.541198\n",
      "Elapsed: [1 day, 3:21:41.685983]  batch: 200  d_loss: -0.104916  g_loss: 0.130348\n",
      "Elapsed: [1 day, 3:23:00.709592]  batch: 250  d_loss: 0.080135  g_loss: 0.273025\n",
      "Elapsed: [1 day, 3:24:19.927908]  batch: 300  d_loss: -0.103784  g_loss: 0.592850\n",
      "Elapsed: [1 day, 3:25:40.703076]  batch: 350  d_loss: 0.097144  g_loss: 0.095132\n",
      "Elapsed: [1 day, 3:27:04.461513]  batch: 400  d_loss: -0.054033  g_loss: 0.411058\n",
      "Elapsed: [1 day, 3:28:23.069761]  batch: 450  d_loss: -0.007257  g_loss: 0.513825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: [1 day, 3:29:37.025236]  batch: 500  d_loss: -0.072391  g_loss: 0.263413\n",
      "Time taken for epoch: 775.837 secs\n",
      "ticker =  65501\n",
      "\n",
      "Epoch: 132\n",
      "Elapsed: [1 day, 3:29:58.640366]  batch: 1  d_loss: 0.012102  g_loss: 0.007364\n",
      "Elapsed: [1 day, 3:30:49.681216]  batch: 50  d_loss: 0.094317  g_loss: 0.165659\n",
      "Elapsed: [1 day, 3:31:52.618139]  batch: 100  d_loss: -0.126690  g_loss: 0.272588\n",
      "Elapsed: [1 day, 3:32:58.187222]  batch: 150  d_loss: 0.037352  g_loss: 0.365715\n",
      "Elapsed: [1 day, 3:34:06.337693]  batch: 200  d_loss: -0.083908  g_loss: 0.437012\n",
      "Elapsed: [1 day, 3:35:11.965392]  batch: 250  d_loss: 0.027564  g_loss: 0.179753\n",
      "Elapsed: [1 day, 3:36:20.157505]  batch: 300  d_loss: -0.069062  g_loss: 0.258912\n",
      "Elapsed: [1 day, 3:37:23.374550]  batch: 350  d_loss: -0.131374  g_loss: 0.403990\n",
      "Elapsed: [1 day, 3:38:47.586965]  batch: 400  d_loss: 0.034936  g_loss: 0.124690\n",
      "Elapsed: [1 day, 3:39:58.792544]  batch: 450  d_loss: -0.292393  g_loss: 0.332862\n",
      "Elapsed: [1 day, 3:41:11.946484]  batch: 500  d_loss: -0.138394  g_loss: 0.436888\n",
      "Time taken for epoch: 694.714 secs\n",
      "ticker =  66001\n",
      "\n",
      "Epoch: 133\n",
      "Elapsed: [1 day, 3:41:32.897253]  batch: 1  d_loss: 0.028385  g_loss: -0.034515\n",
      "Elapsed: [1 day, 3:42:24.729305]  batch: 50  d_loss: -0.097985  g_loss: 0.197837\n",
      "Elapsed: [1 day, 3:43:38.344610]  batch: 100  d_loss: -0.122443  g_loss: 0.646633\n",
      "Elapsed: [1 day, 3:44:35.340581]  batch: 150  d_loss: -0.146364  g_loss: 0.624444\n",
      "Elapsed: [1 day, 3:45:41.684937]  batch: 200  d_loss: -0.009387  g_loss: -0.099990\n",
      "Elapsed: [1 day, 3:46:41.722746]  batch: 250  d_loss: 0.097845  g_loss: 0.202860\n",
      "Elapsed: [1 day, 3:47:41.721994]  batch: 300  d_loss: -0.081881  g_loss: 0.562717\n",
      "Elapsed: [1 day, 3:48:43.254858]  batch: 350  d_loss: -0.000530  g_loss: 0.143770\n",
      "Elapsed: [1 day, 3:49:49.650023]  batch: 400  d_loss: -0.342630  g_loss: 0.290956\n",
      "Elapsed: [1 day, 3:50:55.201629]  batch: 450  d_loss: -0.010590  g_loss: 0.028030\n",
      "Elapsed: [1 day, 3:52:12.708940]  batch: 500  d_loss: -0.144882  g_loss: 0.582001\n",
      "Time taken for epoch: 660.648 secs\n",
      "ticker =  66501\n",
      "\n",
      "Epoch: 134\n",
      "Elapsed: [1 day, 3:52:34.130985]  batch: 1  d_loss: -0.054106  g_loss: 0.144727\n",
      "Elapsed: [1 day, 3:53:24.727028]  batch: 50  d_loss: -0.036567  g_loss: 0.391453\n",
      "Elapsed: [1 day, 3:54:37.928933]  batch: 100  d_loss: -0.203112  g_loss: 0.621576\n",
      "Elapsed: [1 day, 3:55:55.910851]  batch: 150  d_loss: -0.059752  g_loss: 0.581107\n",
      "Elapsed: [1 day, 3:57:12.998841]  batch: 200  d_loss: -0.076156  g_loss: 0.522606\n",
      "Elapsed: [1 day, 3:58:34.604171]  batch: 250  d_loss: -0.180275  g_loss: 0.527169\n",
      "Elapsed: [1 day, 3:59:50.390476]  batch: 300  d_loss: 0.136818  g_loss: -0.063464\n",
      "Elapsed: [1 day, 4:01:08.897262]  batch: 350  d_loss: 0.119247  g_loss: 0.190893\n",
      "Elapsed: [1 day, 4:02:24.788873]  batch: 400  d_loss: -0.265870  g_loss: 0.566538\n",
      "Elapsed: [1 day, 4:03:37.982875]  batch: 450  d_loss: 0.114671  g_loss: -0.204616\n",
      "Elapsed: [1 day, 4:04:56.514553]  batch: 500  d_loss: -0.146037  g_loss: 0.525373\n",
      "Time taken for epoch: 763.727 secs\n",
      "ticker =  67001\n",
      "\n",
      "Epoch: 135\n",
      "Elapsed: [1 day, 4:05:18.770972]  batch: 1  d_loss: 0.135355  g_loss: 0.104214\n",
      "Elapsed: [1 day, 4:06:14.773863]  batch: 50  d_loss: -0.024655  g_loss: 0.209288\n",
      "Elapsed: [1 day, 4:07:27.965443]  batch: 100  d_loss: 0.148785  g_loss: -0.262193\n",
      "Elapsed: [1 day, 4:08:37.330825]  batch: 150  d_loss: 0.026641  g_loss: 0.352241\n",
      "Elapsed: [1 day, 4:09:36.612575]  batch: 200  d_loss: 0.029078  g_loss: -0.150790\n",
      "Elapsed: [1 day, 4:11:00.623688]  batch: 250  d_loss: -0.007061  g_loss: 0.035827\n",
      "Elapsed: [1 day, 4:12:21.730913]  batch: 300  d_loss: 0.084212  g_loss: 0.221189\n",
      "Elapsed: [1 day, 4:13:45.582146]  batch: 350  d_loss: 0.081057  g_loss: -0.213826\n",
      "Elapsed: [1 day, 4:14:23.141053]  batch: 400  d_loss: -0.421838  g_loss: 0.446354\n",
      "Elapsed: [1 day, 4:14:45.673148]  batch: 450  d_loss: -0.222540  g_loss: 0.660076\n",
      "Elapsed: [1 day, 4:15:08.388477]  batch: 500  d_loss: 0.035739  g_loss: 0.210890\n",
      "Time taken for epoch: 611.197 secs\n",
      "ticker =  67501\n",
      "\n",
      "Epoch: 136\n",
      "Elapsed: [1 day, 4:15:25.979827]  batch: 1  d_loss: -0.074521  g_loss: 0.313855\n",
      "Elapsed: [1 day, 4:15:48.417660]  batch: 50  d_loss: 0.022176  g_loss: 0.728297\n",
      "Elapsed: [1 day, 4:16:10.905941]  batch: 100  d_loss: 0.136592  g_loss: -0.218378\n",
      "Elapsed: [1 day, 4:16:38.018698]  batch: 150  d_loss: -0.082200  g_loss: -0.337470\n",
      "Elapsed: [1 day, 4:17:05.900681]  batch: 200  d_loss: -0.132452  g_loss: 0.451551\n",
      "Elapsed: [1 day, 4:17:33.588095]  batch: 250  d_loss: -0.015331  g_loss: 0.264855\n",
      "Elapsed: [1 day, 4:18:01.115046]  batch: 300  d_loss: 0.005939  g_loss: 0.400235\n",
      "Elapsed: [1 day, 4:18:28.599988]  batch: 350  d_loss: 0.096662  g_loss: 0.092593\n",
      "Elapsed: [1 day, 4:18:56.045397]  batch: 400  d_loss: -0.285034  g_loss: 0.592339\n",
      "Elapsed: [1 day, 4:19:23.395183]  batch: 450  d_loss: -0.255160  g_loss: 0.503244\n",
      "Elapsed: [1 day, 4:19:50.701674]  batch: 500  d_loss: -0.091992  g_loss: 0.419025\n",
      "Time taken for epoch: 282.105 secs\n",
      "ticker =  68001\n",
      "\n",
      "Epoch: 137\n",
      "Elapsed: [1 day, 4:20:04.197821]  batch: 1  d_loss: -0.083648  g_loss: 0.627024\n",
      "Elapsed: [1 day, 4:20:30.751178]  batch: 50  d_loss: 0.096543  g_loss: 0.058069\n",
      "Elapsed: [1 day, 4:20:57.846619]  batch: 100  d_loss: -0.215878  g_loss: 0.584651\n",
      "Elapsed: [1 day, 4:21:24.967871]  batch: 150  d_loss: -0.046096  g_loss: 0.251136\n",
      "Elapsed: [1 day, 4:21:52.178476]  batch: 200  d_loss: 0.056629  g_loss: 0.209755\n",
      "Elapsed: [1 day, 4:22:19.337498]  batch: 250  d_loss: 0.068287  g_loss: 0.055625\n",
      "Elapsed: [1 day, 4:22:46.486473]  batch: 300  d_loss: 0.052662  g_loss: 0.037511\n",
      "Elapsed: [1 day, 4:23:13.654089]  batch: 350  d_loss: -0.266360  g_loss: 0.498893\n",
      "Elapsed: [1 day, 4:23:40.730102]  batch: 400  d_loss: -0.058824  g_loss: 0.563374\n",
      "Elapsed: [1 day, 4:24:07.795519]  batch: 450  d_loss: -0.065881  g_loss: 0.431892\n",
      "Elapsed: [1 day, 4:24:34.809380]  batch: 500  d_loss: 0.013823  g_loss: -0.048538\n",
      "Time taken for epoch: 283.975 secs\n",
      "ticker =  68501\n",
      "\n",
      "Epoch: 138\n",
      "Elapsed: [1 day, 4:24:48.643151]  batch: 1  d_loss: -0.101577  g_loss: 0.126525\n",
      "Elapsed: [1 day, 4:25:15.027689]  batch: 50  d_loss: 0.074477  g_loss: -0.044217\n",
      "Elapsed: [1 day, 4:25:42.119886]  batch: 100  d_loss: -0.144343  g_loss: 0.424538\n",
      "Elapsed: [1 day, 4:26:09.270931]  batch: 150  d_loss: -0.128140  g_loss: 0.345524\n",
      "Elapsed: [1 day, 4:26:36.635401]  batch: 200  d_loss: -0.347001  g_loss: 0.655951\n",
      "Elapsed: [1 day, 4:27:03.894874]  batch: 250  d_loss: -0.297196  g_loss: 0.808447\n",
      "Elapsed: [1 day, 4:27:31.056118]  batch: 300  d_loss: -0.082847  g_loss: 0.067909\n",
      "Elapsed: [1 day, 4:27:58.306139]  batch: 350  d_loss: -0.128927  g_loss: 0.470770\n",
      "Elapsed: [1 day, 4:28:25.440654]  batch: 400  d_loss: -0.029529  g_loss: 0.106033\n",
      "Elapsed: [1 day, 4:28:52.691326]  batch: 450  d_loss: -0.198931  g_loss: 0.131660\n",
      "Elapsed: [1 day, 4:29:19.823556]  batch: 500  d_loss: 0.005834  g_loss: -0.040055\n",
      "Time taken for epoch: 284.924 secs\n",
      "ticker =  69001\n",
      "\n",
      "Epoch: 139\n",
      "Elapsed: [1 day, 4:29:34.956078]  batch: 1  d_loss: -0.208059  g_loss: 0.227725\n",
      "Elapsed: [1 day, 4:30:01.134660]  batch: 50  d_loss: -0.198887  g_loss: 0.594157\n",
      "Elapsed: [1 day, 4:30:28.045893]  batch: 100  d_loss: 0.072021  g_loss: 0.247678\n",
      "Elapsed: [1 day, 4:30:54.936896]  batch: 150  d_loss: -0.207688  g_loss: 0.242870\n",
      "Elapsed: [1 day, 4:31:21.858146]  batch: 200  d_loss: -0.055653  g_loss: 0.395405\n",
      "Elapsed: [1 day, 4:31:48.762958]  batch: 250  d_loss: 0.000995  g_loss: 0.199430\n",
      "Elapsed: [1 day, 4:32:15.659546]  batch: 300  d_loss: 0.121738  g_loss: 0.108201\n",
      "Elapsed: [1 day, 4:32:42.516642]  batch: 350  d_loss: 0.139370  g_loss: 0.077911\n",
      "Elapsed: [1 day, 4:33:09.461143]  batch: 400  d_loss: 0.102662  g_loss: 0.251254\n",
      "Elapsed: [1 day, 4:33:36.359441]  batch: 450  d_loss: 0.098938  g_loss: -0.281416\n",
      "Elapsed: [1 day, 4:34:03.253473]  batch: 500  d_loss: 0.073599  g_loss: 0.135168\n",
      "Time taken for epoch: 283.216 secs\n",
      "ticker =  69501\n",
      "\n",
      "Epoch: 140\n",
      "Elapsed: [1 day, 4:34:16.062130]  batch: 1  d_loss: -0.343452  g_loss: 0.352144\n",
      "Elapsed: [1 day, 4:34:42.556721]  batch: 50  d_loss: -0.104964  g_loss: 0.469082\n",
      "Elapsed: [1 day, 4:35:09.475908]  batch: 100  d_loss: -0.188433  g_loss: 0.062805\n",
      "Elapsed: [1 day, 4:35:36.443074]  batch: 150  d_loss: 0.126104  g_loss: -0.336549\n",
      "Elapsed: [1 day, 4:36:03.600317]  batch: 200  d_loss: -0.292068  g_loss: 0.411417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: [1 day, 4:36:30.611864]  batch: 250  d_loss: -0.168855  g_loss: 0.368229\n",
      "Elapsed: [1 day, 4:36:57.616359]  batch: 300  d_loss: -0.062365  g_loss: 0.463297\n",
      "Elapsed: [1 day, 4:37:24.671078]  batch: 350  d_loss: -0.327568  g_loss: 0.457292\n",
      "Elapsed: [1 day, 4:37:51.654029]  batch: 400  d_loss: -0.085582  g_loss: 0.528141\n",
      "Elapsed: [1 day, 4:38:18.720011]  batch: 450  d_loss: -0.123315  g_loss: 0.440099\n",
      "Elapsed: [1 day, 4:38:45.743187]  batch: 500  d_loss: -0.120514  g_loss: 0.479952\n",
      "Time taken for epoch: 282.401 secs\n",
      "ticker =  70001\n",
      "\n",
      "Epoch: 141\n",
      "Elapsed: [1 day, 4:38:58.097622]  batch: 1  d_loss: 0.051492  g_loss: 0.122134\n",
      "Elapsed: [1 day, 4:39:24.171823]  batch: 50  d_loss: -0.104816  g_loss: 0.360802\n",
      "Elapsed: [1 day, 4:39:51.170166]  batch: 100  d_loss: 0.018789  g_loss: -0.030378\n",
      "Elapsed: [1 day, 4:40:18.232843]  batch: 150  d_loss: -0.127154  g_loss: 0.458685\n",
      "Elapsed: [1 day, 4:40:45.246369]  batch: 200  d_loss: -0.160477  g_loss: 0.358790\n",
      "Elapsed: [1 day, 4:41:12.236746]  batch: 250  d_loss: -0.148025  g_loss: 0.433231\n",
      "Elapsed: [1 day, 4:41:39.296473]  batch: 300  d_loss: 0.140018  g_loss: 0.200073\n",
      "Elapsed: [1 day, 4:42:06.331833]  batch: 350  d_loss: -0.058247  g_loss: 0.414510\n",
      "Elapsed: [1 day, 4:42:33.372766]  batch: 400  d_loss: -0.231383  g_loss: 0.446438\n",
      "Elapsed: [1 day, 4:43:00.402409]  batch: 450  d_loss: -0.189045  g_loss: 0.314321\n",
      "Elapsed: [1 day, 4:43:27.417716]  batch: 500  d_loss: 0.064111  g_loss: 0.345598\n",
      "Time taken for epoch: 281.634 secs\n",
      "ticker =  70501\n",
      "\n",
      "Epoch: 142\n",
      "Elapsed: [1 day, 4:43:41.308821]  batch: 1  d_loss: 0.000538  g_loss: 0.196434\n",
      "Elapsed: [1 day, 4:44:07.850610]  batch: 50  d_loss: -0.059254  g_loss: 0.260673\n",
      "Elapsed: [1 day, 4:44:35.051743]  batch: 100  d_loss: -0.215853  g_loss: 0.774343\n",
      "Elapsed: [1 day, 4:45:02.271813]  batch: 150  d_loss: -0.013178  g_loss: -0.252776\n",
      "Elapsed: [1 day, 4:45:29.389846]  batch: 200  d_loss: 0.016951  g_loss: 0.128920\n",
      "Elapsed: [1 day, 4:45:56.505302]  batch: 250  d_loss: -0.124211  g_loss: 0.340430\n",
      "Elapsed: [1 day, 4:46:23.730803]  batch: 300  d_loss: 0.035947  g_loss: -0.051842\n",
      "Elapsed: [1 day, 4:46:50.928693]  batch: 350  d_loss: -0.339672  g_loss: 0.668093\n",
      "Elapsed: [1 day, 4:47:18.004461]  batch: 400  d_loss: -0.001924  g_loss: 0.179269\n",
      "Elapsed: [1 day, 4:47:45.312561]  batch: 450  d_loss: -0.111258  g_loss: 0.375860\n",
      "Elapsed: [1 day, 4:48:12.435596]  batch: 500  d_loss: 0.072130  g_loss: -0.067333\n",
      "Time taken for epoch: 284.892 secs\n",
      "ticker =  71001\n",
      "\n",
      "Epoch: 143\n",
      "Elapsed: [1 day, 4:48:25.461434]  batch: 1  d_loss: -0.261479  g_loss: 0.422335\n",
      "Elapsed: [1 day, 4:48:51.651181]  batch: 50  d_loss: -0.037977  g_loss: 0.299935\n",
      "Elapsed: [1 day, 4:49:18.586115]  batch: 100  d_loss: -0.121496  g_loss: 0.354330\n",
      "Elapsed: [1 day, 4:49:45.558221]  batch: 150  d_loss: 0.105780  g_loss: 0.127640\n",
      "Elapsed: [1 day, 4:50:12.537472]  batch: 200  d_loss: -0.013587  g_loss: 0.094744\n",
      "Elapsed: [1 day, 4:50:39.463922]  batch: 250  d_loss: 0.078095  g_loss: 0.561555\n",
      "Elapsed: [1 day, 4:51:02.634794]  batch: 300  d_loss: -0.089342  g_loss: 0.338634\n",
      "Elapsed: [1 day, 4:51:24.893180]  batch: 350  d_loss: 0.040352  g_loss: 0.594338\n",
      "Elapsed: [1 day, 4:51:47.160762]  batch: 400  d_loss: -0.009904  g_loss: 0.438695\n",
      "Elapsed: [1 day, 4:52:09.410899]  batch: 450  d_loss: 0.112332  g_loss: 0.278239\n",
      "Elapsed: [1 day, 4:52:31.662764]  batch: 500  d_loss: -0.258525  g_loss: 0.441183\n",
      "Time taken for epoch: 259.109 secs\n",
      "ticker =  71501\n",
      "\n",
      "Epoch: 144\n",
      "Elapsed: [1 day, 4:52:41.805245]  batch: 1  d_loss: 0.134650  g_loss: -0.308232\n",
      "Elapsed: [1 day, 4:53:03.669183]  batch: 50  d_loss: 0.071547  g_loss: 0.431588\n",
      "Elapsed: [1 day, 4:53:26.030052]  batch: 100  d_loss: -0.125325  g_loss: 0.405353\n",
      "Elapsed: [1 day, 4:53:48.352055]  batch: 150  d_loss: -0.082669  g_loss: 0.276510\n",
      "Elapsed: [1 day, 4:54:10.627402]  batch: 200  d_loss: -0.254108  g_loss: 0.474336\n",
      "Elapsed: [1 day, 4:54:32.922670]  batch: 250  d_loss: -0.140671  g_loss: 0.680601\n",
      "Elapsed: [1 day, 4:54:55.236213]  batch: 300  d_loss: -0.085946  g_loss: 0.213716\n",
      "Elapsed: [1 day, 4:55:19.088812]  batch: 350  d_loss: -0.123367  g_loss: 0.460563\n",
      "Elapsed: [1 day, 4:55:43.045836]  batch: 400  d_loss: 0.173754  g_loss: 0.172386\n",
      "Elapsed: [1 day, 4:56:06.976628]  batch: 450  d_loss: -0.145330  g_loss: 0.255512\n",
      "Elapsed: [1 day, 4:56:30.954697]  batch: 500  d_loss: -0.075717  g_loss: 0.485678\n",
      "Time taken for epoch: 239.261 secs\n",
      "ticker =  72001\n",
      "\n",
      "Epoch: 145\n",
      "Elapsed: [1 day, 4:56:44.190631]  batch: 1  d_loss: -0.000440  g_loss: 0.383081\n",
      "Elapsed: [1 day, 4:57:06.454425]  batch: 50  d_loss: -0.054948  g_loss: 0.564278\n",
      "Elapsed: [1 day, 4:57:30.291565]  batch: 100  d_loss: -0.175944  g_loss: 0.573354\n",
      "Elapsed: [1 day, 4:57:54.390216]  batch: 150  d_loss: -0.004566  g_loss: 0.259649\n",
      "Elapsed: [1 day, 4:58:20.128975]  batch: 200  d_loss: -0.031137  g_loss: -0.054184\n",
      "Elapsed: [1 day, 4:58:45.853340]  batch: 250  d_loss: -0.280472  g_loss: 0.604915\n",
      "Elapsed: [1 day, 4:59:11.005248]  batch: 300  d_loss: -0.221551  g_loss: 1.134481\n",
      "Elapsed: [1 day, 4:59:37.292339]  batch: 350  d_loss: -0.173278  g_loss: 0.612677\n",
      "Elapsed: [1 day, 5:00:02.151113]  batch: 400  d_loss: -0.143510  g_loss: 0.219884\n",
      "Elapsed: [1 day, 5:00:27.848749]  batch: 450  d_loss: -0.002821  g_loss: -0.123120\n",
      "Elapsed: [1 day, 5:00:53.003170]  batch: 500  d_loss: 0.012656  g_loss: 0.071523\n",
      "Time taken for epoch: 261.854 secs\n",
      "ticker =  72501\n",
      "\n",
      "Epoch: 146\n",
      "Elapsed: [1 day, 5:01:03.755251]  batch: 1  d_loss: -0.030391  g_loss: 0.343865\n",
      "Elapsed: [1 day, 5:01:26.620821]  batch: 50  d_loss: -0.164369  g_loss: 0.266624\n",
      "Elapsed: [1 day, 5:01:52.581299]  batch: 100  d_loss: 0.031466  g_loss: 0.066063\n",
      "Elapsed: [1 day, 5:02:17.834883]  batch: 150  d_loss: 0.092248  g_loss: 0.149271\n",
      "Elapsed: [1 day, 5:02:41.811583]  batch: 200  d_loss: -0.090075  g_loss: 0.054129\n",
      "Elapsed: [1 day, 5:03:04.576317]  batch: 250  d_loss: -0.025180  g_loss: 0.405801\n",
      "Elapsed: [1 day, 5:03:27.411680]  batch: 300  d_loss: 0.045551  g_loss: 0.480422\n",
      "Elapsed: [1 day, 5:03:50.212008]  batch: 350  d_loss: -0.263453  g_loss: 0.297437\n",
      "Elapsed: [1 day, 5:04:13.059712]  batch: 400  d_loss: -0.319415  g_loss: 0.563040\n",
      "Elapsed: [1 day, 5:04:35.847395]  batch: 450  d_loss: 0.048818  g_loss: 0.178962\n",
      "Elapsed: [1 day, 5:04:58.677620]  batch: 500  d_loss: -0.228291  g_loss: 0.618321\n",
      "Time taken for epoch: 245.784 secs\n",
      "ticker =  73001\n",
      "\n",
      "Epoch: 147\n",
      "Elapsed: [1 day, 5:05:16.800123]  batch: 1  d_loss: 0.093195  g_loss: 0.324828\n",
      "Elapsed: [1 day, 5:05:39.243442]  batch: 50  d_loss: -0.164546  g_loss: 0.369575\n",
      "Elapsed: [1 day, 5:06:02.026370]  batch: 100  d_loss: 0.093678  g_loss: -0.437522\n",
      "Elapsed: [1 day, 5:06:24.713995]  batch: 150  d_loss: -0.036661  g_loss: 0.727527\n",
      "Elapsed: [1 day, 5:06:47.425510]  batch: 200  d_loss: -0.125599  g_loss: 0.467879\n",
      "Elapsed: [1 day, 5:07:10.183480]  batch: 250  d_loss: 0.183953  g_loss: -0.126700\n",
      "Elapsed: [1 day, 5:07:32.896512]  batch: 300  d_loss: -0.079755  g_loss: -0.386726\n",
      "Elapsed: [1 day, 5:07:55.632467]  batch: 350  d_loss: 0.020162  g_loss: 0.051839\n",
      "Elapsed: [1 day, 5:08:18.363630]  batch: 400  d_loss: 0.007919  g_loss: 0.012741\n",
      "Elapsed: [1 day, 5:08:41.144408]  batch: 450  d_loss: 0.021633  g_loss: -0.069006\n",
      "Elapsed: [1 day, 5:09:03.900584]  batch: 500  d_loss: 0.012215  g_loss: 0.515472\n",
      "Time taken for epoch: 244.910 secs\n",
      "ticker =  73501\n",
      "\n",
      "Epoch: 148\n",
      "Elapsed: [1 day, 5:09:20.831348]  batch: 1  d_loss: -0.050606  g_loss: 0.536471\n",
      "Elapsed: [1 day, 5:09:42.784138]  batch: 50  d_loss: -0.268086  g_loss: 1.007960\n",
      "Elapsed: [1 day, 5:10:05.484028]  batch: 100  d_loss: 0.008545  g_loss: 0.132403\n",
      "Elapsed: [1 day, 5:10:28.162278]  batch: 150  d_loss: -0.155090  g_loss: 0.696626\n",
      "Elapsed: [1 day, 5:10:50.859347]  batch: 200  d_loss: -0.172224  g_loss: 0.544909\n",
      "Elapsed: [1 day, 5:11:13.569049]  batch: 250  d_loss: -0.245949  g_loss: 0.611458\n",
      "Elapsed: [1 day, 5:11:36.287418]  batch: 300  d_loss: -0.187269  g_loss: 0.287071\n",
      "Elapsed: [1 day, 5:11:58.980220]  batch: 350  d_loss: -0.043746  g_loss: 0.526777\n",
      "Elapsed: [1 day, 5:12:21.673676]  batch: 400  d_loss: -0.173989  g_loss: 0.341798\n",
      "Elapsed: [1 day, 5:12:44.379404]  batch: 450  d_loss: -0.392149  g_loss: 0.851308\n",
      "Elapsed: [1 day, 5:13:07.121247]  batch: 500  d_loss: -0.075332  g_loss: 0.097567\n",
      "Time taken for epoch: 243.083 secs\n",
      "ticker =  74001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 149\n",
      "Elapsed: [1 day, 5:13:24.070204]  batch: 1  d_loss: -0.280204  g_loss: 0.556040\n",
      "Elapsed: [1 day, 5:13:45.916221]  batch: 50  d_loss: 0.001867  g_loss: 0.086287\n",
      "Elapsed: [1 day, 5:14:08.602118]  batch: 100  d_loss: 0.053681  g_loss: 0.497668\n",
      "Elapsed: [1 day, 5:14:31.286700]  batch: 150  d_loss: -0.267893  g_loss: 0.483749\n",
      "Elapsed: [1 day, 5:14:54.001423]  batch: 200  d_loss: -0.110940  g_loss: 0.687385\n",
      "Elapsed: [1 day, 5:15:16.766541]  batch: 250  d_loss: 0.107022  g_loss: -0.355600\n",
      "Elapsed: [1 day, 5:15:39.456154]  batch: 300  d_loss: -0.173507  g_loss: 0.490215\n",
      "Elapsed: [1 day, 5:16:02.209115]  batch: 350  d_loss: 0.015733  g_loss: 0.136132\n",
      "Elapsed: [1 day, 5:16:24.932029]  batch: 400  d_loss: -0.109720  g_loss: 0.163083\n",
      "Elapsed: [1 day, 5:16:47.623327]  batch: 450  d_loss: -0.054447  g_loss: -0.010284\n",
      "Elapsed: [1 day, 5:17:10.387563]  batch: 500  d_loss: 0.026237  g_loss: 0.304077\n",
      "Time taken for epoch: 243.103 secs\n",
      "ticker =  74501\n",
      "\n",
      "Epoch: 150\n",
      "Elapsed: [1 day, 5:17:27.727778]  batch: 1  d_loss: -0.160946  g_loss: 0.603515\n",
      "Elapsed: [1 day, 5:17:49.413459]  batch: 50  d_loss: 0.005037  g_loss: -0.290169\n",
      "Elapsed: [1 day, 5:18:12.109433]  batch: 100  d_loss: -0.031639  g_loss: 0.303370\n",
      "Elapsed: [1 day, 5:18:34.956189]  batch: 150  d_loss: -0.200018  g_loss: 0.402522\n",
      "Elapsed: [1 day, 5:18:57.652509]  batch: 200  d_loss: -0.198914  g_loss: 0.726241\n",
      "Elapsed: [1 day, 5:19:20.334855]  batch: 250  d_loss: 0.007926  g_loss: -0.017133\n",
      "Elapsed: [1 day, 5:19:43.008337]  batch: 300  d_loss: 0.050972  g_loss: -0.258834\n",
      "Elapsed: [1 day, 5:20:05.711444]  batch: 350  d_loss: 0.048794  g_loss: 0.150479\n",
      "Elapsed: [1 day, 5:20:30.378755]  batch: 400  d_loss: -0.251728  g_loss: 0.654042\n",
      "Elapsed: [1 day, 5:20:56.333594]  batch: 450  d_loss: -0.107959  g_loss: -0.006510\n",
      "Elapsed: [1 day, 5:21:20.994325]  batch: 500  d_loss: 0.012622  g_loss: 0.904917\n",
      "Time taken for epoch: 250.947 secs\n",
      "ticker =  75001\n",
      "Training completed ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "path = \".\"\n",
    "data_path = path + \"/training_images\"\n",
    "\n",
    "# some parameters:\n",
    "depth = 7\n",
    "START_DEPTH = 0\n",
    "START_EPOCH = 1\n",
    "# hyper-parameters per depth (resolution)\n",
    "num_epochs = [10, 25, 35, 50, 75, 100, 150]\n",
    "fade_ins = [50, 50, 50, 50, 50, 50,50]\n",
    "batch_sizes = depth*[20]\n",
    "latent_size = 128\n",
    "# get the data. Ignore the test data and their classes\n",
    "dataset = tv.datasets.ImageFolder(root=data_path,transform=tv.transforms.ToTensor())\n",
    "print(dataset)\n",
    "print([dataset[i][0].shape for i in range(1)])\n",
    "# dataset1 = np.load(path + '/Data/Train_5cm_norm.npz')\n",
    "# dataset2 = dataset1['arr_0']\n",
    "# print(dataset2.shape)\n",
    "# dataset = TensorDataset(*dataset2)\n",
    "# ======================================================================\n",
    "# This line creates the PRO-GAN\n",
    "# ======================================================================\n",
    "# pro_gan = ConditionalProGAN(num_classes=10, depth=depth, \n",
    "#                                latent_size=latent_size, device=device)\n",
    "pro_gan = ProGAN(depth=depth, \n",
    "                               latent_size=latent_size, device=device)\n",
    "# ======================================================================\n",
    "\n",
    "# ======================================================================\n",
    "# This line trains the PRO-GAN\n",
    "# ======================================================================\n",
    "pro_gan.train(\n",
    "    dataset=dataset,\n",
    "    epochs=num_epochs,\n",
    "    fade_in_percentage=fade_ins,\n",
    "    batch_sizes=batch_sizes,\n",
    "    log_dir=path + \"/models/\", sample_dir=path + \"/samples/\", save_dir= path + \"/models/\",\n",
    "    start_depth=START_DEPTH,\n",
    "    feedback_factor=10,\n",
    "    start_epoch=START_EPOCH\n",
    ")\n",
    "# ======================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for file in os.listdir('dataset/passive_margin'):\n",
    "        tab = np.load('dataset/passive_margin/'+file)\n",
    "        tab = (tab - np.mean(tab))/np.std(tab)\n",
    "        tab2=np.zeros((tab.shape[0],tab.shape[1],3))\n",
    "        tab2[:,:,0]=tab\n",
    "        tab2[:,:,1]=tab\n",
    "        tab2[:,:,2]=tab\n",
    "        name=file.split('.')[0]\n",
    "        cv2.imwrite(f'training_images/passive_margin/{name}.png',\n",
    "                    np.uint8((tab2-np.min(tab2))/(np.max(tab2)-np.min(tab2))*255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 8759561,
     "status": "aborted",
     "timestamp": 1593966902182,
     "user": {
      "displayName": "Manos Panagiotou",
      "photoUrl": "",
      "userId": "06697745822174955863"
     },
     "user_tz": -180
    },
    "id": "LZyq5O4cqW0p"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite('image.jpg',np.zeros((128,128,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "217.35"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "189*1.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(110000 -48400)/(7*110000)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Restart ProGAN (4) (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
